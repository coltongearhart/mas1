# Classification

```{r}
#| label: load-prereqs
#| echo: false
#| message: false

# knitr options
source("_common.R")

```

\newcommand{\follow}[1]{\sim \text{#1}\,}

<!-- % (followsp is short for 'follow special') shortcut that can be used for iid or ind ~ 'Named dist ' in normal font with space before parameters would go -->

\newcommand{\e}{\mathrm{e}}

<!-- % shortcut for matrix notation -->

## Notes

FIND ??? and !!! to address

### An overview of classification

The linear regression model discussed in earlier assumes that the response variable $Y$ is *quantitative*. But in many situations, the response variable is instead *qualitative*. In this chapter, we study approaches for predicting qualitative responses, a process that is known as *classification*.

Just as in the regression setting, in the classification setting we have a set of training observations $(x_1,y_1), \ldots, (x_n,y_n)$ that we can use to build a classifier. We want our classifier to perform well not only on the training data, but also on test observations that were not used to train the classifier.

### Why not linear regression?

Could try to code categories to numbers such as 

$$
Y = \begin{cases}
   1  & \text{if a} \\
   2  & \text{if b} \\
   3  & \text{if c} \\
\end{cases}
$$
However, this implies an ordering of the outcomes, which means 'b' is above 'a', 'c' is above 'b' and the difference between 'a' and 'b' is the same as the difference between 'b' and 'c'. Typically with categorical variables, order is arbitrary, so it could easily be switched around and lead to a drastically different model. If there is a natural ordering, such as mild, moderate, and severe AND we felt the gap mild and moderate was similar to the gap between moderate and severe, then a 1, 2, 3 coding would be reasonable. Unfortunately, in general there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.

For a *binary* (two level) qualitative response, the situation is better. We can use a dummy variable approach to code the response:

$$
Y = \begin{cases}
   0   & \text{if a} \\
   1   & \text{if b} \\
\end{cases}
$$

We could then fit a linear regression to this binary response, and predict 'b' if $\hat{Y} > 0.5$ and 'a' otherwise. In this case, even if we flip the coding, the linear regression will produce the same final predictions (so coding doesn't matter).

For a binary response with a 0/1 coding as above, regression by least squares is not completely unreasonable: it can be shown that the $X\hat{\beta}$ obtained using linear regression is in fact an estimate of $P(Y = 1 \mid X)$ in this special case. However, if we use linear regression, some of our estimates might be outside the [0, 1] interval, making them hard to interpret as probabilities!

![](files/images/4-classification-linreg.png){width="50%"}

Nevertheless, the predictions provide an ordering and can be interpreted as crude probability estimates. **Curiously, it turns out that the classifications that we get if we use linear regression to predict a binary response will be the same as for the linear discriminant analysis (LDA) procedure shown later.**

To summarize, there are at least two reasons not to perform classification using a regression method: (a) a regression method cannot accommodate a qualitative response with more than two classes; (b) a regression method will not provide meaningful estimates of $P(Y \mid X)$, even with just two classes. Thus, it is preferable to use a classification method that is truly suited for qualitative response values.

### Logistic regression

Consider the Default data set, where the response default falls into one of two categories, Yes or No. **Rather than modeling this response Y directly, logistic regression models the *probability* that Y belongs to a particular category.**

For the Default data, logistic regression models the probability of default. For example, the probability of default given balance can be written as

$$
P(\text{default} = \text{Yes} \mid \text{balance})
$$

The values of $P(\text{default} = \text{Yes} \mid \text{balance})$, which we abbreviate $p(\text{balance})$, will range between 0 and 1. Then for any given value of balance, a prediction can be made for default. For example, one might predict default = Yes for any individual for whom $p(\text{balance}) > 0.5$. Alternatively, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as $p(\text{balance}) > 0.1$.

#### The logisitic model {#sec-logistic-model}

How should we model the relationship between $p(X) = P(Y = 1 \mid X)$ and $X$? Above, we considered using a linear regression model to represent these probabilities:

$$
p(X) = \beta_0 + \beta_1 X
$$

If we use this approach to predict default=Yes using balance, then we obtain the model shown in the left-hand panel of Figure 4.2, which results in the obvious problem of predictions out of bounds. Any time a straight line is fit to a binary response that is coded as 0 or 1, in principle we can always predict $p(X) < 0$ for some values of $X$ and $p(X) > 1$ for others (unless the range of $X$ is limited).

To avoid this problem, we must model $p(X)$ using a function that gives outputs between 0 and 1 for all values of X. Many functions meet this description. In logistic regression, we use the *logistic function*,

$$
p(X) = \frac{\e^{\beta_0 + \beta_1 X}}{1 + \e^{\beta_0 + \beta_1 X}}  = \frac{1}{1 + \e^{-\beta_0 + \beta_1 X}}
$${#eq-logistic-function}

To fit this model, we use *maximum likelihood*. The right-hand panel of Figure 4.2 illustrates the fit of the logistic regression model to the Default data. Notice that for low balances we now predict the probability of default as close to, but never below, zero. Likewise, for high balances we predict a default probability close to, but never above, one. The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of $X$, we will obtain a sensible prediction. We also see that the logistic model is better able to capture the range of probabilities than is the linear regression. The average fitted probability in both cases is 0.0333 (averaged over the training data), which is the same as the overall proportion of defaulters in the data set. We can show:

$$
\begin{align*}
p(X) = p &= \frac{\e^{\beta_0 + \beta_1 X}}{1 + \e^{\beta_0 + \beta_1 X}} \quad\text{for simplicity}\\
p(1 + \e^{\beta_0 + \beta_1 X}) &= \e^{\beta_0 + \beta_1 X}\\
p + p\e^{\beta_0 + \beta_1 X} &= \e^{\beta_0 + \beta_1 X}\\
p + p\e^{\beta_0 + \beta_1 X} - \e^{\beta_0 + \beta_1 X} &= 0\\
p + \e^{\beta_0 + \beta_1 X}(p - 1) &= 0\\
\e^{\beta_0 + \beta_1 X}(p - 1) &= -p\\
\e^{\beta_0 + \beta_1 X} &= \frac{-p}{p - 1}\\
\e^{\beta_0 + \beta_1 X} &= \frac{p(X)}{1 - p(X)}\\
\end{align*}
$$

The quantity $p(X) / [1 - p(X)]$ is called the *odds*, and can take on any value between 0 and $\infty$. Values of the odds close to 0 and $\infty$ indicate very low and very high probabilities of default, respectively.For example, on average 1 in 5 people with an odds of 1/4 will default, since $p(X) = 0.2$ implies an odds of $\frac{0.2}{1 - 0.2} = 1/4$.

By taking the logarithm of both sides, we arrive at the *log odds* or *logit*.

$$
\log\Big(\frac{p(X)}{1 - p(X)}\Big) = \beta_0 + \beta_1 X
$${#eq-logit}

Thus, we see that the logistic regression model $p(X) = \frac{\e^{\beta_0 + \beta_1 X}}{1 + \e^{\beta_0 + \beta_1 X}}$ has a logit that is linear in $X$.

Now in a logistic regression model, increasing $X$ by one unit changes the log odds by $\beta_1$. Equivalently, it multiplies the odds by $\e^{\beta_1}$. However, because the relationship between $p(X)$ and $X$ is not a straight line anymore, $\beta_1$ does *not* correspond to the change in $p(X)$ associated with a one-unit increase in X. The amount that $p(X)$ changes due to a one-unit change in $X$ depends on the current value of $X$. But regardless of the value of $X$, if $\beta_1$ is positive then increasing $X$ will be associated with increasing $p(X)$, and if $\beta_1$ is negative then increasing $X$ will be associated with decreasing $p(X)$.

#### Estimating the regression coefficients

The coefficients in @eq-logistic-function are unknown, and must be estimated based on the available training data. Although we could use (non-linear) least squares to fit the model @eq-logit, the more general method of maximum likelihood is preferred, since it has better statistical properties. The basic intuition behind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for $\beta_0$ and $\beta_1$ such that the predicted probability $\hat{p}(x_i)$ of default for each individual, using @eq-logistic-function, corresponds as closely as possible to the individual’s observed default status.  In other words, we try to find $\hat{\beta}_0$ and $\hat{\beta}_1$ such that plugging these estimates into the model for $p(X)$, given in @eq-logistic-function, yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not. This intuition can be formalized using a mathematical equation called a *likelihood function*:

$$
\ell(\beta_0, \beta_1) = \prod_{i:y_i = 1} p(x_i) \prod_{i':y_{i'}=0} (1 - p(x_{i'}))
$${#eq-logistic-likelihood}

(note that this notation is the same as the following, just without the powers)

$$
\ell(\beta_0, \beta_1) = \prod_{i = 1}^n p(x_i)^{y_i} (1 - p(x_i))^{1 - y_i}
$$

The estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ are chosen to *maximize* this likelihood function. Said another way: to minimize the mis-classification rate, we should predict $Y = 1$ when $p \ge 0.5$ and $Y = 0$ when $p < 0.5$. This means guessing 1 whenever $\beta_0 + X\beta$ is non-negative (as seen in the last version of @eq-logistic-function, where $\frac{1}{1 + \e^{-0}} = 1/2$ is the cutoff point), and 0 otherwise. So logistic regression gives us a linear classifier. The decision boundary separating the two predicted classes is the solution of $\beta_0 + X \beta_1 = 0$, which is a point if $X$ is one dimensional, a line if it is two dimensional, etc.

Maximum likelihood is a very general approach that is used to fit many of the non-linear models that we examine throughout this book. In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood.

![](files/images/4-logreg-output.png){width="50%"}

Many aspects of the logistic regression output shown in Table 4.1 are similar to the linear regression output in the previous chapter. For example, we can measure the accuracy of the coefficient estimates by computing their standard errors. The z-statistic in Table 4.1 plays the same role as the t-statistic in the linear regression output. For instance, the z-statistic associated with $\beta_1$ is equal to $\beta_1/SE(\beta_1)$, and so a large (absolute) value of the z-statistic indicates evidence against the null hypothesis $H_0: \beta_1 = 0$. This null hypothesis implies that $p(X) = e^{\beta_0}$: in other words, that the probability of default does not depend on balance. The estimated intercept in Table 4.1 is typically not of interest; its main purpose is to adjust the average fitted probabilities to the proportion of ones in the data (in this case, the overall default rate).

#### Making predictions

Once the coefficients have been estimated, we can compute the probability of default for any given credit card balance.

![](files/images/4-logreg-prediction.png){width="50%"}

#### Multiple logistic regression

We now consider the problem of predicting a binary response using multiple predictors. By analogy with the extension from simple to multiple linear regression in the previous chapter, we can generalize @eq-logit as follows:

$$
\log\Big(\frac{p(X)}{1 - p(X)}\Big) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
$${#eq-logit-multiple}

This can be rewritten as:

$$
p(X) = \frac{\e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}{1 + \e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}
$${#eq-logistic-function-multiple}

Then we use  maximum likelihood method as before. Beware of interpreting the effects (mainly the sign) of coefficients when some important predictors may be missing from your model. *Confounding* is an issue, just like in MLR.

#### Multinomial logisitic regression

We sometimes wish to classify a response variable that has more than two classes. It turns out that it is possible to extend the two-class logistic regression approach to the setting of $K > 2$ classes. This extension is sometimes known as *multinomial logistic regression*. To do this, we first select a single class to serve as the *baseline*; without loss of generality, we select the $K$th class for this role. Then we replace the model @eq-logistic-function-multiple with the model

$$
p(Y = k \mid X = x) = \frac{\e^{\beta_{k0} + \beta_{k1} X_1 + \cdots + \beta_{kp} X_p}}{1 + \sum_{l = 1}^{K - 1}\e^{\beta_{l0} + \beta_{l1} X_1 + \cdots + \beta_{lp} X_p}}
$$

for $k = 1, \ldots, K-1$ and 

$$
p(Y = K \mid X = x) = \frac{1}{1 + \sum_{l = 1}^{K - 1}\e^{\beta_{l0} + \beta_{l1} X_1 + \cdots + \beta_{lp} X_p}}
$$

It is not hard to show that for $k = 1, \ldots, K-1$, 

$$
\log\Big(\frac{P(Y = k \mid X = x)}{P(Y = K \mid X = x)}\Big) = \beta_{k0} + \beta_{k1} X_1 + \cdots + \beta_{kp} X_p
$${#eq-multinomial-logreg}

Notice that this is quite similar to @eq-logit and indicates that once again, the log odds between any pair of classes is linear in the features. The decision of which class to have as the baseline is unimportant. If we were to change the baseline, the coefficient estimates would differ between the two fitted models, but the fitted values (predictions), the log odds between any pair of classes, and the other key model outputs will remain the same. Nonetheless, interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline.

![](files/images/4-multi-logreg-coefs.png){width="50%"}

Textbook presents an alternative coding for multinomial logistic regression, known as the *softmax* coding. This is used a lot in machine learning; skipping for now.

### Generative models for classification

**Logistic regression involves directly modeling $P(Y = k \mid X = x)$ using the logistic function, given by eq-logistic-function-multiple for the case of two response classes.** In statistical jargon, we model the conditional distribution of the response $Y$, given the predictor(s) $X$. We now consider an alternative and less direct approach to estimating these probabilities. **In this new approach, we model the distribution of the predictors $X$ separately in each of the response classes (i.e. for each value of $Y$). We then use Bayes' theorem to flip these around into estimates for $P(Y = k \mid X = x)$.** When the distribution of $X$ within each class is assumed to be normal, it turns out that the model is very similar in form to logistic regression.

Why do we need another method, when we have logistic regression? There are several reasons:

- When there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable. The methods that we consider in this section do not suffer from this problem.

- If the distribution of the predictors $X$ is approximately normal in each of the classes and the sample size is small, then the approaches in this section may be more accurate than logistic regression.

- The methods in this section can be naturally extended to the case of more than two response classes.

Suppose that we wish to classify an observation into one of $K$ classes, where $K \ge 2$. In other words, the qualitative response variable $Y$ can take on $ K$ possible distinct and unordered values. Let $\pi_k$ represent the overall or prior probability that a randomly chosen observation comes from the $k$th class. Let $f_k(X) \equiv P(X \mid Y = k)$ denote the *density function* of $X$ for an observation that comes from the $k$th class. In other words, $f_k(x)$ is relatively large if there is a high probability that an observation in the $k$th class has $X \approx x$, and $f_k(x)$ is small if it is very unlikely that an observation in the $k$th class has $X \approx x$. Then Bayes' theorem states that 

$$
P(Y = k \mid X = x) = \frac{\pi_k f_k(x)}{\sum_{l = 1}^K \pi_l f_l(x)}
$${#eq-bayes-theorem}

!!! draw tree picture

In accordance with our earlier notation, we will use the abbreviation $p_k(x) = P(Y = k \mid X = x)$; this is the posterior probability that an observation $X = x$ belongs to the $k$th class. That is, it is the probability that the observation belongs to the $k$th class, *given* the predictor value for that observation.

The above equation suggests that instead of directly computing the posterior probability $p_k(x)$ as in @sec-logistic-model, we can simply plug in estimates of $\pi_k$ and $f_k(x)$ into @eq-bayes-theorem. In general, estimating $\pi_k$ is easy if we have a random sample from the population: we simply compute the fraction of the training observations that belong to the $k$th class. However, estimating the density function $f_k(x)$ is much more challenging. As we will see, to estimate $f_k(x)$, we will typically have to make some simplifying assumptions.

We know from earlier that the Bayes classifier, which classifies an observation $x$ to the class for which $p_k(x)$ is largest, has the lowest possible error rate out of all classifiers. (Of course, this is only true if all of the terms in @eq-bayes-theorem are correctly specified.) Therefore, if we can find a way to estimate $f_k(x)$, then we can plug it into @eq-bayes-theorem in order to approximate the Bayes classifier.

In the following sections, we discuss three classifiers that use different estimates of $f_k(x)$ in @eq-bayes-theorem to approximate the Bayes classifier: *linear discriminant analysis*, *quadratic discriminant analysis*, and *naive Bayes*.

#### Linear discrimant analysis for $p = 1$

For now, assume that $p = 1$ —- that is, we have only one predictor. We would like to obtain an estimate for $f_k(x)$ that we can plug into @eq-bayes-theorem in order to estimate $p_k(x)$. We will then classify an observation to the class for which $p_k(x)$ is greatest. To estimate $f_k(x)$, we will first make some assumptions about its form.

In particular, we assume that $f_k(x)$ is *normal* or *Gaussian.* In the one- normal dimensional setting, the normal density takes the form

$$
f_k(x) = \frac{1}{\sqrt{2\pi} \sigma_k} \exp\Big(-\frac{1}{2\sigma_k^2}(x - \mu_k)^2\Big)
$${#eq-normal-x}

where $\mu_k$ and $\sigma_k^2$ are the mean and variance parameters for the $k$th class. For now, let us further assume that $\sigma_1^2 = \cdots = \sigma_K^2$, that is, there is a shared variance term across all $K$ classes, which for simplicity we can denote by $\sigma^2$. Plugging this into @eq-bayes-theorem, we find that 

$$
p_k(x) = \frac{\pi_k \frac{1}{\sqrt{2\pi} \sigma} \exp\big(-\frac{1}{2\sigma^2}(x - \mu_k)^2\big)}{\sum_{l = 1}^K \pi_l \frac{1}{\sqrt{2\pi} \sigma} \exp\big(-\frac{1}{2\sigma^2}(x - \mu_l)^2\big)}
$$

The Bayes classifier involves assigning an observation $X = x$ to the class for which the above equation is the largest (recall this Bayes classifier is different than Bayes theorem @eq-bayes-theorem, which allows us to manipulate conditional distributions). Taking the log of this and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which

$$
\delta_k(x) = x \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
$${#eq-bayes-decision-boundary}

is largest. For instance, if $K = 2$ and $\pi_1 = \pi_2$, then the Bayes classifier assignes an observation to class 1 if $2x(\mu_1 - \mu_2) > \mu_1^2 - \mu_2^2$ and to class 2 otherwise. The Bayes decision boundary is the point for which $\delta_1(x) = \delta_2(x)$; one can show that this amounts to

$$
x = \frac{\mu_1^2 - \mu_2^2}{2(\mu_1 - \mu_2)} = \frac{\mu_1 + \mu_2}{2}
$${#eq-bayes-decision-boundary2}

<!-- !!! do this math -->

An example is shown in the left-hand panel of Figure 4.4. The two normal density functions that are displayed, $f_1(x)$ and $f_2(x)$, represent two distinct classes. The mean and variance parameters for the two density functions are $\mu_1 = −1.25, \mu_2 = 1.25$ and $\sigma_1^2 = \sigma_2^2 = 1$. The two densities overlap, and so given that $X = x$, there is some uncertainty about the class to which the observation belongs. If we assume that an observation is equally likely to come from either class -- that is, $\pi_1 = \pi_2 = 0.5$ --then by inspection of @bayes-decision-boundary2, we see that the Bayes classifier assigns the observation to class 1 if $x < 0$ and class 2 otherwise. Note that in this case, we can compute the Bayes classifier because we know that $X$ is drawn from a Gaussian distribution within each class, and we know all of the parameters involved. In a real-life situation, we are not able to calculate the Bayes classifier.

In practice, even if we are quite certain of our assumption that $X$ is drawn from a Gaussian distribution within each class, to apply the Bayes classifier we still have to estimate the parameters $\mu_1, \ldots, \mu_K, \pi_1, \ldots, \pi_K$, and $\sigma^2$. The *linear discriminant analysis* (LDA) method approximates the Bayes classifier by plugging estimates for $\pi_k$, $\mu_k$, and $\sigma^2$ into @bayes-decision-boundary. In particular, the following estimates are used:

$$
\begin{align*}
\hat{\mu}_k &= \frac{1}{n_k} \sum_{i:y_i = k} x_i\\
\hat{\sigma}^2 &= \frac{1}{n - K} \sum_{k = 1}^K \sum_{i:y_i = k} (x_i - \hat{\mu}_k)^2\\
\end{align*}
$$

where $n$ is the total number of training observations, and $n_k$ is the number of training observations in the $k$th class. The estimate for $\mu_k$ is simply the average of all the training observations from the $k$th class, while $\sigma^2$ can be seen as a weighted average of the sample variances for each of the $K$ classes. Sometimes we have knowledge of the class membership probabilities $\pi_1,  \ldots, \pi_K$, which can be used directly. In the absence of any additional information, LDA estimates $\pi_k$ using the proportion of the training observations that belong to the $k$th class. In other words,

$$
\hat{\pi}_k = n_k / n
$$

The LDA classifier plugs the estimates above into @eq-bayes-decision-boundary and assigns an observation $X = x$ to the class for which 

$$
\hat{\delta}_k(x) = x \frac{\hat{\mu}_k}{\hat{\sigma}^2} - \frac{\hat{\mu}_k^2}{2\hat{\sigma}^2} + \log(\hat{\pi}_k)
$$

is largest. The word *linear* in the classifier's name stems from the fact
that the discriminant functions $\hat{\delta}_k(x)$ above are linear functions of $x$ (as discriminant opposed to a more complex function of $x$).

The right-hand panel of Figure 4.4 displays a histogram of a random sample of 20 observations from each class. In this case, since $n_1 = n_2 = 20$, we have $\hat{\pi}_1 = \hat{\pi}_2$. As a result, the decision boundary corresponds to the midpoint between the sample means for the two classes, $(\hat{\mu}_1 + \hat{\mu}_2) / 2$.

To reiterate, the LDA classifier results from assuming that the observations within each class come from a normal distribution with a class-specific mean and a common variance $\sigma^2$, and plugging estimates for these parameters into the Bayes classifier. In a later section, we will consider a less stringent set of assumptions, by allowing the observations in the $k$th class to have a class-specific variance, $\sigma_k^2$.

#### Linear discriminant analysis for $p > 1$

We now extend the LDA classifier to the case of multiple predictors. To do this, we will assume that $X = (X_1, X_2, \ldots, X_p)$ is drawn from a *multivariate Gaussian* (or multivariate normal) distribution, with a class-specific  mean vector and a common covariance matrix.

The multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution, as in @eq-normal-x, with some correlation between each pair of predictors.  

![](files/images/4-multivariate-normal.png){width="50%"}

To indicate that a $p$-dimensional random variable $X$ has a multivariate Gaussian distribution, we write $X \follow{N}(\mu, \Sigma)$. Here, $E(X) = \mu$ is the mean of $X$ (a vector with $p$ components), and $\text{Cov}(X) = \Sigma$ is the $p \times p$ covariance matrix of $X$. Formally, the multivariate Gaussian density is defined as

$$
f(x) = \frac{1}{(2\pi)^{p/2} \lvert \Sigma \rvert^{1/2}} \exp\Big(-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu)\Big)
$$

In the case of $p > 1$ predictors, the LDA classifier assumes that the observations in the $k$th class are drawn from a multivariate Gaussian distribution $N(\mu_k, \Sigma)$, where $\mu_k$ is a class-specific mean vector, and $\Sigma$ is a covariance matrix that is common to all $K$ classes. Plugging the density function for the $k$th class, $f_k(X = x)$, into @eq-bayes-theorem and performing a little bit of algebra reveals that the Bayes classifier assigns an observation $X = x$ to the class for which

$$
\delta_k(x) = x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T \Sigma^{-1}\mu_k + \log \pi_k
$${#eq-bayes-decision-boundary3}

is largest. This is the vector / matrix version of @eq-bayes-decision-boundary.

An example is shown in the left-hand panel of Figure 4.6.

![](files/images/4-lda-example.png){width="50%"}

Three equally-sized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix. The three ellipses represent regions that contain 95% of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries. In other words, they represent the set of values x for which $\delta_k(x) = \delta_l(x)$; i.e.

$$
x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T \Sigma^{-1}\mu_k = x^T \Sigma^{-1}\mu_l - \frac{1}{2}\mu_l^T \Sigma^{-1}\mu_l
$$

for $k \ne l$. (The $\log \pi_k$) term has disappeared because each of the three classes has the same number of training observations). Note that there are three lines representing the Bayes decision boundaries because there are three *pairs of classes* among the three classes. That is, one Bayes decision boundary separates class 1 from class 2, one separates class 1 from class 3, and one separates class 2 from class 3. These three Bayes decision boundaries divide the predictor space into three regions. The Bayes classifier will classify an observation according to the region in which it is located.

Once again, estimate the all the unknown parameters, similarly to how performed with $p = 1$ and plug into @eq-bayes-decision-boundary3. Then assign the class to that which results in the largest $\hat{\delta}_k(x)$. Note that this is still a *linear* function of $x$

Results of classification can be compactly shown in *confusion matrices*.

![](files/images/4-confusion-matrix.png){width="50%"}

For the default data, the LDA model fit to the 10,000 training samples results in a training error rate of 2.75%. This sounds like a low error rate, but two caveats must be noted.

- First of all, training error rates will usually be lower than test error
rates, which are the real quantity of interest.

- Second, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that an individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3.33 %. In other words, the trivial null classifier will achieve an error rate that null is only a bit higher than the LDA training set error rate.

In practice, a binary classifier such as this one can make two types of
errors: it can incorrectly assign an individual who defaults to the no default category, or it can incorrectly assign an individual who does not default to
the default category. It is often of interest to determine which of these two
types of errors are being made. The table reveals that LDA predicted that a total of 104 people would default. Of these people, 81 actually defaulted and 23 did not. Hence only 23 out of 9,667 of the individuals who did not default were incorrectly labeled. This looks like a pretty low error rate! However, of the 333 individuals who defaulted, 252 (or 75.7%) were missed by LDA. **So while the overall error rate is low, the error rate among individuals who defaulted is very high.**

Also, notice that in this example student status is qualitative -- thus, the normality assumption made by LDA is clearly violated! **However, LDA is often remarkably robust to model violations, as this example shows. Naive Bayes, discussed later, provides an alternative to LDA that does not assume normally distributed predictors.**

Class-specific performance is also important in medicine and biology, where the terms *sensitivity* and *specificity* characterize the performance of a classifier or screening test.

- Sensitivity = true positive % (e.g. true defaulters that are identified; 81/333 = 24.3%)

- Specificity = true negative % (e.g. true non-defaulters that are identified; 9644/9667 = 1 - 23/9667 = 99.8%)

Why does LDA do such a poor job of classifying the customers who default? In other words, why does it have such low sensitivity? As we have seen, LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers. That is, the Bayes classifier will yield the smallest possible total number of misclassified observations, *regardless of the class from which the errors stem*. Some misclassifications will result from incorrectly assigning a customer who does not default to the default class, and others will result from incorrectly assigning a customer who defaults to the non-default class. In contrast, a credit card company might particularly wish to avoid incorrectly classifying an individual who will default, whereas incorrectly classifying an individual who will not default, though still to be avoided, is less problematic. We will now see that it is possible to modify LDA in order to develop a classifier that better meets the credit card company’s needs.

The Bayes classifier works by assigning an observation to the class for which the posterior probability $p_k(X)$ is greatest. In the two-class case, this amounts to assigning an observation to the default class if

$$
P(\text{default} = \text{Yes} \mid X = x) > 0.5
$$

Thus, the Bayes classifier, and by extension LDA, uses a threshold of 50 % for the posterior probability of default in order to assign an observation to the *default* class. However, if we are concerned about incorrectly predicting the default status for individuals who default, then we can consider lowering this threshold. For instance, we might label any customer with a posterior probability of default above 20% to the *default* class:

$$
P(\text{default} = \text{Yes} \mid X = x) > 0.2
$$

The error rates that result from taking this approach are shown in Table 4.5. Now LDA predicts that 430 individuals will default. Of the 333 individuals who default, LDA correctly predicts all but 138, or 41.4 %. This is a vast improvement over the error rate of 75.7% that resulted from using the threshold of 50%. However, this improvement comes at a cost: now 235 individuals who do not default are incorrectly classified. As a result, the overall error rate has increased slightly to 3.73 %.

![](files/images/4-confusion-matrix2.png){width="50%"}

Figure 4.7 illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. Various error rates are shown as a function of the threshold value. How can we decide which threshold value is best? Such a decision must be based on *domain knowledge*, such as detailed information about the costs associated with default.

Increasing threshold to say 80% causes:

- **Sensitivity to DECREASE and specificity to INCREASE ([good source, for modelling too]([https://workshops.tidymodels.org/slides/intro-04-evaluating-models.html#/two-class-data)).**

- This means higher proportion of true negatives, with the cost of lower proportion of true positives.

- Predict more as no (harder to predict as yes), so capture more true no's, but miss some of the yes's.

- Just flip below: False positive rate decreases, but the false negative rate increases.

Thus, decreasing the threshold:

- Just flip above: Sensitivity increases and specificity decreases.

- Error rates:

    - Decreases the false negative rate (error rate among defaulters), because more confident in the negatives.
    
    - But increases the false positive rate (error rate among non-defaulters), because easier to be classified as yes.
    
    - Thus increasing the threshold **

![](files/images/4-comparing-thresholds.png){width="50%"}

The *ROC curve* is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. The overall performance of a classifier, summarized over all possible thresholds, is given by the *area under the (ROC) curve* (AUC). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. We expect a classifier that performs no better than chance to have an AUC of 0.5 (when evaluated on an independent test set not used in model training). ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds. As we have seen above, varying the classifier threshold changes its true positive and false positive rate. These are also called the *sensitivity* and one minus the *specificity* of our classifier.

![](files/images/4-roc-curve.png){width="50%"}

Here is a summary of the terms in this section. To make the connection with the epidemiology literature, we think of "+" as the "disease" that we are trying to detect, and "−" as the "non-disease" state. To make the connection to the classical hypothesis testing literature, we think of "−" as the null hypothesis and "'+" as the alternative (non-null) hypothesis. In the context of the Default data, "+" indicates an individual who defaults, and "−" indicates one who does not.

![](files/images/4-definitions.png){width="50%"}

- Sensitivity = true positive rate

- Specificity = true negative rate

#### Quadratic discriminant analysis

As we have discussed, LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a covariance matrix that is common to all $K$ classes. *Quadratic discriminant analysis* (QDA) provides an alternative approach. Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction. However, unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form $X \sim N(\mu_k,\Sigma_k)$, where $\Sigma_k$ is a covariance matrix for the $k$th class. Under this assumption, the Bayes classifier assigns an observation $X = x$ to the class for which

![](files/images/4-qda-formula.png){width="50%"}

is the largest. Unlike before, the quantity $x$ appears as a quadratic function now.

Why does it matter whether or not we assume that the K classes share a common covariance matrix? In other words, why would one prefer LDA to QDA, or vice-versa? The answer lies in the bias-variance trade-off. When there are $p$ predictors, then estimating a covariance matrix requires estimating $p(p+1)/2$ parameters (lower triangle + diagonal). QDA estimates a separate covariance matrix for each class, for a total of $Kp(p+1)/2$ parameters. With 50 predictors this is some multiple of 1,275, which is a lot of parameters. By instead assuming that the $K$ classes share a common covariance matrix, the LDA model becomes linear in $x$, which means there are $Kp$ linear coefficients to estimate. Consequently, LDA is a much less flexible classifier than QDA, and so has substantially lower variance. This can potentially lead to improved prediction performance. But there is a trade-off: if LDA’s assumption that the $K$ classes share a common covariance matrix is badly off, then LDA can suffer from high bias. **Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the $K$ classes is clearly untenable.**

<!-- ??? how to estimate sigma for LDA? why p parameters -->

Figure 4.9 illustrates the performances of LDA and QDA in two scenarios. In the left-hand panel, the two Gaussian classes have a common correlation of 0.7 between $X_1$ and $X_2$. As a result, the Bayes decision boundary is linear and is accurately approximated by the LDA decision boundary. The QDA decision boundary is inferior, because it suffers from higher variance without a corresponding decrease in bias. In contrast, the right-hand panel displays a situation in which the orange class has a correlation of 0.7 between the variables and the blue class has a correlation of −0.7. Now the Bayes decision boundary is quadratic, and so QDA more accurately approximates this boundary than does LDA.

![](files/images/4-qda-example.png){width="50%"}

#### Naive Bayes

Here, we use Bayes’ theorem to motivate the popular naive Bayes classifier. Recall that Bayes’ theorem @eq-bayes-theorem provides an expression for the posterior probability $p_k(x) = P(Y = k \mid X = x)$ in terms of $\pi_1, \ldots, \pi_K$ and $f_k(x), \ldots, f_K(x)$. To use this in practice, we need estimates for for these terms. Estimating the prior probabilities $\pi_k$ is typically straightforward.

However estimating $f_k(x)$ is more subtle. Recall that $f_k(x)$ is the $p$-dimensional density function for an observation in the $k$th class, for $k = 1, \ldots, K$. In general, estimating a $p$-dimensional density function is challenging. In LDA, we make a very strong assumption that greatly simplifies the task: we assume that $f_k$ is the density function for a multivariate normal random variable with class-specific mean $\mu_k$, and shared covariance matrix $\Sigma$. By contrast, in QDA we change it to have class-specific covariance matrix $\Sigma_k$. By making these very strong assumptions, we are able to replace the very challenging problem of estimating $K$ $p$-dimensional density functions with the much simpler problem of estimating $K$ $p$-dimensional mean vectors and one (in the case of LDA) or $K$ (in the case of QDA) $(p \times p)$-dimensional covariance matrices.

The naive Bayes classifier takes a different tack for estimating $f_1(x), \ldots, f_K(x)$. Instead of assuming that these functions belong to a particular family of distributions (e.g. multivariate normal), we instead make a single assumption: 

**Within the kth class, the $p$ predictors are independent.**

Stated mathematically, this assumption means that for $k = 1, \ldots, K$, 

$$
f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times \cdots \times f_{kp}(x_p)
$${#eq-naive-bayes-assumption}

where $f_{kj}$ is the density function of the $j$th predictor among observations in the $k$th class.

Why is this assumption so powerful? Essentially, estimating a $p$-dimensional density function is challenging because we must consider not only the marginal distribution of each predictor -- that is, the distribution of each predictor on its own -- but also the *joint distribution* of the predictors -- that is, the association between the different predictors. In the case of a multivariate normal distribution, the association between the different predictors is summarized by the off-diagonal elements of the covariance matrix. However, in general, this association can be very hard to characterize, and exceedingly challenging to estimate. But by assuming that the $p$ covariates are independent within each class, we completely eliminate the need to worry about the association between the $p$ predictors, because we have simply assumed that there is no association between the predictors!

Do we really believe the naive Bayes assumption that the $p$ covariates are independent within each class? In most settings, we do not. **But even though this modeling assumption is made for convenience, it often leads to pretty decent results, especially in settings where $n$ is not large enough relative to $p$ for us to effectively estimate the joint distribution of the predictors within each class. In fact, since estimating a joint distribution requires such a huge amount of data, naive Bayes is a good choice in a wide range of settings.** Essentially, the naive Bayes assumption introduces some bias, but reduces variance, leading to a classifier that works quite well in practice as a result of the bias-variance trade-off.

Once we have made the naive Bayes assumption, we can plug @eq-naive-bayes-assumption into @eq-bayes-theorem to obtain an expression for the posterior probability,

$$
P(Y = k \mid X = x) = \frac{\pi_k \times f_{k1}(x_1) \times \cdots \times f_{kp}(x_p)}{\sum_{l = 1}^K \pi_l \times f_{l1}(x_1) \times \cdots \times f_{lp}(x_p)}
$$

for $k = 1, \ldots, K$.

To estimate the one-dimensional density function $f_{kj}$ using training data
$x_{1j}, \cdots, x_{nj}$, we have a few options.

- If $X_j$ is quantitative, then we can assume that $X_j \mid Y = k \sim N(\mu_{jk}, \sigma_{jk}^2)$. In other words, we assume that within each class, the $j$th predictor is drawn from a (univariate) normal distribution. While this may sound a bit like QDA, there is one key difference, in that here we are assuming that the predictors are independent; this amounts to QDA with an additional assumption that the class-specific covariance matrix is diagonal.

- If $X_j$ is quantitative, then another option is to use a non-parametric estimate for $f_{kj}$. A very simple way to do this is by making a histogram for the observations of the $j$th predictor within each class. Then we can estimate $f_{kj}(x_j)$ as the fraction of the training observations in the $k$th class that belong to the same histogram bin as $x_j$. Alternatively, we can use a kernel density estimator, which is essentially a smoothed version of a histogram.

- If $X_j$ is qualitative, then we can simply count the proportion of training observations for the $j$th predictor corresponding to each class. For instance, suppose that $X_j \in \{1, 2, 3\}$, and we have 100 observations in the $k$th class. Suppose that the $j$th predictor takes on values of 1, 2, and 3 in 32, 55, and 13 of those observations, respectively. Then we can estimate $f_{kj}$ as

$$
\hat{f}_{kj}(x_j) = 
\begin{cases}
0.32 & \text{if } x_j = 1\\
0.55 & \text{if } x_j = 2\\
0.13 & \text{if } x_j = 3\\
\end{cases}
$$

Just as with LDA, we can easily adjust the probability threshold for predicting a default. In this example, it should not be too surprising that naive Bayes does not convincingly outperform LDA: this data set has $n = 10,000$ and $p = 4$, and so the reduction in variance resulting from the naive Bayes assumption is not necessarily worthwhile. We expect to see a greater pay-off to using naive Bayes relative to LDA or QDA in instances where $p$ is larger or $n$ is smaller, so that reducing the variance is very important.

### A comparison of classification methods

#### An analytical comparison

We now perform an *analytical* (or mathematical) comparison of LDA, QDA, naive Bayes, and logistic regression. We consider these approaches in a setting with K classes, so that we assign an observation to the class that maximizes $P(Y = k \mid X = x)$. Equivalently, we can set $K$ as the baseline class and assign an observation to the class that maximizes

$$
\log\Big(\frac{P(Y = k \mid X = x)}{P(Y = K \mid X = x)}\Big)
$$

for $k = 1, \ldots, K$. Examining the specific form of this equation for each method provides a clear understanding of their similarities and differences.

![](files/images/4-lda-derivation.png){width="50%"}

where $a_k = \log\Big(\frac{\pi_k}{\pi_K}\Big) - \frac{1}{2}(\mu_k + \mu_K)^T \Sigma^{-1}(\mu_k - \mu_K)$ and $b_{kj}$ is the $j$th component of $\Sigma^{-1}(\mu_k - \mu_K)$. **Hence LDA, like logistic regression, assumes that the log odds of the posterior probabilities is linear in $x$.**

Using similar calculations, in the QDA setting this becomes

![](files/images/4-qda-derivation.png){width="50%"}

Again, as the name suggests, QDA assumes that the log odds of the posterior probabilities is quadratic in $x$.

Finally with naive Bayes setting, we get

![](files/images/4-naive-bayes-derivation.png){width="50%"}

where $a_k = \log\big(\frac{\pi_k}{\pi_K}\big)$ and $g_{kj}(x_j) = \log\big(\frac{f_{kj}(x_j)}{f_{Kj}(x_j)}\big)$. Hence, the right-hand side of above takes the form of a *generalized additive model*, which will be discussed later.

Inspection of the above results yields the following observations about LDA, QDA, and naive Bayes:

- LDA is a special case of QDA with $c_{kjl} = 0$ for all $j = 1, \ldots, p, l = 1, \ldots, p$, and $k = 1, \ldots, K$. (Of course, this is not surprising, since LDA is simply a restricted version of QDA with $\Sigma_1 = \cdots = \Sigma_K = \Sigma$.)

- Any classifier with a linear decision boundary is a special case of naive Bayes with $g_{kj}(x_j) = b_{kj}x_j$. In particular, this means that LDA is a special case of naive Bayes! This is not at all obvious from the descriptions of LDA and naive Bayes earlier in the chapter, since each method makes very different assumptions: LDA assumes that the features are normally distributed with a common within-class covariance matrix, and naive Bayes instead assumes independence of the features.

- If we model $f_{kj}(x_j)$ in the naive Bayes classifier using a one-dimensional Gaussian distribution $N(\mu_{kj}, \sigma_j^2)$, then we end up with $g_{kj}(x_j) = b_{kj}x_j$ where $b_{kj} = (\mu_{kj} - \mu_{Kj}) / \sigma_j^2$. In this case, naive Bayes is actually a special case of LDA with $\Sigma$ restricted to be a diagonal matrix with $j$th diagonal element equal to $\sigma_j^2$.

- Neither QDA nor naive Bayes is a special case of the other. Naive Bayes can produce a more flexible fit, since any choice can be made for $g_{kj}(x_j)$. However, it is restricted to a purely additive fit, in the sense that in the final derivation, a function of $x_j$ is added to a function of $x$, for $j \ne l$; however, these terms are never multiplied. By contrast, QDA includes multiplicative terms of the form $c_{kjl} x_j x_l$. **Therefore, QDA has the potential to be more accurate in settings where interactions among the predictors are important in discriminating between classes.**

None of these methods uniformly dominates the others: in any setting, the choice of method will depend on the true distribution of the predictors in each of the $K$ classes, as well as other considerations, such as the values of $n$ and $p$. The latter ties into the bias-variance trade-off.

How does logistic regression tie into this story? Recall from @eq-multinomial-logreg that multinomial logistic regression takes the form

$$
\log\Big(\frac{P(Y = k \mid X = x)}{P(Y = K \mid X = x)}\Big) = \beta_{k0} + \sum_{j=1}^p \beta_{kj} x_j
$$

This is identical to the *linear form* of LDA the derivation: in both cases $\log\Big(\frac{P(Y = k \mid X = x)}{P(Y = K \mid X = x)}\Big) $ is a linear function of the predictors. In LDA, the coefficients in this linear function are functions of estimates for $\pi_k$, $\pi_K$, $\mu_k$, $\mu_K$, and $\Sigma$ obtained by assuming that $X_1,\ldots,X_p$ follow a normal distribution within each class. By contrast, in logistic regression, the coefficients are chosen to maximize the likelihood function @eq-logistic-likelihood. **Thus, we expect LDA to outperform logistic regression when the normality assumption (approximately) holds, and we expect logistic regression to perform better when it does not.**

We close with a brief discussion of $K$-nearest neighbors (KNN), introduced in earlier. Recall that KNN takes a completely different approach from the classifiers seen in this chapter. In order to make a prediction for an observation $X = x$, the training observations that are closest to $x$ are identified. Then $X$ is assigned to the class to which the plurality of these observations belong. Hence KNN is a completely non-parametric approach: no assumptions are made about the shape of the decision boundary. We make the following observations about KNN:

- **Because KNN is completely non-parametric, we can expect this ap- proach to dominate LDA and logistic regression when the decision boundary is highly non-linear, provided that n is very large and p is small.**

- **In order to provide accurate classification, KNN requires a lot of observations relative to the number of predictors** -- that is, $n$ much larger than $p$. This has to do with the fact that KNN is non-parametric, and thus tends to reduce the bias while incurring a lot of variance.

- **In settings where the decision boundary is non-linear but $n$ is only modest, or $p$ is not very small, then QDA may be preferred to KNN**. This is because QDA can provide a non-linear decision boundary while taking advantage of a parametric form, which means that it requires a smaller sample size for accurate classification, relative to KNN.

- Note that the of $K$ in KNN is really important and is often chosen via *cross-validation*.

- Unlike logistic regression, KNN does not tell us which predictors are important: we don’t get a table of coefficients as in Table 4.3.

#### An empirical comparison

Checkout section 4.5.2 of textbook for some simulation results. This could help when determining which model to use for a particular situation.

- **When the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well.**

- **When the boundaries are moderately non-linear, QDA or naive Bayes may give better results.**

- **Finally, for much more complicated decision boundaries, a non-parametric approach such as KNN can be superior. But the level of smoothness for a non-parametric approach must be chosen carefully.**

In the next chapter we examine a number of approaches for choosing the correct level of smoothness and, in general, for selecting the best overall method.

Finally, recall from the previous chapter that in the regression setting we can accommodate a non-linear relationship between the predictors and the response by performing regression using transformations of the predictors. A similar approach could be taken in the classification setting. For instance, we could create a more flexible version of logistic regression by including $X^2$, $X^3$, and even $X^4$ as predictors. This may or may not improve logistic regression's performance, depending on whether the increase in variance due to the added flexibility is offset by a sufficiently large reduction in bias. We could do the same for LDA. If we added all possible quadratic terms and cross-products to LDA, the form of the model would be the same as the QDA model, although the parameter estimates would be different. This device allows us to move somewhere between an LDA and a QDA model.

### Generalized linear models

In the previous chapter, we assumed that the response $Y$ is quantitative, and explored the use of least squares linear regression to predict $Y$. Thus far in this chapter, we have instead assumed that $Y$ is qualitative. However, we may sometimes be faced with situations in which $Y$ is neither qualitative nor quantitative, and so neither linear regression nor the classification approaches  is applicable.

This occurs when we are working with *count* data (non-negative integers). If we fit linear regression to count data, here are some issues that can occur:

- Can predict negative counts. This calls into question our ability to perform meaningful predictions on the data, and it also raises concerns about the accuracy of the coefficient estimates, confidence intervals, and other outputs of the regression model.

- Furthermore, it is reasonable to suspect that with the expected value of the response is small, the variance of the response should be small as well; however, when the expected value of counts is large, the variance should increase as well. This is a major violation of the assumptions of a linear model, which state that $Y = \sum_{j=1}^p X_j \beta_j + \epsilon$, where $\epsilon$ is a mean-zero error term with variance $\sigma^2$ that is constant, and not a function of the covariates. Therefore, the heteroscedasticity of the data calls into question the suitability of a linear regression model. This is bad:

![](files/images/4-error-variance-violation.png){width="50%"}

- Finally, with a continuous-valued error term, we can get a continuous-valued response, which doesn't match the scenario.

Some of the problems that arise when fitting a linear regression model to count data can be overcome by transforming the response; for instance, we can fit the model

$$
\log(Y) = \sum_{j=1}^p X_j \beta_j = \epsilon
$$

Transforming the response avoids the possibility of negative predictions, and it overcomes much of the heteroscedasticity in the untransformed data, as is shown in the right-hand panel of Figure 4.14. However, it is not quite a satisfactory solution, since predictions and inference are made in terms of the log of the response, rather than the response. This leads to challenges in interpretation, e.g. "a one-unit increase in $X_j$ is associated with an increase in the mean of the log of $Y$ by an amount $\beta_j$". Furthermore, a log transformation of the response cannot be applied in settings where the response can take on a value of 0. Thus, while fitting a linear model to a transformation of the response may be an adequate approach for some count-valued data sets, it often leaves something to be desired. We will see in the next section that a Poisson regression model provides a much more natural and elegant approach for this task.

#### Poisson regression

To overcome the inadequacies of linear regression for analyzing count data, we will make use of an alternative approach, called *Poisson regression*. Recall if $Y \follow{Poisson}(\lambda)$, then 

$$
P(Y = k) = \frac{\e^{-\lambda}\lambda^k}{k!}, \quad \text{for } k = 1, 2, \ldots
$${#eq-poisson}

Here, $\lambda > 0$ is the expected value of $Y$, i.e. $E(Y)$. It turns out that $\lambda$ also equals the variance of $Y$ , i.e. $\lambda = E(Y) = V(Y)$. This means that if $Y$ follows the Poisson distribution, then the larger the mean of $Y$, the larger its variance. 

The Poisson distribution is typically used to model counts. To see how we
might use the Poisson distribution in practice, let $Y$ denote the number of
users of the bike sharing program during a particular hour of the day, under
a particular set of weather conditions, and during a particular month of the
year. We might model $Y$ as a Poisson distribution with mean $E(Y) = \lambda = 5$.
This means that the probability of no users during this particular hour is
$P(Y = 0) = \frac{\e^{-5} 5^0}{0!} = e^{-5} = 0.0067$. Of course, in reality, we expect the mean number of users of the bike sharing program, $\lambda = E(Y)$, to vary as a function of the hour of the day, the month of the year, the weather conditions, and so forth. So rather than modeling the number of bikers, $Y$, as a Poisson distribution with a *fixed* mean value like $\lambda = 5$, we would like to *allow the mean to vary as a function of the covariates*. In particular, we consider the following model for the mean $\lambda = E(Y)$, which we now write as $\lambda(X_1, \ldots, X_p)$ to emphasize that it is a function of the covariates $X_1, \ldots, X_p$:

$$
\log(\lambda(X_1, \ldots, X_p)) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
$${#eq-log-lambda}

or equivalently

$$
\lambda(X_1, \ldots, X_p) = \e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}
$${#eq-lambda}

Here, $\beta_0, \beta_1, \ldots, \beta_p$ are parameters to be estimated. Together, @eq-poisson and @eq-log-lambda define the Poisson regression model. Notice that in @eq-log-lambda, we take the *log* of $\lambda(X_1, \ldots, X_p)$ to be linear in $X_1, \ldots, X_p$, in order to ensure that $\lambda(X_1, \ldots, X_p)$ takes on nonnegative values for all values of the covariates.

To estimate the coefficients $\beta_0, \beta_1, \ldots, \beta_p$, we use the use the same maximum likelihood approach that we adopted for logistic regression. Specifically, given $n$ independent observations from the Poisson regression model, the likelihood takes the form

$$
\ell(\beta_0, \beta_1, \ldots, \beta_p) = \prod_{i = 1}^n \frac{e^{-\lambda(x_i)} \lambda(x_i)^{y_i}}{y_i!}
$$

where $\lambda(x_i) = \e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}$ due to @eq-lambda. We estimate the coefficients that maximize the likelihood $\ell(\beta_0, \beta_1, \ldots, \beta_p)$, i.e. that make the observed data as likely as possible.

Some important distinctions between the Poisson regression model and the linear regression model are as follows:

- *Interpretation*: To interpret the coefficients in the Poisson regression model, we must pay close attention to @eq-lambda, which states that an increase in $X_j$ by one unit is associated with a change in $E(Y) = \lambda$ by a factor of $\e^{\beta_j}$. For example, a change in weather from clear to cloudy skies is associated with a change in mean bike usage by a factor of $\exp(−0.08) = 0.923$, i.e. on average, only 92.3% as many people will use bikes when it is cloudy relative to when it is clear.

- *Mean-variance relationship*: As mentioned earlier, under the Poisson model, $\lambda = E(Y) = V(Y)$. Thus, by modeling bike usage with a Poisson regression, we implicitly assume that mean bike usage in a given hour equals the variance of bike usage during that hour. By contrast, under a linear regression model, the variance of bike usage always takes on a constant value.

- *nonnegative fitted values*: There are no negative predictions using the Poisson regression model.

#### Generalized linear models in greater generality

We have now discussed three types of regression models: linear, logistic and Poisson. These approaches share some common characteristics:

- Each approach uses predictors $X_1, \ldots, X_p$ to predict a response $Y$. We assume that, conditional on $X_1, \ldots, X_p$, $Y$ belongs to a certain family of distributions. For linear regression, we typically assume that $Y$ follows a Gaussian or normal distribution. For logistic regression, we assume that $Y$ follows a Bernoulli distribution. Finally, for Poisson regression, we assume that $Y$ follows a Poisson distribution.

- Each approach models the mean of $Y$ as a function of the predictors. In linear regression, the mean of $Y$ takes the form

$$
E(Y \mid X_1, \ldots, X_p) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
$$

i.e., it is a linear function of the predictors. For logistic regression, the mean instead takes the form 

$$
\begin{align*}
E(Y \mid X_1, \ldots, X_p) &= P(Y = 1 \mid X_1, \ldots, X_p)\\
 &= \frac{\e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}{1 + \e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}
\end{align*}
$$

(this is because mean(Bernoulli) = $p$ = ... < logistic function >) while for Poisson regression it takes the form

$$
E(Y \mid X_1, \ldots, X_p) = \lambda(X_1, \ldots, X_p) = \e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}
$$

All of these above equations can be expressed using a *link function*, $\eta$, which applies a transformation to $E(Y \mid X_1, \ldots, X_p) $ so that the transformed mean is a linear function of the predictors. That is,

$$
\eta(E(Y \mid X_1, \ldots, X_p)) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
$${#eq-link-function}

The link functions for linear, logistic and Poisson regression are $\eta(\mu) = \mu$, $\eta(\mu) = \log(\mu/(1 − \mu))$, and $\eta(\mu) = \log(\mu)$, respectively.

The Gaussian, Bernoulli and Poisson distributions are all members of a wider class of distributions, known as the *exponential family*. Other well-known members of this family are the exponential distribution, the Gamma distribution, and the negative binomial distribution. In general, we can perform a regression by modeling the response Y as coming from a particular member of the exponential family, and then transforming the mean of the response so that the transformed mean is a linear function of the predictors via @eq-link-function. Any regression approach that follows this very general recipe is known as a generalized linear model (GLM). Thus, linear regression, logistic regression, and Poisson regression are three examples of GLMs. Other examples not covered here include Gamma regression and negative binomial regression.

## Lab

### Load data

```{r}

# load data
data_stock <- ISLR2::Smarket
colnames(data_stock)

# view correlations
data_stock %>% 
  select(where(is.numeric)) %>% 
  as.matrix %>% 
  cor %>% 
  corrplot::corrplot()

```

### Logisitic regression

Lets fit the full model and view some model summaries.

```{r}

# load packages
library(magrittr)

# fit logistic regression model
mod_logreg <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                  data = data_stock,
                  family = "binomial")

# view model summary
summary(mod_logreg)

# view coding of response
# -> thus model is predicting P(Direction = Up | X = x)
contrasts(data_stock$Direction)

# get model fits for probabilities
# -> base R
# -> to get the fitted log odds, use type = "link"
preds_logreg <- predict(mod_logreg, type = "response")
# -> broom
# --> uses the same arguments as type for .fitted
# --> then classify
preds_logreg <- broom::augment(mod_logreg, type.predict = "response") %>% 
  select(Direction, .fitted) %>% 
  mutate(predicted = if_else(.fitted > 0.5, "Up", "Down") %>% factor(levels = c("Down", "Up")))

# create confusion matrix
# -> base R
preds_logreg %$% table(predicted, 
                     Direction,
                     dnn = c("predicted", "actual"))
# -> tidymodels
(c_mat <- yardstick::conf_mat(preds_logreg %>% 
                      mutate(predicted = as.factor(predicted)),
                    truth = "Direction",
                    estimate = "predicted"))
summary(c_mat)

# calculate accuracy and error rate = 1 - accuracy
# -> base R
mean(preds_logreg$predicted == data_stock$Direction)
1 - mean(preds_logreg$predicted == data_stock$Direction)

```

Now repeat analysis, but use a holdout sample.

```{r}

# sample data 
data_train <- data_stock %>% 
  filter(Year < 2005)
data_test <- data_stock %>% 
  filter(Year >= 2005)

# fit logistic regression model on training data
mod_logreg2 <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                   data = data_train,
                   family = "binomial")

# predict on holdout data
preds_logreg2 <- broom::augment(mod_logreg2,
                              newdata = data_test,
                              type.predict = "response") %>% 
  select(Direction, .fitted) %>% 
  mutate(predicted = if_else(.fitted > 0.5, "Up", "Down") %>% factor(levels = c("Down", "Up")))

# view results
preds_logreg2 %>% 
  yardstick::conf_mat(truth = "Direction",
                      estimate = "predicted") #%>% 
  #summary(event_level = "second")

```

Now we can do some model selection to get a better model. **After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement.**

```{r}

# fit smaller model
mod_logreg3 <- glm(Direction ~ Lag1 + Lag2,
                   data = data_train,
                   family = "binomial")

# view model summary
mod_logreg3 %>% broom::glance()

# predict on holdout data
preds_logreg3 <- broom::augment(mod_logreg3,
                              newdata = data_test,
                              type.predict = "response") %>% 
  select(Direction, .fitted) %>% 
  mutate(predicted = if_else(.fitted > 0.5, "Up", "Down") %>% factor(levels = c("Down", "Up")))

# view results
preds_logreg3 %>% 
  yardstick::conf_mat(truth = "Direction",
                      estimate = "predicted") %>%
  summary(event_level = "second")

# calculate accuracy of naive approach, which is just predicting yes everyday
# -> get also numbers from truth confusion matrix
data_test %>% 
  summarize(mean(Direction == "Up"))

```

### Linear discriminant analysis

```{r}

# fit LDA model
mod_lda <- MASS::lda(Direction ~ Lag1 + Lag2, data = data_train)

# view model
mod_lda

# plot model
# -> produces plots of the linear discriminants, obtained by computing −0.642 × Lag1 − 0.514 × Lag2 for each of the training observations.
plot(mod_lda)

```

The *coefficients of linear discriminants* output provides the linear combination of Lag1 and Lag2 that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of X = x in @eq-bayes-decision-boundary3.

<!-- ??? could do more research into exactly what these are -->

If −0.642 × Lag1 − 0.514 × Lag2 is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.

```{r}

# predict
preds_lda <- predict(mod_lda, newdata = data_test)

# view prediction output
# -> class prediction
preds_lda$class %>% head
# -> posterior probabilities
preds_lda$posterior %>% head
# -> linear discriminants
preds_lda$x %>% head

# analyze predictions
table(preds_lda$class,
      data_test$Direction,
      dnn = c("predicted", "Direction"))

# calculate accuracy
mean(preds_lda$class == data_test$Direction)

```

### Quadratic discriminant analysis

```{r}

# fit QDA model
mod_qda <- MASS::qda(Direction ~ Lag1 + Lag2, data = data_train)

# view model
# -> now output does not contain the coefficients of the linear discriminants, because no longer linear
mod_qda

# predict
preds_qda <- predict(mod_qda, newdata = data_test)

# analyze predictions
table(preds_qda$class,
      data_test$Direction,
      dnn = c("predicted", "Direction"))

# calculate accuracy
mean(preds_qda$class == data_test$Direction)

```

A higher accuracy for the test data suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression.

### Naive Bayes 

```{r}

# fit model
# -> by default, this models each quantitative feature with a Gaussian distribution
# --> but a kernal density method can also be used to estimate the distributions
mod_nb <- e1071::naiveBayes(Direction ~ Lag1 + Lag2, data = data_train)

# view model output
mod_nb

# predict
preds_nb <- predict(mod_nb, newdata = data_test)

# analyze predictions
table(preds_nb,
      data_test$Direction,
      dnn = c("predicted", "Direction"))

# calculate accuracy
mean(preds_nb == data_test$Direction)

```

Naive Bayes performs very well on this data, with accurate predictions over 59% of the time. This is slightly worse than QDA, but much better than LDA.

### KNN

```{r}

# fit model
# -> this function forms prediction in a single command (not in two steps like the other models: fit then predict)
# -> NOTE: if there is a tie (say k = 2 and one observation from each class), then R randomly breaks the tie
# --> so need to set seed if want reproducibility
preds_knn1 <- class::knn(train = data_train %>% select(Lag1, Lag2) %>% as.matrix, # predictors of train set as matrix
                      test = data_test %>% select(Lag1, Lag2) %>% as.matrix, # predictors of test set as matrix
                      cl = data_train$Direction, # truth for train set
                      k = 1)

# analyze predictions
table(preds_knn1,
      data_test$Direction,
      dnn = c("predicted", "Direction"))

# calculate accuracy
mean(preds_knn1 == data_test$Direction)

# overfitting...

# refit with better k
preds_knn3 <- class::knn(train = data_train %>% select(Lag1, Lag2) %>% as.matrix, # predictors of train set as matrix
                      test = data_test %>% select(Lag1, Lag2) %>% as.matrix, # predictors of test set as matrix
                      cl = data_train$Direction, # truth for train set
                      k = 3)

# analyze predictions
table(preds_knn3,
      data_test$Direction,
      dnn = c("predicted", "Direction"))

# calculate accuracy
mean(preds_knn3 == data_test$Direction)


```

The results have improved slightly. But increasing $K$ further turns out to provide no further improvements.

It appears that for this data, QDA provides the best results of the methods that we have examined so far.

Notes about KNN in general:

- Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the *distance* between the observations, and hence on the KNN classifier, than variables that are on a small scale.

- For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of $1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. Same for scale of the response, changing units from dollars to cents will lead to different classification results.

- **A good way to handle this problem is to *standardize* the data so that all variables are given a mean of zero and a standard deviation of one. Can use `scale()` to accomplish this.**

When using classifying methods in general, can look at the naive approach of guessing all positives as a baseline.

### Poisson regression

First, try to fit a linear regression model.

```{r}

# load data
data_bike <- ISLR2::Bikeshare

# fit linear regression model
mod_lr <- lm(bikers ~ mnth + hr + workingday + temp + weathersit, data = data_bike)

# view model summary
summary(mod_lr)
head(coef(mod_lr))

# view predictions #check negative ones
sum(predict(mod))
```

Now we can change contrasts so can get a coefficient estimate for every level of the predictors.

- [Contrasts](https://learnb4ss.github.io/learnB4SS/articles/contrasts.html) are an attribute of the column.

- Default coding is dummy variable (aka treatment contrasts), where coefficients are set relative to the reference level.

- For sum contrasts, all contrasts sum to 0 for each dummy variable and the reference level is in fact the grand mean. Also, now coefficients are the difference relative to the grand mean (which is now the intercept).

- Lastly for sum contrasts, the last coefficient isn't given in the output, but it can be easily calculated: *it is the negative sum of the coefficient estimates for all of the other levels*.

```{r}

# set new contrasts
contrasts(data_bike$hr) <- "contr.sum"
contrasts(data_bike$mnth) <- "contr.sum"

# refit model
mod_lr2 <- lm(bikers ~ mnth + hr + workingday + temp + weathersit, data = data_bike)

# view model summary
summary(mod_lr2)
coef(mod_lr2) %>% {c(head(.), tail(.))}

# create plot of the coefficients for one of the factor variables
coef(mod_lr2)[2:12] %>% 
  {c(., -sum(.))} %>% 
  plot(type = "b", xlab = "Month", ylab = "Coefficient")

```

Now we can fit a Poisson regression model.

```{r}

# fit poisson regression model
mod_pois <- glm(bikers ~ mnth + hr + workingday + temp + weathersit,
                data = data_bike,
                family = "poisson")

# view model summary
summary(mod_pois)

# plot estimated coefficients (still using sum contrasts)
coef(mod_pois)[2:12] %>% 
  {c(., -sum(.))} %>% 
  plot(type = "b", xlab = "Month", ylab = "Coefficient")

# make predictions
preds_pois <- predict(mod_pois, type = "response")

```

## Exercises

### Conceptual

#### Question 1

> Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.

< already showed in notes >

#### Question 2

> It was stated in the text that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest. Prove that this is the case. In other words, under the assumption that the observations in the $k$th class are drawn from a $N(\mu_k,\sigma^2)$ distribution, the Bayes' classifier assigns an observation to the class for which the discriminant function is maximized.

![](files/images/4-q2.png)

#### Question 3

> This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where $p = 1$; i.e. there is only one feature.

> Suppose that we have $K$ classes, and that if an observation belongs to the kth class then $X$ comes from a one-dimensional normal distribution, $X \sim N(\mu_k,\sigma^2_k)$. Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is *not* linear. Argue that it is in fact quadratic.

> *Hint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that $\sigma_1^2 = ... = \sigma_K^2$.*

![](files/images/4-q3.png)

#### Question 4

> When the number of features $p$ is large, there tends to be a deterioration in the performance of KNN and other *local* approaches that perform prediction using only observations that are *near* the test observation for which a prediction must be made. This phenomenon is known as the *curse of dimensionality*, and it ties into the fact that non-parametric approaches often perform poorly when $p$ is large. We will now investigate this curse.

> a. Suppose that we have a set of observations, each with measurements on $p = 1$ feature, $X$. We assume that $X$ is uniformly (evenly) distributed on $[0, 1]$. Associated with each observation is a response value. Suppose that we wish to predict a test observation's response using only observations that are within 10% of the range of $X$ closest to that test observation. For instance, in order to predict the response for a test observation with $X = 0.6$, we will use observations in the range $[0.55, 0.65]$. On average, what fraction of the available observations will we use to make the prediction?

For values in $[0,0.05]$, we use less than 10% of observations (between 5% and 10%, 7.5% on average), similarly with values in $[0.95,1]$. For values in $[0.05,0.95]$ we use 10% of available observations. The (weighted) average is then $7.5 \times 0.1 + 10 \times 0.9 = 9.75\%$.

> b. Now suppose that we have a set of observations, each with measurements on
$p = 2$ features, $X_1$ and $X_2$. We assume that $(X_1, X_2)$ are
uniformly distributed on $[0, 1] \times [0, 1]$. We wish to predict a test
observation's response using only observations that are within 10% of the
range of $X_1$ _and_ within 10% of the range of $X_2$ closest to that test
observation. For instance, in order to predict the response for a test
observation with $X_1 = 0.6$ and $X_2 = 0.35$, we will use observations in
the range $[0.55, 0.65]$ for $X_1$ and in the range $[0.3, 0.4]$ for $X_2$.
On average, what fraction of the available observations will we use to
make the prediction?

Since we need the observation to be within range for $X_1$ and $X_2$ we square
9.75% = $0.0975^2 \times 100 = 0.95\%$

> c. Now suppose that we have a set of observations on $p = 100$ features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation's response using observations within the 10% of each feature's range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?

Similar to above, we use: $0.0975^{100} \times 100 = 8 \times 10^{-100}\%$, essentially zero.

> d. Using your answers to parts (a)--(c), argue that a drawback of KNN when $p$ is large is that there are very few training observations "near" any given test observation.

As $p$ increases, the fraction of observations near any given point rapidly approaches zero. For instance, even if you use 50% of the nearest observations for each $p$, with $p = 10$, only $0.5^{10} \times 100 \approx 0.1\%$ points are "near".

> e. Now suppose that we wish to make a prediction for a test observation by creating a $p$-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For $p = $1,2, and 100, what is the length of each side of the hypercube? Comment on your answer. 

When $p = 1$, clearly the length is 0.1. When $p = 2$, we need the value $l$ such that $l^2 = 0.1$, so $l = \sqrt{0.1} \approx 0.32$. With $p$ variables, $l = 0.1^{1/p}$, so in the case of $p = 100$, $l = 0.977$. Therefore, the length of each side of the hypercube rapidly approaches 1 (or 100%) of the range of each $p$.

#### Question 5 

> We now examine the differences between LDA and QDA.

> a. If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?

QDA because because of overfitting, will always perform better on the training set. But because the decision boundary is linear, on the testing set LDA will perform better.

> b. If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?

QDA because because it is a more flexible model will perform better. And because the decision boundary is non-linear, on the testing set QDA will still perform better.

> c. In general, as the sample size $n$ increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?

Improve, will have more data points to take into account the added parameters from LDA (less overfitting), more data to pick up on smaller effects.

> d. True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.

Not necessarily. The decrease in error rate from flexibility may not offset the increase in bias due to overfitting.

#### Question 6

> Suppose we collect data for a group of students in a statistics class with variables $X_1 =$ hours studied, $X_2 =$ undergrad GPA, and $Y =$ receive an A. We fit a logistic regression and produce estimated coefficient, $\hat\beta_0 = -6$, $\hat\beta_1= 0.05$, $\hat\beta_2 = 1$.

> a. Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class.

> b. How many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?

![](files/images/4-q6.png)

#### Question 7

> Suppose that we wish to predict whether a given stock will issue a dividend this year ("Yes" or "No") based on $X$, last year's percent profit. We examine a large number of companies and discover that the mean value of $X$ for companies that issued a dividend was $\bar{X} = 10$, while the mean for those that didn't was $\bar{X} = 0$. In addition, the variance of $X$ for these two sets of companies was $\hat{\sigma}^2 = 36$. Finally, 80% of companies issued dividends. Assuming that $X$ follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was $X = 4$ last year.

> *Hint: Recall that the density function for a normal random variable is $f(x) =\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}$. You will need to use Bayes' theorem.*

![](files/images/4-q7.png)

#### Question 8

> Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. $K = 1$) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?

For $K = 1$, performance on the training set is perfect and the error rate
is zero, implying a test error rate of 36%. Logistic regression outperforms
1-nearest neighbor on the test set and therefore should be preferred.

#### Question 9


> This problem has to do with *odds*.

> a. On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?

> b. Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?

![](files/images/4-q9.png)

#### Question 10

> Equation 4.32 derived an expression for $\log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})$ in the setting where $p > 1$, so that the mean for the $k$th class, $\mu_k$, is a $p$-dimensional vector, and the shared covariance $\Sigma$ is a  $p \times p$ matrix. However, in the setting with $p = 1$, (4.32) takes a  simpler form, since the means $\mu_1, ..., \mu_k$ and the variance $\sigma^2$ are scalars. In this simpler setting, repeat the calculation in (4.32), and provide expressions for $a_k$ and $b_{kj}$ in terms of $\pi_k, \pi_K, \mu_k, \mu_K,$ and $\sigma^2$.

![](files/images/4-q10.png)

#### Question 11

> Work out the detailed forms of $a_k$, $b_{kj}$, and $b_{kjl}$ in (4.33). Your answer should involve $\pi_k$, $\pi_K$, $\mu_k$, $\mu_K$, $\Sigma_k$, and $\Sigma_K$.

< skipping >

#### Question 12

> Suppose that you wish to classify an observation $X \in \mathbb{R}$ into `apples` and `oranges`. You fit a logistic regression model and find that

> $$
> \hat{Pr}(Y=orange|X=x) = 
> \frac{\exp(\hat\beta_0 + \hat\beta_1x)}{1 + \exp(\hat\beta_0 + \hat\beta_1x)}
> $$

> Your friend fits a logistic regression model to the same data using the *softmax* formulation in (4.13), and finds that

> $$
> \hat{Pr}(Y=orange|X=x) = 
> \frac{\exp(\hat\alpha_{orange0} + \hat\alpha_{orange1}x)}
> {\exp(\hat\alpha_{orange0} + \hat\alpha_{orange1}x) + \exp(\hat\alpha_{apple0} + \hat\alpha_{apple1}x)}
> $$

> a. What is the log odds of `orange` versus `apple` in your model?

> b. What is the log odds of `orange` versus `apple` in your friend's model?

> c. Suppose that in your model, $\hat\beta_0 = 2$ and $\hat\beta_1 = −1$. What are the coefficient estimates in your friend's model? Be as specific as possible.

> d. Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates $\hat\alpha_{orange0} = 1.2$, $\hat\alpha_{orange1} = −2$, $\hat\alpha_{apple0} = 3$, $\hat\alpha_{apple1} = 0.6$. What are the coefficient estimates in your model?

> e. Finally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend's model? Explain your answer.

![](files/images/4-q12.png)

### Applied


#### Question 13

EDA

```{r}

# load data
data_weekly <- ISLR2::Weekly

# table for the response
data_weekly %>% 
  summarize(.by = Direction,
            n = n()) %>% 
  gt::gt() # try gtSummary to add total row

# correlation plot
data_weekly %>% 
  select(where(is.numeric)) %>% 
  as.matrix %>% 
  cor %>% 
  corrplot::corrplot()

# density plots of numeric predictors
data_weekly %>% 
  pivot_longer(cols = starts_with("Lag"),
               names_to = "lag",
               values_to = "value") %>% 
  ggplot() + 
  geom_density(aes(x = value,
                   color = lag)) + 
  facet_grid(Direction ~ .)

# line plot of number of increases and decreases per year
data_weekly %>% 
  group_by(Year, Direction) %>% 
  summarize(n = n()) %>% 
  ggplot(aes(x = Year,
                y = n,
                color = Direction)) + 
  geom_line() + 
  geom_smooth(se = FALSE)

# histograms of other predictors
# -> some by class of response
hist(data_weekly$Today)
data_weekly %>% 
  split(.$Direction) %>% 
  map(\(df) hist(df$Volume))

```

Observations

- Balanced-ish response
- Fluctuation in number of weeks with Up/Down by year, no real pattern though
- Lag variables are roughly normal within class; Volume is not (right skewed)
- Year and Volume have a strong, positive correlation

Now we can fit a logistic regression model

```{r}

# fit logistic regression model
mod_logreg <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
                  data = data_weekly,
                  family = "binomial")

# view model summary
broom::tidy(mod_logreg)

```

Only Lag2 is significant.

Analyze predictions on training data.

```{r}

# calculate confusion matrix
mod_logreg %>% 
  broom::augment(type.predict = "response") %>% 
  mutate(predicted = if_else(.fitted > 0.5, "Up", "Down") %>% as.factor) %>% 
  yardstick::conf_mat(truth = "Direction",
                      estimate = "predicted") %>%
  summary()

```

Model is misclassifying the true "Down"s at a high rate; model is not specific.

```{r}

# split data
data_train <- data_weekly %>% 
  filter(Year <= 2008)
data_test <- data_weekly %>% 
  filter(Year > 2008)

# define function to fit different types of models
# -> NOTE: only works with first order models
fit_model <- function(model = c("logreg", "lda", "qda", "knn", "nb"), formula, response_levels, df_train, df_test, threshold = 0.5, k = 1){
  
  # set items
  model = match.arg(model)
  
  # knn does model and prediction in one step
  if(identical(model, "knn")) {
    
    # extract string of predictors (and format as vector) and response from formula
    x = formula %>% str_sub(.,
                            start =  str_locate(., "~")[1]+2,
                            end = -1) %>% 
      data.frame(x = .) %>% 
      separate_wider_delim(cols = x,
                           delim = " + ",
                           names_sep = "") %>% 
      reduce(.f = c)
    y = formula %>% str_sub(.,
                            start = 1,
                            end = str_locate(., "~")[1]-2) 
    
    # fit model and calculate predictions
    pred_class = class::knn(train = df_train %>% select(any_of(x)) %>% as.matrix,
                            test = df_test %>% select(any_of(x)) %>% as.matrix,
                            cl = df_train %>% pull(any_of(y)),
                            k = k) %>% 
      data.frame(pred_class = .)
    
    # initialize empty dataframe for returning
    pred_prob = data.frame(pred_prob = rep(NA, length(pred_class)))
    
  }
  else{
    
    # set item
    formula = as.formula(formula)
    
    # fit model
    mod = if(identical(model, "logreg")){
      
      glm(formula, data = df_train, family = "binomial")
      
    }
    else if(identical(model, "lda")){
      
      MASS::lda(formula, data = df_train)
      
    }
    else if(identical(model, "qda")){
      
      MASS::qda(formula, data = df_train)
      
    }
    else if(identical(model, "nb")){
      
      e1071::naiveBayes(formula, data = df_train)

    }
    else{
      
      e1071::naiveBayes(formula, data = df_train)
    }
    
    # make predictions
    pred_prob = if(identical(model, "logreg")){
      
      predict(mod, type = "response", newdata = df_test)
      
    }
    else if(model %in% c("lda", "qda")){
      
      predict(mod, newdata = df_test)$posterior[,response_levels[2]]
      
    }
    else{
      
      predict(mod, type = "raw", newdata = df_test)[,response_levels[2]]

    }
    
    # classify based on predicted probability and threshold
    pred_class = ifelse(pred_prob > threshold, 1, 0) %>% 
      data.frame(pred_class = .) %>% 
      mutate(pred_class = case_when(pred_class == 1 ~ response_levels[2],
                                    .default = response_levels[1]) %>% factor(levels = response_levels))
  
  }
  
  bind_cols(pred_prob = pred_prob, pred_class) %>% return
  
}

# initialize items
models <- c("logreg", "lda", "qda", "knn", "nb")  
formula <- "Direction ~ Lag2"
k <- 1

# define function to calculate confusion matrix / summary statistics
# -> NOTE: preds is output of fit_model()
calc_conf_mat <- function(preds, df_test, truth, estimate) {
  
  df_test %>% 
      bind_cols(preds) %>% 
      yardstick::conf_mat(truth = truth,
                          estimate = estimate)
     
}

# fit models and assess quality
# -> good enough, could loop over a set of formulas
# -> just manually change formula / k above and look at results
# -> change event_level if needed
models %>% 
  set_names(x = ., nm = .) %>% 
  map(\(model) fit_model(model = model, formula = formula, response_levels = c("Down", "Up"), df_train = data_train, df_test = data_test, threshold = 0.5, k = k)) %>% 
  map(\(preds) calc_conf_mat(preds, df_test = data_test, truth = "Direction", estimate = "pred_class")) %>% 
  map(\(conf_mat) summary(conf_mat, event_level = "second")[1,]) %>% 
  reduce(bind_rows) %>% 
  bind_cols(data.frame(model = models,
                       formula = formula)) %>% 
  select(model, formula, .estimate) %>% 
  mutate(k = ifelse(model == "knn", k, NA),
         .after = formula)
  
```

Logistic regression and LDA are the best performing.

< Didn't experiment too much with different predictor sets / transformations >

#### Question 14

```{r}

# load data and modify
data_mpg <- ISLR2::Auto %>% 
  mutate(mpg01 = ifelse(mpg <= median(mpg), 0, 1) %>% as.factor)

# comparative boxplots of the response against each numeric X
nms_x <- data_mpg %>% 
  select(where(is.numeric), -mpg) %>% 
  colnames
map2(data_mpg %>% select(where(is.numeric), -mpg), nms_x, function(x, nm) {
  boxplot(x ~ mpg01, main = nm, data = data_mpg)
})

# scatterplot matrix
pairs(data_mpg %>% select(where(is.numeric)))

```

Displacement, horsepower, weight, and year appear to have a relationship with mpg.

Now we can split into train and test sets.

```{r}

# make test / train split
split_mpg <- rsample::initial_split(data = data_mpg, prop = .8)

# save train and data
data_train <- split_mpg %>% rsample::training()
data_test <- split_mpg %>% rsample::testing()

# fit all models and get error rates
models <- c("logreg", "lda", "qda", "knn", "nb")
k <- 7
models %>% 
  set_names(x = ., nm = .) %>% 
  map(\(model) fit_model(model = model, formula = "mpg01 ~ displacement + horsepower + weight + year", response_levels = c("0", "1"), df_train = data_train, df_test = data_test, threshold = 0.5, k = k)) %>% 
  map(\(preds) calc_conf_mat(preds, df_test = data_test, truth = "mpg01", estimate = "pred_class")) %>% 
  map(\(conf_mat) summary(conf_mat, event_level = "second")[1,]) %>% # pull accuracy (first row)
  reduce(bind_rows) %>% 
  mutate(error_rate = 1 - .estimate) %>%  # add error rates
  bind_cols(data.frame(model = models)) %>% 
  select(model, accuracy = .estimate, error_rate) %>% # rename summary of interest
  mutate(k = ifelse(model == "knn", k, NA),
         .after = model)

```

For the models tested here, $k = 7$ appears to perform best. QDA has a lower
error rate overall than LDA and logistic regression, but only slightly.

These results suggests the decision boundary is slightly more complicated than linear.

#### Question 15

< function writing >

#### Question 16

< another model fitting problem with a different dataset, save for learning tidy models >

```{r}


```