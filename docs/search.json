[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MAS I",
    "section": "",
    "text": "Overview\nThese notes are to prepare for Exam MAS I (modern actuarial statistics I), which covers three topics: probability models, statistics, and extended linear models.\n\n\n\n\n\n\n\n\nQuarto blog publish details\n\n\n\nThis book was created using Quarto and published with Github Pages.\n\n\n\n\n\n\n\n\nGithub repository for code\n\n\n\nYou can find the code to reproduce this project at coltongearhart/mas1.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "placeholder.html",
    "href": "placeholder.html",
    "title": "1  Placeholder",
    "section": "",
    "text": "1.1 Placeholder\nContent",
    "crumbs": [
      "Probability Models",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Placeholder</span>"
    ]
  },
  {
    "objectID": "islr-2.html",
    "href": "islr-2.html",
    "title": "\n3  ISLR – Statistical learning\n",
    "section": "",
    "text": "3.1 Notes",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ISLR -- Statistical learning</span>"
    ]
  },
  {
    "objectID": "islr-2.html#notes",
    "href": "islr-2.html#notes",
    "title": "\n3  ISLR – Statistical learning\n",
    "section": "",
    "text": "3.1.1 What is statistical learning?\nSuppose that we observe a quantitative response \\(Y\\) and \\(p\\) different predictors, \\(X_1, X_2, \\ldots , X_p\\). We assume that there is some relationship between \\(Y\\) and \\(X = (X_1, X_2, \\ldots, Xp)\\), which can be written in the very general form\n\\[\nY = f(X) + \\epsilon\n\\]\nHere \\(f\\) is some fixed but unknown function of \\(X_1, \\ldots , X_p\\), and \\(\\epsilon\\) is a random error term, which is independent of \\(X\\) and has mean zero. In this formulation, \\(f\\) represents the systematic information that \\(X\\) provides about \\(Y\\).\nIn essence, statistical learning refers to a set of approaches for estimating \\(f\\).\nWhy estimate \\(f\\)?\nThere are two main reasons that we may wish to estimate \\(f\\): prediction and inference.\nPrediction\nIn many situations, a set of inputs \\(X\\) are readily available, but the output \\(Y\\) cannot be easily obtained. In this setting, since the error term averages to zero, we can predict \\(Y\\) using\n\\[\n\\hat{Y} = \\hat{f}(X),\n\\]\nwhere \\(\\hat{f}\\) represents our estimate for \\(f\\) , and \\(\\hat{Y}\\) represents the resulting prediction for \\(Y\\). In this setting, \\(\\hat{f}\\) is often treated as a black box, in the sense that one is not typically concerned with the exact form of \\(\\hat{f}\\), provided that it yields accurate predictions for \\(Y\\).\nThe accuracy of \\(\\hat{Y}\\) as a prediction for \\(Y\\) depends on two quantities, which we will call the reducible error and the irreducible error. In general, \\(\\hat{f}\\) will not be a perfect estimate for \\(f\\), and this inaccuracy will introduce some error. This error is reducible because we can potentially improve the accuracy of \\(\\hat{f}\\) by using the most appropriate statistical learning technique to estimate \\(f\\). However, even if it were possible to form a perfect estimate for \\(f\\), so that our estimated response took the form \\(\\hat{Y} = f(X)\\), our prediction would still have some error in it! This is because \\(Y\\) is also a function of \\(\\epsilon\\), which, by definition, cannot be predicted using \\(X\\). Therefore, variability associated with \\(\\epsilon\\) also affects the accuracy of our predictions. This is known as the irreducible error, because no matter how well we estimate \\(f\\), we cannot reduce the error introduced by \\(\\epsilon\\).\nWhy is the irreducible error larger than zero? The quantity \\(\\epsilon\\) may contain unmeasured variables that are useful in predicting \\(Y\\): since we don’t measure them, \\(f\\) cannot use them for its prediction. The quantity \\(\\epsilon\\) may also contain unmeasurable variation.\nConsider a given estimate \\(\\hat{f}\\) and a set of predictors \\(X\\), which yields the prediction \\(\\hat{Y} = \\hat{f}(X)\\). Assume for a moment that both \\(\\hat{f}\\) and \\(X\\) are fixed, so that the only variability comes from \\(\\epsilon\\). Then, we can say:\n\nwhere \\(E(Y − \\hat{Y})^2\\) represents the average, or expected value, of the squared difference between the predicted and actual value of \\(Y\\), and \\(Var(\\epsilon)\\) represents the variance associated with the error term \\(\\epsilon\\).\nThe focus of this book is on techniques for estimating \\(f\\) with the aim of minimizing the reducible error. It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for \\(Y\\). This bound is almost always unknown in practice.\nInference\nWe are often interested in understanding the association between \\(Y\\) and \\(X_1, \\ldots , X_p\\). In this situation we wish to estimate \\(f\\), but our goal is not necessarily to make predictions for \\(Y\\). Now \\(\\hat{f}\\) cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:\n\nWhich predictors are associated with the response? It is often the case that only a small fraction of the available predictors are substantially associated with \\(Y\\). Identifying the few important predictors among a large set of possible variables can be extremely useful, depending on the application.\nWhat is the relationship between the response and each predictor? Some predictors may have a positive relationship with \\(Y\\), in the sense that larger values of the predictor are associated with larger values of \\(Y\\). Other predictors may have the opposite relationship. Depending on the complexity of \\(f\\), the relationship between the response and a given predictor may also depend on the values of the other predictors.\nCan the relationship between \\(Y\\) and each predictor be adequately summarized using a linear equation, or is the relationship more complicated? Historically, most methods for estimating \\(f\\) have taken a linear form. In some situations, such an assumption is reasonable or even desirable. But often the true relationship is more complicated, in which case a linear model may not provide an accurate representation of the relationship between the input and output variables.\nHow do we estimate \\(f\\)?\nThroughout this book, we explore many linear and non-linear approaches for estimating \\(f\\). However, these methods generally share certain characteristics. Here is an overview.\nNote we will always assume that we have observed a set of \\(n\\) different data points, called the training data because we will use these observations to train, or teach, our method how to estimate \\(f\\).\nOur goal is to apply a statistical learning method to the training data in order to estimate the unknown function \\(f\\). In other words, we want to find a function \\(\\hat{f}\\) such that \\(Y \\approx \\hat{f}(X)\\) for any observation \\((X,Y)\\). Broadly speaking, most statistical learning methods for this task can be characterized as either parametric or non-parametric. We now briefly discuss these two types of approaches.\nParametric\nParametric methods involve a two-step model-based approach.\nStep 1\n\nFirst, we make an assumption about the functional form, or shape, of \\(f\\). For example, one very simple assumption is that \\(f\\) is linear in \\(X\\):\n\n\\[\nf(X) = \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\n\\]\n\nThis is a linear model. Once we have assumed that \\(f\\) is linear, the problem of estimating \\(f\\) is greatly simplified. Instead of having to estimate an entirely arbitrary \\(p\\)-dimensional function \\(f(X)\\), one only needs to estimate the p+1 coefficients \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\).\n\nStep 2\n\nAfter a model has been selected, we need a procedure that uses the training data to fit or train the model. In the case of the linear model, we need to estimate the parameters \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\).That is,we want to find values of these parameters such that\n\n\\[\nY \\approx \\beta_0 + \\beta_1 X_1 + \\ldots + \\beta_p X_p\n\\]\n\nThe most common approach to fitting the model above is referred to as (ordinary) least squares. However, least squares is one of many possible ways to fit the linear model.\n\nThe model-based approach just described is referred to as parametric; it reduces the problem of estimating \\(f\\) down to one of estimating a set of parameters. Assuming a parametric form for \\(f\\) simplifies the problem of estimating \\(f\\) because it is generally much easier to estimate a set of parameters, such as \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) in the linear model, than it is to fit an entirely arbitrary function \\(f\\). The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of \\(f\\). If the chosen model is too far from the true \\(f\\), then our estimate will be poor. We can try to address this problem by choosing flexible models that can fit many different possible functional forms for \\(f\\). But in general, fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as overfitting the data, which essentially means they follow the errors, or noise, too closely.\nNon-parametric\nNon-parametric methods do not make explicit assumptions about the functional form of \\(f\\). Instead they seek an estimate of \\(f\\) that gets as close to the data points as possible without being too rough or wiggly. Such approaches can have a major advantage over parametric approaches: by avoiding the assumption of a particular functional form for \\(f\\), they have the potential to accurately fit a wider range of possible shapes for \\(f\\).\nAny parametric approach brings with it the possibility that the functional form used to estimate \\(f\\) is very different from the true \\(f\\), in which case the resulting model will not fit the data well. In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of \\(f\\) is made. But non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating \\(f\\) to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for \\(f\\).\nBe careful of overfitting though, non-parametric methods can fit the data perfectly if complex enough, which causes the fit obtained to not yield accurate estimates of the response on new observations that were not part of the original training data set.\n\nThe trade-off between prediction accuracy and model interpretability\nOf the many methods that we examine in this book, some are less flexible, or more restrictive, in the sense that they can produce just a relatively small range of shapes to estimate \\(f\\). For example, linear regression is a relatively inflexible approach, because it can only generate linear functions (smooth lines / planes). Other methods are considerably more flexible because they can generate a much wider range of possible shapes to estimate \\(f\\).\nOne might reasonably ask the following question: why would we ever choose to use a more restrictive method instead of a very flexible approach? There are several reasons that we might prefer a more restrictive model. If we are mainly interested in inference, then restrictive models are much more interpretable. For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between \\(Y\\) and \\(X_1, X_2, \\ldots, X_p\\). In contrast, very flexible approaches, such as the splines and boosting methods can lead to such complicated estimates of \\(f\\) that it is difficult to understand how any individual predictor is associated with the response.\nHere is an illustration of the trade-off between flexibility and interpretability for some of the methods that we cover in this book.\n\nLeast squares linear regression is relatively inflexible but is quite interpretable. The lasso relies upon the linear model but uses an alternative fitting procedure for estimating the coefficients \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\). The new procedure is more restrictive in estimating the coefficients, and sets a number of them to exactly zero. Hence in this sense the lasso is a less flexible approach than linear regression. It is also more interpretable than linear regression, because in the final model the response variable will only be related to a small subset of the predictors — namely, those with nonzero coefficient estimates.\nGeneralized additive models (GAMs), discussed in Chapter 7, instead extend the linear model to allow for certain non-linear relationships. Consequently, GAMs are more flexible than linear regression. They are also somewhat less interpretable than linear regression, because the relationship between each predictor and the response is now modeled using a curve. Finally, fully non-linear methods such as bagging, boosting, support vector machines with non-linear kernels, and neural networks (deep learning) are highly flexible approaches that are harder to interpret.\nWe have established that when inference is the goal, there are clear advantages to using simple and relatively inflexible statistical learning methods. In some settings, however, we are only interested in prediction, and the interpretability of the predictive model is simply not of interest; our sole requirement for the algorithm is that it predict accurately (i.e. interpretability is not a concern). In this setting, we might expect that it will be best to use the most flexible model available. Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for overfitting in highly flexible methods.\nSupervised vs unsupervised learning\nMost statistical learning problems fall in to one of two categories: supervised or unsupervised. The examples that we have discussed so far in this chapter all fall into the supervised learning domain. For each observation of the predictor measurement(s) \\(x_i, i = 1, \\ldots , n\\) there is an associated response measurement \\(y_i\\). We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference). Many classical statistical learning methods such as linear regression and logistic regression, as well as more modern approaches such as GAM, boosting, and support vector machines, operate in the supervised learning domain. The vast majority of this book is devoted to this setting.\nBy contrast, unsupervised learning describes the somewhat more challenging situation in which for every observation \\(i = 1, \\ldots, n\\), we observe a vector of measurements \\(x_i\\) but no associated response \\(y_i\\). It is not possible to fit a linear regression model, since there is no response variable to predict. In this setting, we are in some sense working blind; the situation is referred to as unsupervised because we lack a response variable that can supervise our analysis. What sort of statistical analysis is possible? We can seek to understand the relationships between the variables or between the observations. One statistical learning tool that we may use in this setting is cluster analysis, or clustering. The goal of cluster analysis is to ascertain, on the basis of \\(x_1, \\ldots, x_n\\), whether the observations fall into relatively distinct groups.\n\nHowever, in practice the group memberships are unknown, and the goal is to determine the group to which each observation belongs. A clustering method could not be expected to assign all of the overlapping points to their correct group.\nIn the example above, there are only two variables, and so one can simply visually inspect the scatterplots of the observations in order to identify clusters. However, in practice, we often encounter data sets that contain many more than two variables. In this case, we cannot easily plot the observations. For instance, if there are \\(p\\) variables in our data set, then \\(p(p − 1)/2\\) distinct scatterplots can be made, and visual inspection is simply not a viable way to identify clusters. For this reason, automated clustering methods are important.\nMany problems fall naturally into the supervised or unsupervised learning paradigms. However, sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut. For instance, suppose that we have a set of \\(n\\) observations. For \\(m\\) of the observations, where \\(m &lt; n\\), we have both predictor measurements and a response measurement. For the remaining \\(n − m\\) observations, we have predictor measurements but no response measurement. Such a scenario can arise if the predictors can be measured relatively cheaply but the corresponding responses are much more expensive to collect. We refer to this setting as a semi-supervised learning problem. In this setting, we wish to use a statistical learning method that can incorporate the \\(m\\) observations for which response measurements are available as well as the \\(n − m\\) observations for which they are not. Although this is an interesting topic, it is beyond the scope of this book.\nRegression vs classification problems\nWe tend to refer to problems with a quantitative response as regression problems, while those involving a qualitative response are often referred to as classification problems. However, the distinction is not always that crisp. Least squares linear regression is used with a quantitative response, whereas logistic regression is typically used with a qualitative (two-class, or binary) response. Thus, despite its name, logistic regression is a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as \\(K\\)-nearest neighbors and boosting, can be used in the case of either quantitative or qualitative responses.\nWe tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative; i.e. we might use linear regression when quantitative and logistic regression when qualitative. However, whether the predictors are qualitative or quantitative is generally considered less important. Most of the statistical learning methods discussed in this book can be applied regardless of the predictor variable type, provided that any qualitative predictors are properly coded before the analysis is performed.\n\n3.1.2 Assessing model accuracy\nOne of the key aims of this book is to introduce the reader to a wide range of statistical learning methods that extend far beyond the standard linear regression approach. Why is it necessary to introduce so many different statistical learning approaches, rather than just a single best method? There is no free lunch in statistics: no one method dominates all others over all possible data sets. On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. Hence it is an important task to decide for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.\nNow, we discuss some of the most important concepts that arise in selecting a statistical learning procedure for a specific data set.\nMeasuring the quality of fit\nIn order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the mean squared error (MSE), given by\n\\[\nMSE = \\frac{1}{n} \\sum_{i = 1}^n (y_i - \\hat{f}(x_i))^2\n\\] The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially.\nThis MSE is computed using the training data that was used to fit the model, and so should more accurately be referred to as the training MSE. But in general, we do not really care how well the method works training on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data. To state it more mathematically, suppose that we fit our statistical learn- ing method on our training observations \\(\\{(x_1,y_1), (x_2,y_2), \\ldots, (x_n,y_n)\\}\\), and we obtain the estimate \\(\\hat{f}\\). We can then compute \\(\\hat{f}(x_1), \\hat{f}(x_2), \\ldots , \\hat{f}(x_n)\\). If these are approximately equal to \\(y_1, y_2, \\ldots, y_n\\), then the training MSE is small. However, we are really not interested in whether \\(\\hat{f}(x_i) \\approx y_i\\); instead, we want to know whether \\(\\hat{f}(x_0)\\) is approximately equal to \\(y_0\\), where \\((x_0,y_0)\\) is a previously unseen test observation not used to train the statistical learning method. We want to choose the method that gives the lowest test MSE, as opposed to the lowest training MSE. In other words, if we had a large number of test observations, we could compute the average squared prediction error for these test observations \\((x_0,y_0)\\):\n\\[\n\\text{Ave}(y_0 - \\hat{f}(x_0))^2\n\\]\nWe’d like to select the model for which this quantity is as small as possible.\nHow can we go about trying to select a method that minimizes the test MSE? In some settings, we may have a test data set available. We can then simply evaluate the above formula on the test observations, and select the learning method for which the test MSE is smallest. But what if no test observations are available? In that case, one might imagine simply selecting a statistical learning method that minimizes the training MSE. This seems like it might be a sensible approach, since the training MSE and the test MSE appear to be closely related. Unfortunately, there is a fundamental problem with this strategy: there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but the test MSE is often much larger.\nThe figure below illustrates this concept. In the left-hand panel, we have generated observations from some \\(f(X)\\) with the true \\(f\\) given by the black curve. The orange, blue and green curves illustrate three possible estimates for \\(f\\) obtained using methods with increasing levels of flexibility.\n\nThe blue and green curves were produced using smoothing splines. It is clear that as the level of flexibility increases, the curves fit the observed data more closely. The green curve is the most flexible and matches the data very well; however, we observe that it fits the true \\(f\\) (shown in black) poorly because it is too wiggly.\nOn the right panel, the grey curve displays the average training MSE as a function of flexibility, or more formally the degrees of freedom, for a number of smoothing splines. The degrees of freedom is a quantity that summarizes the flexibility of a curve. A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve (i.e. more error dfs; a more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve). The training MSE declines monotonically as flexibility increases. In this example the true \\(f\\) is non-linear, and so the orange linear fit is not flexible enough to estimate f well. The green curve has the lowest training MSE of all three methods, since it corresponds to the most flexible of the three curves fit in the left-hand panel.\nIn this example, we know the true function \\(f\\), and so we can also compute the test MSE over a very large test set, as a function of flexibility. (Of course, in general \\(f\\) is unknown, so this will not be possible. The horizontal dashed line indicates \\(\\text{Var}(\\epsilon)\\), the irreducible error, which corresponds to the lowest achievable test MSE among all possible methods. As the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a U-shape in the test MSE (red line). This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be overfitting the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function \\(f\\). When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data. Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.\nHere is another example that displays the same patterns. However, because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the orange least squares fit is substantially better than the highly flexible green curve.\n\nFinally, here is another example in which \\(f\\) is highly non-linear. Now there is a rapid decrease in both curves before the test MSE starts to increase slowly.\n\nIn practice, one can usually compute the training MSE with relative ease, but estimating test MSE is considerably more difficult because usually no test data are available. As the previous three examples illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably among data sets. Throughout this book, we discuss a variety of approaches that can be used in practice to estimate this minimum point. One important method is cross-validation, which is a method for estimating test MSE using the training data.\nThe bias-variance trade-off\nThe U-shape observed in the test MSE curves turns out to be the result of two competing properties of statistical learning methods. It is possible to show that the expected test MSE, for a given value \\(x_0\\), can always be decomposed into the sum of three fundamental quantities: the variance of \\(\\hat{f}(x_0)\\), the squared bias of \\(f(x_0)\\) and the variance of the error terms \\(\\epsilon\\). That is,\n\\[\nE\\big(y_o - \\hat{f}(x_0)\\big)^2 = \\text{Var}(\\hat{f}(x_0)) + [\\text{Bias}(\\hat{f}(x_0))]^2 + \\text{Var}(\\epsilon)\n\\]\n\nHere, the notation \\(E\\big(y_o - \\hat{f}(x_0)\\big)^2\\) defines the expected test MSE at \\(x_0\\), and refers to the average test MSE that we would obtain if we repeatedly estimated \\(f\\) using a large number of training sets, and tested each at \\(x_0\\). The overall expected test MSE can be computed by averaging \\(E\\big(y_o - \\hat{f}(x_0)\\big)^2\\) over all possible values of \\(x_0\\) in the test set.\nThis equation tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves low variance and low bias. Note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative. Hence, we see that the expected test MSE can never lie below \\(\\text{Var}(\\epsilon)\\), the irreducible error.\nWhat do we mean by the variance and bias of a statistical learning method? Variance refers to the amount by which \\(\\hat{f}\\) would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different \\(\\hat{f}\\). But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in \\(\\hat{f}\\). In general, more flexible statistical methods have higher variance.\nOn the other hand, bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. Generally, more flexible methods result in less bias.\nAs a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases. For the previous examples, here are the variance, bias curves:\n\nThis shows that variance is an increasing function of flexibility, and bias is a decreasing function. The goal is to find the balance point. This trade-off is one of the most important recurring themes in this book.\nIn a real-life situation in which \\(f\\) is unobserved, it is generally not possible to explicitly compute the test MSE, bias, or variance for a statistical learning method. Nevertheless, one should always keep the bias-variance trade-off in mind. In this book we explore methods that are extremely flexible and hence can essentially eliminate bias. However, this does not guarantee that they will outperform a much simpler method such as linear regression. To take an extreme example, suppose that the true \\(f\\) is linear. In this situation linear regression will have no bias, making it very hard for a more flexible method to compete. In contrast, if the true \\(f\\) is highly non-linear and we have an ample number of training observations, then we may do better using a highly flexible approach. Later, we discuss cross-validation, which is a way to estimate the test MSE using the training data.\nThe classification setting\nThus far, our discussion of model accuracy has been focused on the regression setting. But many of the concepts that we have encountered, such as the bias-variance trade-off, transfer over to the classification setting with only some modifications due to the fact that \\(y_i\\) is no longer quantitative. Suppose that we seek to estimate \\(f\\) on the basis of training observations \\(\\{(x_1,y_1), \\ldots, (x_n,y_n)\\}\\), where now \\(y_1, \\ldots , y_n\\) are qualitative. The most common approach for quantifying the accuracy of our estimate \\(\\hat{f}\\) is the training error rate, the proportion of mistakes that are made if we apply our estimate \\(\\hat{f}\\) to the training observations:\n\\[\n\\frac{1}{n} \\sum_{i = 1}^n I(y_i \\ne \\hat{y}_i)\n\\]\nHere \\(\\hat{y}_i\\) is the predicted class label for the \\(i\\)th observation using \\(\\hat{f}\\). And \\(I(y_i \\ne \\hat{y}_i))\\) is an indicator variable that equals 1 if \\(y_i \\ne \\hat{y}_i\\) and zero if \\(y_i = \\hat{y}_i\\) (i.e. indicator if wrong). Thus, the above equation computes the fraction of incorrect classifications.\nThe test error rate associated with a set of test observations of the form \\((x_0, y_0)\\) is given by\n\\[\n\\text{Ave}(I(y_0 \\ne \\hat{y}_0))\n\\]\nA good classifier is one for which the test error is smallest.\nThe Bayes classifier\nIt is possible to show (though the proof is outside of the scope of this book) that the test error rate given in is minimized, on average, by a very simple classifier that assigns each observation to the most likely class, given its predictor values. In other words, we should simply assign a test observation with predictor vector \\(x_0\\) to the class \\(j\\) for which\n\\[\nP(Y = j \\mid X = x_0)\n\\]\nis the largest. Note that the above formula is a conditional probability: it is the probability that \\(Y = j\\), given the observed predictor vector \\(x_0\\). This very simple classifier is called the Bayes classifier. It is super simple to calculate this:\n\nIn a two-class problem where there are only two possible response values, say class 1 or class 2, the Bayes classifier corresponds to predicting class one if \\(P(Y = j \\mid X = x_0) &gt; 0.5\\), and class two otherwise.\nHere is an example:\n\nFor each value of \\(X_1\\) and \\(X_2\\), there is a different probability of the response being orange or blue. Since this is simulated data, we know how the data were generated and we can calculate the conditional probabilities for each value of \\(X_1\\) and \\(X_2\\). The purple dashed line represents the points where the probability is exactly 50%. This is called the Bayes decision boundary.\nThe Bayes classifier produces the lowest possible test error rate, called the Bayes error rate. Since the Bayes classifier will always choose the class for which the above formula is largest, the error rate will be \\(1 − \\text{max}_j \\, P(Y = j \\mid X = x_0)\\) at \\(X = x_0\\). In general, the overall Bayes error rate is given by\n\\[\n1 - E\\big(\\underset{j}{\\text{max}} \\, P(Y = j \\mid X)\\big)\n\\]\nwhere the expectation averages the probability over all possible values of \\(X\\). The Bayes error rate is analogous to the irreducible error, discussed earlier.\n\n\\(K\\)-Nearest neighbors\nIn theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of \\(Y\\) given \\(X\\), and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods.\nMany approaches attempt to estimate the conditional distribution of \\(Y\\) given \\(X\\), and then classify a given observation to the class with highest estimated probability. One such method is the K-nearest neighbors (KNN) classifier. Given a positive integer \\(K\\) and a test observation \\(x_0\\), the KNN classifier first identifies the \\(K\\) points in the training data that are closest to \\(x_0\\), represented by \\(\\cal{N}_0\\). It then estimates the conditional probability for class \\(j\\) as the fraction of points in \\(\\cal{N}_0\\) whose response values equal \\(j\\):\n\\[\nP(Y = j \\mid X = x_0) = \\frac{1}{K}\\sum_{i \\in \\cal{N}_0} I(y_i = j)\n\\]\nFinally, KNN classifies the test observation \\(x_0\\) to the class with the largest probability from the equation above.\nHere is an example of the KNN approach:\n\nDespite the fact that it is a very simple approach, KNN can often produce classifiers that are surprisingly close to the optimal Bayes classifier. Note that even though the true distribution is not known by the KNN classifier, the KNN decision boundary is very close to that of the Bayes classifier.\n\nHowever, the choice of \\(K\\) has a drastic effect on the KNN classifier obtained.\n\nWhen \\(K = 1\\), the decision boundary is overly flexible and finds patterns in the data that don’t correspond to the Bayes decision boundary. This corresponds to a classifier that has low bias but very high variance. As \\(K\\) grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier.\nJust as in the regression setting, there is not a strong relationship between the training error rate and the test error rate. With \\(K = 1\\), the KNN training error rate is 0, but the test error rate may be quite high. In general, as we use more flexible classification methods, the training error rate will decline but the test error rate may not.\n\nAs \\(1/K\\) increases, the method becomes more flexible. As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately \\(K = 10\\)) before increasing again when the method becomes excessively flexible and overfits.\nIn both the regression and classification settings, choosing the correct level of flexibility is critical to the success of any statistical learning method. The bias-variance trade-off, and the resulting U-shape in the test error, can make this a difficult task.",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ISLR -- Statistical learning</span>"
    ]
  },
  {
    "objectID": "islr-2.html#lab",
    "href": "islr-2.html#lab",
    "title": "\n3  ISLR – Statistical learning\n",
    "section": "\n3.2 Lab",
    "text": "3.2 Lab\n&lt; just basic R commands &gt;",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ISLR -- Statistical learning</span>"
    ]
  },
  {
    "objectID": "islr-2.html#exercises",
    "href": "islr-2.html#exercises",
    "title": "\n3  ISLR – Statistical learning\n",
    "section": "\n3.3 Exercises",
    "text": "3.3 Exercises\n\n3.3.1 Conceptual\nQuestion 1\n\nFor each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.\n\nThe sample size \\(n\\) is extremely large, and the number of predictors \\(p\\) is small.\n\n\nA flexible model will perform better than an inflexible model because there are ample degrees of freedom to estimate many parameters and still have many leftover for error df.\n\n\nThe number of predictors \\(p\\) is extremely large, and the number of observations \\(n\\) is small.\n\n\nAn inflexible will be better because there is a high chance of some predictors being randomly associated.\n\n\nThe relationship between the predictors and response is highly non-linear.\n\n\nA flexible model will perform better because it can pick up on the non-linear trends better.\n\n\nThe variance of the error terms, i.e. \\(\\sigma^2 = \\text{Var}(\\epsilon)\\), is extremely high.\n\n\nInflexible model will be better because the flexible model will pick up on the noise a lot better even though it is not related to the relationship to the \\(X\\)s.\nQuestion 2\n\nExplain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide \\(n\\) and \\(p\\).\n\nWe collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.\n\n\nRegression; inference; \\(n = 500, p = 3\\)\n\n\nWe are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.\n\n\nClassification; prediction; \\(n = 20, p = 13\\)\n\n\nWe are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.\n\n\nRegression; prediction; \\(n = 52, p = 3\\).\nQuestion 3\n\nWe now revisit the bias-variance decomposition.\n\nProvide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.\n\n\n\n\n\nExplain why each of the five curves has the shape displayed in part (a).\n\n\n\nBias: Decreases until leveling off as flexibility increases (nears zero)\nVariance: Increases faster as flexibility increases\nBayes (irreducible error): Constant, unrelated to flexibility (\\(X\\))\nTraining error rate: Decrease monotonically as flexibility increases\nTesting error rate: U-shaped with minimum where bias and variance are simultaneously minimized\nQuestion 4\n\nYou will now think of some real-life applications for statistical learning.\n\nDescribe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\n\nInsurance, predict whether a policyholder will have a claim or not. Response is binary, predictors are variables such as credit score, age of business, type of business, etc. Goal is prediction, want to see if a new customer is likely to have a claim.\n&lt; good enough &gt;\n\n\n\nDescribe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.\n\n\nInsurance, predict the amount of a claim. Response is claim size in dollars, same predictors as in a.\n\n\nDescribe three real-life applications in which cluster analysis might be useful.\n\n\nInsurance, determining if a business is low, medium or high risk. Same predictors as a.\nQuestion 5\n\nWhat are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?\n\n\nFlexible models can pick up on non-linear patterns in regression and are more useful in prediction when we are only concerned with accuracy, especially with large \\(n\\); and they are less biased, however more variable. A less flexible approach is better for inference when we want to understand the relationship(s) between \\(Y\\) and \\(X\\); more bias, but less variable.\nQuestion 6\n\nDescribe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?\n\nParametric approach assumes a functional form of the response and one needs to only estimate parameters, which is a simpler task. And because of the assumption, the model is more interpretable. However a non-parametric method just fits the data as best as possible. Non-parametric methods can be less biased, but are more variable, especially as the complexity increases and more data is needed because of the more parameters being estimated.\nQuestion 7\n\nThe table below provides a training data set containing six observations, three predictors, and one qualitative response variable.\n\n\n\nSuppose we wish to use this data set to make a prediction for \\(Y\\) when \\(X_1 = X_2 = X_3 = 0\\) using \\(K\\)-nearest neighbors.\n\n\n\nCompute the Euclidean distance between each observation and the test point, \\(X_1 = X_2 = X_3 = 0\\).\n\n\n\n# read in data\ndf &lt;- tibble(\n  x1 = c(0, 2, 0, 0, -1, 1),\n  x2 = c(3, 0, 1, 1, 0, 1),\n  x3 = c(0, 0, 3, 2, 1, 1),\n  y = c(\"Red\", \"Red\", \"Red\", \"Green\", \"Green\", \"Red\")\n)\n\n# define function to calculate euclidean distance \n# -&gt; sum function calculates the sum of the squares of absolute difference between  corresponding elements of vec_1 and vec_2 \ncalc_euc_dist &lt;- function(vec_1, vec_2) {\n  subtract(vec_1, vec_2) %&gt;% raise_to_power(2) %&gt;% sum %&gt;% sqrt\n} \n\n# calculate euclidean distance for each observation\ndf %&gt;%\n  rowwise() %&gt;% \n  mutate(euc_dist = calc_euc_dist(c(x1, x2, x3), rep(0, 3)),\n         .keep = \"none\")\n\n# A tibble: 6 × 1\n# Rowwise: \n  euc_dist\n     &lt;dbl&gt;\n1     3   \n2     2   \n3     3.16\n4     2.24\n5     1.41\n6     1.73\n\n\n\n\nWhat is our prediction with K = 1? Why?\n\n\n\ndf %&gt;%\n  rowwise() %&gt;% \n  mutate(euc_dist = calc_euc_dist(c(x1, x2, x3), rep(0, 3)),\n         .keep = \"unused\") %&gt;% \n  ungroup() %&gt;% \n  slice_min(order_by = euc_dist, n = 1)\n\n# A tibble: 1 × 2\n  y     euc_dist\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Green     1.41\n\n\nGreen is our prediction\n\n\nWhat is our prediction with K = 3? Why?\n\n\n\n# turn above into a function\n\n# define function to do KNN prediction for a particular test point\nalgorithm_knn &lt;- function(df, y, x0, k = 1, final_result = FALSE) {\n \n  # get column names\n  nms = df %&gt;% \n    colnames\n  \n  # standardized column names\n  # -&gt; place response variable first and renamed\n  # -&gt; followed by explanatory variables renamed x1, x2, ... (unnecessary)\n  df_z &lt;- df %&gt;% \n    select(y = any_of(y), any_of(nms[which(nms != y)])) #%&gt;% \n    #rename_with(~ paste0(\"x\", 1:(ncol(df)-1)), .cols = any_of(nms[which(nms != y)]))\n   \n  # calculate euclidean distances\n  euc_dist = c()\n  for (i in 1:nrow(df_z)) {\n    \n    euc_dist[i] = calc_euc_dist(as.vector(df_z[i, 2:ncol(df_z)], mode = \"double\"), x0) \n    \n  }\n  \n  # create dataframe of results\n  df_dist = df_z %&gt;% \n    select(y) %&gt;% \n    bind_cols(euc_dist = euc_dist)\n    \n  # display results\n  results = df_dist %&gt;% \n    slice_min(order_by = euc_dist, n = k) %&gt;% \n    summarize(.by = y, \n              est_prob = n() / nrow(.))\n  \n  # conditionally simplify results\n  if(final_result) {\n    results %&lt;&gt;% \n      slice_min(order_by = est_prob) %&gt;% \n      select(y) %&gt;% \n      as.character\n  }\n  \n  return(results)\n  \n}\n\n# test function on K = 1\nalgorithm_knn(df, y = \"y\", x0 = rep(0, 3), k = 1, final_result = FALSE)\n\n# A tibble: 1 × 2\n  y     est_prob\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Green        1\n\n# now for K = 3\nalgorithm_knn(df, y = \"y\", x0 = rep(0, 3), k = 3, final_result = FALSE)\n\n# A tibble: 2 × 2\n  y     est_prob\n  &lt;chr&gt;    &lt;dbl&gt;\n1 Green    0.333\n2 Red      0.667\n\nalgorithm_knn(df, y = \"y\", x0 = rep(0, 3), k = 3, final_result = TRUE)\n\n[1] \"Green\"\n\n# test a set of points\nx0 &lt;- tibble(\n  x1 = extraDistr::rdunif(n = 5, min = 0, max = 3),\n  x2 = extraDistr::rdunif(n = 5, min = 0, max = 3),\n  x3 = extraDistr::rdunif(n = 5, min = 0, max = 3)\n)\n\nresults &lt;- c()\nfor (i in 1:nrow(x0)) {\n  results[i] = algorithm_knn(df, y = \"y\", x0 = x0[i,], k = 3, final_result = TRUE)\n}\nresults\n\n[1] \"Green\" \"Green\" \"Green\" \"Green\" \"Green\"\n\n# note if results include a vector of two classes, then there was a tie. Not sure how to break ties, but could research and code something...\n\n\n\nIf the Bayes decision boundary in this problem is highly non- linear, then would we expect the best value for K to be large or small? Why?\n\n\nSmall so that the model is more flexible (high \\(k\\) leads to linear boundaries due to averaging).\n\n3.3.2 Applied\nQuestion 8\n\n# load data\ndata_college &lt;- ISLR2::College\n\n# summarize numeric variables\ndata_college %&gt;% \n  select(where(is.numeric)) %&gt;% \n  summary\n\n      Apps           Accept          Enroll       Top10perc       Top25perc    \n Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00   Min.   :  9.0  \n 1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00   1st Qu.: 41.0  \n Median : 1558   Median : 1110   Median : 434   Median :23.00   Median : 54.0  \n Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56   Mean   : 55.8  \n 3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00   3rd Qu.: 69.0  \n Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00   Max.   :100.0  \n  F.Undergrad     P.Undergrad         Outstate       Room.Board  \n Min.   :  139   Min.   :    1.0   Min.   : 2340   Min.   :1780  \n 1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320   1st Qu.:3597  \n Median : 1707   Median :  353.0   Median : 9990   Median :4200  \n Mean   : 3700   Mean   :  855.3   Mean   :10441   Mean   :4358  \n 3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925   3rd Qu.:5050  \n Max.   :31643   Max.   :21836.0   Max.   :21700   Max.   :8124  \n     Books           Personal         PhD            Terminal    \n Min.   :  96.0   Min.   : 250   Min.   :  8.00   Min.   : 24.0  \n 1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00   1st Qu.: 71.0  \n Median : 500.0   Median :1200   Median : 75.00   Median : 82.0  \n Mean   : 549.4   Mean   :1341   Mean   : 72.66   Mean   : 79.7  \n 3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00   3rd Qu.: 92.0  \n Max.   :2340.0   Max.   :6800   Max.   :103.00   Max.   :100.0  \n   S.F.Ratio      perc.alumni        Expend        Grad.Rate     \n Min.   : 2.50   Min.   : 0.00   Min.   : 3186   Min.   : 10.00  \n 1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751   1st Qu.: 53.00  \n Median :13.60   Median :21.00   Median : 8377   Median : 65.00  \n Mean   :14.09   Mean   :22.74   Mean   : 9660   Mean   : 65.46  \n 3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830   3rd Qu.: 78.00  \n Max.   :39.80   Max.   :64.00   Max.   :56233   Max.   :118.00  \n\n# scatterplot matrix\ndata_college[, 1:10] %&gt;% pairs\n\n\n\n\n\n\n# side-by-side boxplots\nboxplot(Outstate ~ Private, data = data_college)\n\n\n\n\n\n\n# create new variable and summarize\ndata_college %&lt;&gt;% \n  mutate(Elite = case_when(\n    Top10perc &gt; 50 ~ \"Yes\",\n    .default = \"No\"\n  ))\ntable(data_college$Elite)\n\n\n No Yes \n699  78 \n\n\nQuestion 9\n\n# load data\ndata_auto &lt;- ISLR2::Auto \n\n# determine variable types\ndata_auto %&gt;% map_chr(class)\n\n         mpg    cylinders displacement   horsepower       weight acceleration \n   \"numeric\"    \"integer\"    \"numeric\"    \"integer\"    \"integer\"    \"numeric\" \n        year       origin         name \n   \"integer\"    \"integer\"     \"factor\" \n\n# find range of each quantitative predictor\ndata_auto %&gt;% \n  select(where(is.numeric)) %&gt;% \n  reframe(across(1:ncol(.), range))\n\n   mpg cylinders displacement horsepower weight acceleration year origin\n1  9.0         3           68         46   1613          8.0   70      1\n2 46.6         8          455        230   5140         24.8   82      3\n\n# find mean and st dev of each quantitative predictor\n# -&gt; then format nicely\ndata_auto %&gt;% \n  select(where(is.numeric)) %&gt;% \n  summarize(across(1:ncol(.), list(mean = mean, sd = sd))) %&gt;% \n  pivot_longer(everything(),\n               names_to = \"var\",\n               values_to = \"val\") %&gt;% \n  separate_wider_delim(cols = var,\n                       delim = \"_\",\n                       names = c(\"var\", \"fun\")) %&gt;% \n  pivot_wider(names_from = \"fun\",\n              values_from = \"val\")\n\n# A tibble: 8 × 3\n  var             mean      sd\n  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 mpg            23.4    7.81 \n2 cylinders       5.47   1.71 \n3 displacement  194.   105.   \n4 horsepower    104.    38.5  \n5 weight       2978.   849.   \n6 acceleration   15.5    2.76 \n7 year           76.0    3.68 \n8 origin          1.58   0.806\n\n# EDA\n# -&gt; trying to predict mpg\ndata_auto %&gt;% \n  select(where(is.numeric)) %&gt;% \n  pairs\n\n\n\n\n\n\n# useful variables\n# -&gt; all predictors appear to be correlated with the response (some have non-linear relationship), however there are several that are highly correlated among themselves\n\nQuestion 10\n\n# load data\ndata_boston &lt;- ISLR2::Boston\n\n# &lt; ... easy stuff ... &gt;",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>ISLR -- Statistical learning</span>"
    ]
  },
  {
    "objectID": "islr-3.html",
    "href": "islr-3.html",
    "title": "\n4  ISLR – Linear regression\n",
    "section": "",
    "text": "4.1 Notes",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>ISLR -- Linear regression</span>"
    ]
  },
  {
    "objectID": "islr-3.html#notes",
    "href": "islr-3.html#notes",
    "title": "\n4  ISLR – Linear regression\n",
    "section": "",
    "text": "4.1.1 Simple linear regression\nhttps://coltongearhart.github.io/regression/notes-slr.html\n\n4.1.2 Multiple linear regression\nhttps://coltongearhart.github.io/regression/notes-multiple-regression-1.html and https://coltongearhart.github.io/regression/notes-multiple-regression-2.html\n\n4.1.3 Other considerations in the regression model\nhttps://coltongearhart.github.io/regression/notes-reg-models-quan-and-qual.html\nThings to study more from other text\n\nLeverage\n\n\\[\nh_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{\\sum_{i'=1}^n (x_{i'} - \\bar{x})^2}\n\\tag{4.1}\\]\n\nVIF\n\n4.1.4 Comparison of linear regression with \\(K\\)-nearest neighbors\nAs discussed earlier, non-parametric methods do not explicitly assume a parametric form for \\(f(X)\\), and thereby provide an alternative and more flexible approach for performing regression. Here we consider one of the simplest and best-known non-parametric methods,\\(K\\)-nearest neighbors regression  (KNN regression). The KNN regression method is closely related to the KNN classifier discussed in the previous chapter. Given a value for \\(K\\) and a prediction point \\(x_0\\), KNN regression first identifies the \\(K\\) training observations that are closest to \\(x_0\\), represented by \\(\\cal{N}_0\\). It then estimates \\(f(x_0)\\) using the average of all the training responses in \\(\\cal{N}_0\\). In other words,\n\\[\n\\hat{f}(x_0) = \\frac{1}{K}\\sum_{x_i \\in \\cal{N}_0} y_i\n\\]\n\nWe see that when \\(K = 1\\), the KNN fit perfectly interpolates the training observations, and consequently takes the form of a step function. When \\(K = 9\\), the KNN fit still is a step function, but averaging over nine observations results in much smaller regions of constant prediction, and consequently a smoother fit. In general, the optimal value for K will depend on the bias-variance tradeoff.\nA small value for \\(K\\) provides the most flexible fit, which will have low bias but high variance. This variance is due to the fact that the prediction in a given region is entirely dependent on just one observation. In contrast, larger values of \\(K\\) provide a smoother and less variable fit; the prediction in a region is an average of several points, and so changing one observation has a smaller effect. However, the smoothing may cause bias by masking some of the structure in \\(f(X)\\).\nIn what setting will a parametric approach such as least squares linear regression outperform a non-parametric approach such as KNN regression? The answer is simple: the parametric approach will outperform the non- parametric approach if the parametric form that has been selected is close to the true form of \\(f\\).\n\nFigure 3.17 provides an example with data generated from a one-dimensional linear regression model. The black solid lines represent \\(f(X)\\), while the blue curves correspond to the KNN fits using \\(K = 1\\) and \\(K = 9\\). In this case, the \\(K = 1\\) predictions are far too variable, while the smoother \\(K = 9\\) fit is much closer to \\(f(X)\\). **However, since the true relationship is linear, it is hard for a non-parametric approach to compete with linear regression: a non-parametric approach incurs a cost in variance that is not offset by a reduction in bias.*\n\nFigure 3.19 examines the relative performances of least squares regression and KNN under increasing levels of non-linearity in the relationship between \\(X\\) and \\(Y\\). In more non-linear situation, KNN substantially outperforms linear regression for all values of \\(K\\). Note that as the extent of non-linearity increases, there is little change in the test set MSE for the non-parametric KNN method, but there is a large increase in the test set MSE of linear regression.\nIn a real life situation in which the true relationship is unknown, one might suspect that KNN should be favored over linear regression because it will at worst be slightly inferior to linear regression if the true relationship is linear, and may give substantially better results if the true relationship is non-linear. But in reality, even when the true relationship is highly non-linear, KNN may still provide inferior results to linear regression. But in higher dimensions, KNN often performs worse than linear regression.\n\nFigure 3.20 considers the same strongly non-linear situation as in the second row of Figure 3.19, except that we have added additional noise predictors that are not associated with the response. But for \\(p = 3\\) the results are mixed, and for \\(p \\ge 4\\) linear regression is superior to KNN. In fact, the increase in dimension has only caused a small deterioration in the linear regression test set MSE, but it has caused more than a ten-fold increase in the MSE for KNN. This decrease in performance as the dimension increases is a common problem for KNN, and results from the fact that in higher dimensions there is effectively a reduction in sample size. In this data set there are 50 training observations; when \\(p = 1\\), this provides enough information to accurately estimate \\(f(X)\\). However, spreading 50 observations over \\(p = 20\\) dimensions results in a phenomenon in which a given observation has no nearby neighbors – this is the so-called curse of dimensionality.\nAs a general rule, parametric methods will tend to outperform non-parametric approaches when there is a small number of observations per predictor. Even when the dimension is small, we might prefer linear regression to KNN from an interpretability standpoint. If the test MSE of KNN is only slightly lower than that of linear regression, we might be willing to forego a little bit of prediction accuracy for the sake of a simple model that can be described in terms of just a few coefficients, and for which p-values are available.",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>ISLR -- Linear regression</span>"
    ]
  },
  {
    "objectID": "islr-3.html#lab",
    "href": "islr-3.html#lab",
    "title": "\n4  ISLR – Linear regression\n",
    "section": "\n4.2 Lab",
    "text": "4.2 Lab\n&lt; just basic regression commands &gt;\nSome key points\n\n\nhatvalues(&lt; mod &gt;): Computes the hat values for each observation.\n\nThen can run hatvalues(&lt; mod &gt;) %&gt;% which.max to get the largest one.\n\n\ncar::vif(): Computes the VIF for each predictor.\n\nIn the formula, poly() by default orthogonalizes the predictors, so they are not simply a sequence of higher powers of the argument.\n\nHowever, a linear model applied to the output of the poly() function will have the same fitted values as a linear model applied to the raw polynomials (raw = TRUE; although the coefficient estimates, standard errors, and p-values will differ).\n\n\ncontrasts(&lt; factor &gt;): Returns the coding that R uses for the dummy variables.",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>ISLR -- Linear regression</span>"
    ]
  },
  {
    "objectID": "islr-3.html#exercises",
    "href": "islr-3.html#exercises",
    "title": "\n4  ISLR – Linear regression\n",
    "section": "\n4.3 Exercises",
    "text": "4.3 Exercises\n\n4.3.1 Conceptual\nQuestion 1\n\nDescribe the null hypotheses to which the p-values shown below correspond. Explain what conclusions you can draw based on these p-values. Your explanation should be phrased in terms of the predictors, rather than in terms of the coefficients of the linear model.\n\n\n\n# A tibble: 4 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     1.04     0.271       3.85 1.73e- 4\n2 Petal.Width     0.558    0.123       4.55 1.10e- 5\n3 Petal.Length   -0.586    0.0621     -9.43 8.75e-17\n4 Sepal.Length    0.607    0.0622      9.77 1.20e-17\n\n\nFor Petal.Length:\n\\[\n\\begin{align*}\nH_0&: \\beta_{\\text{Petal.Length}} = 0 \\\\\nH_A&: \\beta_{\\text{Petal.Length}} \\ne 0\n\\end{align*}\n\\]\n\n# fit model\nmod_iris &lt;- lm(Sepal.Width ~ Petal.Width + Petal.Length + Sepal.Length, data = iris, x = TRUE)\n\n# verify p-value\nbeta_hat &lt;- coef(mod_iris)[\"Petal.Length\"]\nX &lt;- mod_iris$x\nse_beta_hats &lt;- summary(mod_iris)$sigma^2 * solve(t(X) %*% (X))\nse_beta_hat &lt;- sqrt(se_beta_hats[3,3])\npt(q = beta_hat / se_beta_hat, df = nobs(mod_iris) - length(coef(mod_iris)), lower.tail = TRUE) * 2 %&gt;% as.numeric\n\n             Petal.Length \n0.00000000000000008753029 \n\n\nQuestion 2\n\nCarefully explain the differences between the KNN classifier and KNN regression methods.\n\nSimply stated, The KNN classifier is categorical and assigns a value based on the most frequent observed category among \\(K\\) nearest neighbors, whereas KNN regression assigns a continuous variable, the average of the response variables for the \\(K\\) nearest neighbors. This represents two different summary functions, which are: for classification\n\\[\n\\hat{f}(x_0) = \\text{max}_j\\bigg\\{\\frac{1}{K}\\sum_{x_i \\in \\cal{N}_0} I(y_i = j)\\bigg\\}\n\\]\nand for regression\n\\[\n\\hat{f}(x_0) = \\frac{1}{K}\\sum_{x_i \\in \\cal{N}_0} y_i\n\\]\nQuestion 3\n\nSuppose we have a data set with five predictors, \\(X_1\\) = GPA, \\(X_2\\) = IQ, \\(X_3\\) = Level (1 for College and 0 for High School), \\(X_4\\) = Interaction between GPA and IQ, and \\(X_5\\) = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \\(\\hat\\beta_0 = 50\\), \\(\\hat\\beta_1 = 20\\), \\(\\hat\\beta_2 = 0.07\\), \\(\\hat\\beta_3 = 35\\), \\(\\hat\\beta_4 = 0.01\\), \\(\\hat\\beta_5 = -10\\).\n\n\n\nWhich answer is correct, and why?\n\nFor a fixed value of IQ and GPA, high school graduates earn more on average than college graduates.\nFor a fixed value of IQ and GPA, college graduates earn more on average than high school graduates.\nFor a fixed value of IQ and GPA, high school graduates earn more on average than college graduates provided that the GPA is high enough.\nFor a fixed value of IQ and GPA, college graduates earn more on average than high school graduates provided that the GPA is high enough.\n\n\n\n\n\nFalse: \\(\\hat{\\beta}_3 &gt; 0\\)\nFalse: Have to take into account the interactions\nTrue: \\(\\hat{\\beta}_5 = -10\\), which means the slope for GPA decreases by 10 for college students. So for large enough GPAs, high school plane surpasses that of college\nFalse: above reason\n\n\n# fitted response curve\nmodel &lt;- function(gpa, iq, level) {\n  50 +\n  gpa * 20 +\n  iq * 0.07 +\n  level * 35 +\n  gpa * iq * 0.01 +\n  gpa * level * -10\n}\n\n# set predictors\nx &lt;- seq(1, 5, length = 10)\ny &lt;- seq(1, 200, length = 20)\n\n# calculate response\ncollege &lt;- outer(x, y, FUN = model, level = 1) %&gt;% t\nhigh_school &lt;- outer(x, y, FUN = model, level = 0) %&gt;% t\n\n# plot surfaces\nplot_ly(x = x, y = y) %&gt;% \n  add_surface(\n    z = ~college,\n    colorscale = list(c(0, 1), c(\"rgb(107,184,214)\", \"rgb(0,90,124)\")),\n    colorbar = list(title = \"College\")) %&gt;% \n  add_surface(\n    z = ~high_school,\n    colorscale = list(c(0, 1), c(\"rgb(255,112,184)\", \"rgb(128,0,64)\")),\n    colorbar = list(title = \"High school\")) %&gt;% \n  layout(scene = list(\n    xaxis = list(title = \"GPA\"),\n    yaxis = list(title = \"IQ\"),\n    zaxis = list(title = \"Salary\")))\n\nError in layout(., scene = list(xaxis = list(title = \"GPA\"), yaxis = list(title = \"IQ\"), : unused argument (scene = list(xaxis = list(title = \"GPA\"), yaxis = list(title = \"IQ\"), zaxis = list(title = \"Salary\")))\n\n\n\n\nPredict the salary of a college graduate with IQ of 110 and a GPA of 4.0.\n\n\n\nmodel(gpa = 4, iq = 110, level = 1)\n\n[1] 137.1\n\n\n\n\nTrue or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n\n\nFalse. Scale is based on units, need information about the significance.\nQuestion 4\n\nI collect a set of data (\\(n = 100\\) observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. \\(Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\epsilon\\).\n\n\n\nSuppose that the true relationship between \\(X\\) and \\(Y\\) is linear, i.e. \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\). Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n\n\nThe more complex model will have a smaller training RSS because of the lower bias.\n\n\nAnswer (a) using test rather than training RSS.\n\n\nGiven that the true model is actually linear, the more complex model will have a larger testing RSS because of the larger variance (overfitting).\n\n\nSuppose that the true relationship between \\(X\\) and \\(Y\\) is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n\n\nThe further from linear the true model gets, the larger the training RSS will become for the linear model because of low flexibility and high bias.\n\n\nAnswer (c) using test rather than training RSS.\n\n\nSame as (c).\nQuestion 5\n\nConsider the fitted values that result from performing linear regression without an intercept. In this setting, the \\(i\\)th fitted value takes the form \\[\n\\hat{y}_i = x_i\\hat\\beta\n\\] where \\[\n\\hat{\\beta} = \\left(\\sum_{i=1}^nx_iy_i\\right) / \\left(\\sum_{i' = 1}^n x^2_{i'}\\right).\n\\] Show that we can write \\[\n\\hat{y}_i = \\sum_{i' = 1}^na_{i'}y_{i'}\n\\] What is \\(a_{i'}\\)? Note: We interpret this result by saying that the fitted values from linear regression are linear combinations of the response values.\n\n\\[\n\\begin{align}\n\\hat{y}_i\n  & = x_i \\frac{\\sum_{i=1}^nx_iy_i}{\\sum_{i' = 1}^n x^2_{i'}} \\\\\n  & = x_i \\frac{\\sum_{i'=1}^nx_{i'}y_{i'}}{\\sum_{i'' = 1}^n x^2_{i''}} \\\\\n  & = \\frac{\\sum_{i'=1}^n x_i x_{i'}y_{i'}}{\\sum_{i'' = 1}^n x^2_{i''}} \\\\\n  & = \\sum_{i'=1}^n \\frac{ x_i x_{i'}y_{i'}}{\\sum_{i'' = 1}^n x^2_{i''}} \\\\\n  & = \\sum_{i'=1}^n \\frac{ x_i x_{i'}}{\\sum_{i'' = 1}^n x^2_{i''}} y_{i'}\n\\end{align}\n\\]\nQuestion 6\n\nUsing (3.4), argue that in the case of simple linear regression, the least squares line always passes through the point \\((\\bar{x}, \\bar{y})\\).\n\n\\[\n\\begin{align}\n\\hat{y} &= \\hat\\beta_0 + \\hat\\beta_1\\bar{x} \\\\\n  &= (\\bar{y} - \\hat\\beta_1\\bar{x}) + \\hat\\beta_1\\bar{x} \\\\\n  &= \\bar{y}\n\\end{align}\n\\]\nQuestion 7\n\nIt is claimed in the text that in the case of simple linear regression of \\(Y\\) onto \\(X\\), the \\(R^2\\) statistic (3.17) is equal to the square of the correlation between \\(X\\) and \\(Y\\) (3.18). Prove that this is the case. For simplicity, you may assume that \\(\\bar{x} = \\bar{y} = 0\\).\n\n&lt; just algebra after making simplifying assumptions &gt;\n\n4.3.2 Applied\nQuestion 8\n\n# read in data\ndata_car &lt;- ISLR2::Auto\nplot(mpg ~ horsepower, data_car)\n\n# fit model\nmod_car &lt;- lm(mpg ~ horsepower, data_car)\nplot(mpg ~ horsepower, data_car)\nabline(mod_car, col = \"red\")\n\n\n\n\n\n\nglimpse(mod_car)\n\nList of 12\n $ coefficients : Named num [1:2] 39.936 -0.158\n  ..- attr(*, \"names\")= chr [1:2] \"(Intercept)\" \"horsepower\"\n $ residuals    : Named num [1:392] -1.416 1.109 1.741 -0.259 -0.838 ...\n  ..- attr(*, \"names\")= chr [1:392] \"1\" \"2\" \"3\" \"4\" ...\n $ effects      : Named num [1:392] -464.206 120.138 1.745 -0.255 -0.819 ...\n  ..- attr(*, \"names\")= chr [1:392] \"(Intercept)\" \"horsepower\" \"\" \"\" ...\n $ rank         : int 2\n $ fitted.values: Named num [1:392] 19.4 13.9 16.3 16.3 17.8 ...\n  ..- attr(*, \"names\")= chr [1:392] \"1\" \"2\" \"3\" \"4\" ...\n $ assign       : int [1:2] 0 1\n $ qr           :List of 5\n  ..$ qr   : num [1:392, 1:2] -19.799 0.0505 0.0505 0.0505 0.0505 ...\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. ..- attr(*, \"assign\")= int [1:2] 0 1\n  ..$ qraux: num [1:2] 1.05 1.08\n  ..$ pivot: int [1:2] 1 2\n  ..$ tol  : num 0.0000001\n  ..$ rank : int 2\n  ..- attr(*, \"class\")= chr \"qr\"\n $ df.residual  : int 390\n $ xlevels      : Named list()\n $ call         : language lm(formula = mpg ~ horsepower, data = data_car)\n $ terms        :Classes 'terms', 'formula'  language mpg ~ horsepower\n  .. ..- attr(*, \"variables\")= language list(mpg, horsepower)\n  .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. ..- attr(*, \"term.labels\")= chr \"horsepower\"\n  .. ..- attr(*, \"order\")= int 1\n  .. ..- attr(*, \"intercept\")= int 1\n  .. ..- attr(*, \"response\")= int 1\n  .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. ..- attr(*, \"predvars\")= language list(mpg, horsepower)\n  .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"horsepower\"\n $ model        :'data.frame':  392 obs. of  2 variables:\n  ..$ mpg       : num [1:392] 18 15 18 16 17 15 14 14 14 15 ...\n  ..$ horsepower: int [1:392] 130 165 150 150 140 198 220 215 225 190 ...\n  ..- attr(*, \"terms\")=Classes 'terms', 'formula'  language mpg ~ horsepower\n  .. .. ..- attr(*, \"variables\")= language list(mpg, horsepower)\n  .. .. ..- attr(*, \"factors\")= int [1:2, 1] 0 1\n  .. .. .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..- attr(*, \"term.labels\")= chr \"horsepower\"\n  .. .. ..- attr(*, \"order\")= int 1\n  .. .. ..- attr(*, \"intercept\")= int 1\n  .. .. ..- attr(*, \"response\")= int 1\n  .. .. ..- attr(*, \".Environment\")=&lt;environment: R_GlobalEnv&gt; \n  .. .. ..- attr(*, \"predvars\")= language list(mpg, horsepower)\n  .. .. ..- attr(*, \"dataClasses\")= Named chr [1:2] \"numeric\" \"numeric\"\n  .. .. .. ..- attr(*, \"names\")= chr [1:2] \"mpg\" \"horsepower\"\n - attr(*, \"class\")= chr \"lm\"\n\n# get strength\nmod_car %&gt;% broom::glance() %&gt;% pull(r.squared)\n\n[1] 0.6059483\n\n# get coef\nbroom::tidy(mod_car)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   39.9     0.717        55.7 1.22e-187\n2 horsepower    -0.158   0.00645     -24.5 7.03e- 81\n\n# inference\n# -&gt; prediction of new obs\npredict(mod_car, newdata = data.frame(horsepower = c(98)), interval = \"pred\")\n\n       fit     lwr      upr\n1 24.46708 14.8094 34.12476\n\npredict(mod_car, newdata = data.frame(horsepower = c(98)), interval = \"conf\")\n\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\n\n# diagnostic plots\nplot(mod_car, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n# -&gt; nonlinearity and unequal variance\n\nQuestion 9\n\n# scatterplot matrix\npairs(data_car)\n\n\n\n\n\n\n# correlation matrix\ncor(data_car[, 1:8])\n\n                    mpg  cylinders displacement horsepower     weight\nmpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442\ncylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273\ndisplacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944\nhorsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377\nweight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000\nacceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392\nyear          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199\norigin        0.5652088 -0.5689316   -0.6145351 -0.4551715 -0.5850054\n             acceleration       year     origin\nmpg             0.4233285  0.5805410  0.5652088\ncylinders      -0.5046834 -0.3456474 -0.5689316\ndisplacement   -0.5438005 -0.3698552 -0.6145351\nhorsepower     -0.6891955 -0.4163615 -0.4551715\nweight         -0.4168392 -0.3091199 -0.5850054\nacceleration    1.0000000  0.2903161  0.2127458\nyear            0.2903161  1.0000000  0.1815277\norigin          0.2127458  0.1815277  1.0000000\n\n# fit model\nmod_car_mlr &lt;- lm(mpg ~ . - name, data = data_car, x = TRUE)\nsummary(mod_car_mlr)\n\n\nCall:\nlm(formula = mpg ~ . - name, data = data_car, x = TRUE)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.5903 -2.1565 -0.1169  1.8690 13.0604 \n\nCoefficients:\n               Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)  -17.218435   4.644294  -3.707              0.00024 ***\ncylinders     -0.493376   0.323282  -1.526              0.12780    \ndisplacement   0.019896   0.007515   2.647              0.00844 ** \nhorsepower    -0.016951   0.013787  -1.230              0.21963    \nweight        -0.006474   0.000652  -9.929 &lt; 0.0000000000000002 ***\nacceleration   0.080576   0.098845   0.815              0.41548    \nyear           0.750773   0.050973  14.729 &lt; 0.0000000000000002 ***\norigin         1.426141   0.278136   5.127          0.000000467 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.328 on 384 degrees of freedom\nMultiple R-squared:  0.8215,    Adjusted R-squared:  0.8182 \nF-statistic: 252.4 on 7 and 384 DF,  p-value: &lt; 0.00000000000000022\n\n# is a relationship between Y and the set of Xs\n# -&gt; displacement, weight, year and origin appear to be significant predictors\n# -&gt; hat(beta)_year &gt;  0 ==&gt; newer cars get better gas mileage\n\n# diagnostic plots\nplot(mod_car_mlr, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n# -&gt; unequal variance\n\n# search for higher order effects\n\n# get residuals\ne &lt;- residuals(mod_car_mlr) \n\n# plot against each X\nnms_x &lt;- colnames(mod_car_mlr$x[, -1])\nmap2(data.frame(mod_car_mlr$x)[, -1], nms_x, function(x, nm) {\n  plot(x = x, y = e, main = nm)\n  lines(lowess(x = x, y = e), col = \"red\")\n  abline(h = 0, col = \"grey\")\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$cylinders\nNULL\n\n$displacement\nNULL\n\n$horsepower\nNULL\n\n$weight\nNULL\n\n$acceleration\nNULL\n\n$year\nNULL\n\n$origin\nNULL\n\n# if there is relationship of any kind, that indicates there is something more going on because we have controlled for the linear main effects\n# -&gt; something with displacement, horsepower, weight, and, year\n# -&gt; combined with the hereditary principle, these seem like good variables\n\n# try interaction effects\nmod_car_int &lt;- update(mod_car_mlr, . ~ . + displacement:weight + displacement:year + displacement:origin + weight:year + weight:origin + year:origin, x = TRUE)\nsummary(mod_car_int)\n\n\nCall:\nlm(formula = mpg ~ cylinders + displacement + horsepower + weight + \n    acceleration + year + origin + displacement:weight + displacement:year + \n    displacement:origin + weight:year + weight:origin + year:origin, \n    data = data_car, x = TRUE)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.717 -1.608 -0.033  1.301 13.031 \n\nCoefficients:\n                         Estimate    Std. Error t value           Pr(&gt;|t|)    \n(Intercept)         -41.805566419  24.906721073  -1.678           0.094079 .  \ncylinders             0.452489261   0.304470575   1.486           0.138073    \ndisplacement          0.080930869   0.082684765   0.979           0.328311    \nhorsepower           -0.044418845   0.012621585  -3.519           0.000485 ***\nweight               -0.006558690   0.011117698  -0.590           0.555588    \nacceleration          0.113084940   0.086916781   1.301           0.194027    \nyear                  1.279561152   0.323065989   3.961 0.0000893223144297 ***\norigin               -1.670022247   5.242728051  -0.319           0.750251    \ndisplacement:weight   0.000022021   0.000002833   7.772 0.0000000000000736 ***\ndisplacement:year    -0.002170403   0.001144312  -1.897           0.058631 .  \ndisplacement:origin   0.011891682   0.012453639   0.955           0.340251    \nweight:year          -0.000055344   0.000150271  -0.368           0.712861    \nweight:origin         0.000206268   0.000967806   0.213           0.831340    \nyear:origin           0.004978089   0.066417438   0.075           0.940293    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.856 on 378 degrees of freedom\nMultiple R-squared:  0.8705,    Adjusted R-squared:  0.8661 \nF-statistic: 195.5 on 13 and 378 DF,  p-value: &lt; 0.00000000000000022\n\n# only a couple of the interactions are significant\n\nplot(mod_car_int, which = 1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n# get residuals\ne &lt;- residuals(mod_car_int) \n\n# plot against each X\nnms_x &lt;- colnames(mod_car_int$x[, -1])\nmap2(data.frame(mod_car_int$x)[, -1], nms_x, function(x, nm) {\n  plot(x = x, y = e, main = nm)\n  lines(lowess(x = x, y = e), col = \"red\")\n  abline(h = 0, col = \"grey\")\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$cylinders\nNULL\n\n$displacement\nNULL\n\n$horsepower\nNULL\n\n$weight\nNULL\n\n$acceleration\nNULL\n\n$year\nNULL\n\n$origin\nNULL\n\n$displacement.weight\nNULL\n\n$displacement.year\nNULL\n\n$displacement.origin\nNULL\n\n$weight.year\nNULL\n\n$weight.origin\nNULL\n\n$year.origin\nNULL\n\n# now only unequal variance is left after controlling for interactions as well\n# -&gt; except for maybe year\n# -&gt; every function of year is significant\nmod_car_int2 &lt;- update(mod_car_int, . ~ . + sqrt(year), data = data_car)\n\n# see if significant\nanova(mod_car_int2, mod_car_int)\n\nAnalysis of Variance Table\n\nModel 1: mpg ~ cylinders + displacement + horsepower + weight + acceleration + \n    year + origin + sqrt(year) + displacement:weight + displacement:year + \n    displacement:origin + weight:year + weight:origin + year:origin\nModel 2: mpg ~ cylinders + displacement + horsepower + weight + acceleration + \n    year + origin + displacement:weight + displacement:year + \n    displacement:origin + weight:year + weight:origin + year:origin\n  Res.Df    RSS Df Sum of Sq     F     Pr(&gt;F)    \n1    377 2927.9                                  \n2    378 3083.5 -1   -155.63 20.04 0.00001006 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nQuestion 10\n\n# read in data\n# -&gt; price = quan, urban and us = qual\ndata_seats &lt;- ISLR2::Carseats\n\n# fit mlr model\nmod_seats &lt;- lm(Sales ~ Price + Urban + US, data = data_seats, x = TRUE)\nmod_seats %&gt;% broom::tidy()\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  13.0      0.651     20.0    3.63e-62\n2 Price        -0.0545   0.00524  -10.4    1.61e-22\n3 UrbanYes     -0.0219   0.272     -0.0807 9.36e- 1\n4 USYes         1.20     0.259      4.63   4.86e- 6\n\n\nCoefficient interpretations\n\\(\\hat{\\beta}_0\\) = 13.043: For a non-urban store located outside of the US, we expect there to be 13,043 car seat sales on average.\n\\(\\hat{\\beta}_{\\text{Price}}\\) = -0.054: For each dollar increase in the price of the carseat, we expect the unit sales to decrease by approximately 54 units, on average.\n\\(\\hat{\\beta}_{\\text{UrbanYes}}\\) = -0.022: Stores located in urban areas have approximately 22 less sales on average than non-urban stores.\n\\(\\hat{\\beta}_{\\text{USYes}}\\) = 1.201: US stores have approximately 1201 more sales on average than non-US stores.\nWritten-out model\n\\[\n\\text{Sales} = \\Bigg(\\hat{\\beta}_0 + \\begin{cases}\n   \\hat{\\beta}_{\\text{UrbanYes}},     & \\text{if $\\text{Urban}$ = Yes, $\\text{US}$ = No} \\\\\n    \\hat{\\beta}_{\\text{USYes}},    & \\text{if $\\text{Urban}$ = No, $\\text{US}$ = Yes} \\\\\n    \\hat{\\beta}_{\\text{UrbanYes}} + \\hat{\\beta}_{\\text{USYes}},    & \\text{if $\\text{Urban}$ = $\\text{US}$ = Yes} \\\\\n    0,       & \\text{Otherwise}\n\\end{cases}\\Bigg)\n+ \\hat{\\beta}_{\\text{Price}} \\times \\text{Price}\n\\]\nwhich come out to be\n\\[\n\\text{Sales} = \\Bigg(13 + \\begin{cases}\n   -0.022,   & \\text{if $\\text{Urban}$ is Yes, $\\text{US}$ is No} \\\\\n    1.20,    & \\text{if $\\text{Urban}$ is No, $\\text{US}$ is Yes} \\\\\n    1.18,    & \\text{if $\\text{Urban}$ and $\\text{US}$ is Yes} \\\\\n    0,       & \\text{Otherwise}\n\\end{cases}\n\\Bigg)\n- 0.054 \\times \\textit{Price}\n\\]\n\n# reject null for Price and US (not for Urban)\n\n# fit smaller model and test\nmod_seats2 &lt;- update(mod_seats, . ~ . - Urban)\nanova(mod_seats, mod_seats2)\n\nAnalysis of Variance Table\n\nModel 1: Sales ~ Price + Urban + US\nModel 2: Sales ~ Price + US\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    396 2420.8                           \n2    397 2420.9 -1  -0.03979 0.0065 0.9357\n\n# -&gt; fail to reject --&gt; reduced model is better\n\nmod_seats %&gt;% broom::glance() %&gt;% pull(r.squared)\n\n[1] 0.2392754\n\nmod_seats2 %&gt;% broom::glance() %&gt;% pull(r.squared)\n\n[1] 0.2392629\n\n# -&gt; just about the same fit\n\n# 95% confidence intervals for coefficients on reduced model\nconfint(mod_seats2)\n\n                  2.5 %      97.5 %\n(Intercept) 11.79032020 14.27126531\nPrice       -0.06475984 -0.04419543\nUSYes        0.69151957  1.70776632\n\n# inspect for outliers / high leverage observations\nplot(mod_seats2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# -&gt; there are a few high leverage points, but they are not influential\n\nQuestion 11\n\n# generate data\nset.seed(1)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100)\n\n# fit models without intercept\nmod_yx &lt;- lm(y ~ x + 0)\nsummary(mod_yx)\n\n\nCall:\nlm(formula = y ~ x + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9154 -0.6472 -0.1771  0.5056  2.3109 \n\nCoefficients:\n  Estimate Std. Error t value            Pr(&gt;|t|)    \nx   1.9939     0.1065   18.73 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9586 on 99 degrees of freedom\nMultiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 \nF-statistic: 350.7 on 1 and 99 DF,  p-value: &lt; 0.00000000000000022\n\nmod_xy &lt;- lm(x ~ y + 0)\nsummary(mod_xy)\n\n\nCall:\nlm(formula = x ~ y + 0)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8699 -0.2368  0.1030  0.2858  0.8938 \n\nCoefficients:\n  Estimate Std. Error t value            Pr(&gt;|t|)    \ny  0.39111    0.02089   18.73 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4246 on 99 degrees of freedom\nMultiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 \nF-statistic: 350.7 on 1 and 99 DF,  p-value: &lt; 0.00000000000000022\n\n# results\n# -&gt; different estimates, same test-statistics\n# -&gt; this can be shown mathematically using the derived formulas for regression without an intercept\n\n# now with an intercept, show test statistics are the same for y ~ x and x ~ y\nmod_yx &lt;- lm(y ~ x)\nmod_yx %&gt;% broom::glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.778         0.776 0.963      344. 7.72e-34     1  -137.  280.  288.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nmod_xy &lt;- lm(x ~ y)\nmod_xy %&gt;% broom::glance()\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.778         0.776 0.425      344. 7.72e-34     1  -55.3  117.  124.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nQuestion 12\nCan show that estimates will be the same if swap \\(X\\) and \\(Y\\) variables under a certain condition.\n\\[\nY \\sim X: \\hat\\beta = \\sum_i x_iy_i / \\sum_{i'} x_{i'}^2\n\\]\nThe coefficient for the regression of X onto Y swaps the \\(x\\) and \\(y\\) variables:\n\\[\nX \\sim Y: \\hat\\beta = \\sum_i x_iy_i / \\sum_{i'} y_{i'}^2\n\\]\nSo they are the same when \\(\\sum_{i} x_{i}^2 = \\sum_{i} y_{i}^2\\)\nQuestion 13\n\n# investigate coefficient confidence intervals based on the amount of noise\nset.seed(1)\nx &lt;- rnorm(100)\nbeta0 &lt;- -1\nbeta1 &lt;- 0.5\n\n# generate three response vecotrs with difference error variances\ny &lt;- c(0.5, 0.1, 1) %&gt;% map(\\(epsilon) beta0 + beta1 * x + rnorm(100, 0, epsilon))\nnames(y) &lt;- c(\"original epsilon\", \"smaller epsilon\", \"larger epsilon\")\n\n# fit models based on same x\nmods &lt;- y %&gt;% map(\\(y) lm(y ~ x))\n\n# calculate coefficient confidence intervals\nmods %&gt;% map(\\(mod) coef(mod)[2])\n\n$`original epsilon`\n        x \n0.4994698 \n\n$`smaller epsilon`\n        x \n0.5021167 \n\n$`larger epsilon`\n        x \n0.4443139 \n\nmods %&gt;% map(\\(mod) confint(mod))\n\n$`original epsilon`\n                 2.5 %     97.5 %\n(Intercept) -1.1150804 -0.9226122\nx            0.3925794  0.6063602\n\n$`smaller epsilon`\n                 2.5 %     97.5 %\n(Intercept) -1.0180413 -0.9764850\nx            0.4790377  0.5251957\n\n$`larger epsilon`\n                 2.5 %     97.5 %\n(Intercept) -1.1413399 -0.7433293\nx            0.2232721  0.6653558\n\n# wider confidence intervals with more noise as expected, and more biased fits\n\nQuestion 14\n\n# generate data\nset.seed(1)\nx1 &lt;- runif(100)\nx2 &lt;- 0.5 * x1 + rnorm(100) / 10\ny &lt;- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)\n\nThe model is of the form:\n\\[\nY = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\epsilon\n\\]\nThe coefficients are \\(\\beta_0 = 2\\), \\(\\beta_1 = 2\\), \\(\\beta_3 = 0.3\\).\n\n# investigate relationship between x1 and x2\ncor(x1, x2)\n\n[1] 0.8351212\n\nplot(x1, x2)\n\n\n\n\n\n\n# fit model\nmod &lt;- lm(y ~ x1 + x2)\n\n# compare fitted coefficients to population values, then test\ncoef(mod)\n\n(Intercept)          x1          x2 \n   2.130500    1.439555    1.009674 \n\nmod %&gt;% broom::tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     2.13     0.232     9.19  7.61e-15\n2 x1              1.44     0.721     2.00  4.87e- 2\n3 x2              1.01     1.13      0.891 3.75e- 1\n\n# fail to reject for x2\n\n# now fit models with the predictors individually\nmod1 &lt;- lm(y ~ x1)\nmod2 &lt;- lm(y ~ x2)\nmod1 %&gt;% broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     2.11     0.231      9.15 8.27e-15\n2 x1              1.98     0.396      4.99 2.66e- 6\n\nmod2 %&gt;% broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     2.39     0.195     12.3  1.68e-21\n2 x2              2.90     0.633      4.58 1.37e- 5\n\n# both reject\n# -&gt; this contradicts the mlr model where x2 was not significant\n\n# obtain new observations\nx1 &lt;- c(x1, 0.1)\nx2 &lt;- c(x2, 0.8)\ny &lt;- c(y, 6)\n\n# refit models and see effect\nmod_new &lt;- lm(y ~ x1 + x2)\nmod1_new &lt;- lm(y ~ x1)\nmod2_new &lt;- lm(y ~ x2)\nmod_new %&gt;% broom::tidy()\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    2.23      0.231     9.62  7.91e-16\n2 x1             0.539     0.592     0.911 3.65e- 1\n3 x2             2.51      0.898     2.80  6.14e- 3\n\nmod1_new %&gt;% broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     2.26     0.239      9.44 1.78e-15\n2 x1              1.77     0.412      4.28 4.29e- 5\n\nmod2_new %&gt;% broom::tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     2.35     0.191     12.3  1.40e-21\n2 x2              3.12     0.604      5.16 1.25e- 6\n\n# seems similar results in the individual predictor models, however in the mlr model now both are significant -&gt; investigate why\nplot(mod_new) # high leverage with respect to x1 and x2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(y ~ x1)\npoints(0.1, 6, col = \"red\") # outlier in y\n\n\n\n\n\n\nplot(y ~ x2)\npoints(0.8, 6, col = \"red\") # high leverage (i.e. outlier in x)\n\n\n\n\n\n\n# useful to consider plot of the xs\nplot(x1, x2)\npoints(0.1, 0.8, col = \"red\")\n\n\n\n\n\n\n\nQuestion 15\n\n# read in data\ndata_boston &lt;- ISLR2::Boston\n\n# fit many SLR models\nmods &lt;- data_boston %&gt;% \n  select(-crim) %&gt;% \n  map(\\(x) lm(data_boston$crim ~ x))\nmods %&gt;% map(\\(mod) broom::glance(mod)[1,4:5])\n\n$zn\n# A tibble: 1 × 2\n  statistic    p.value\n      &lt;dbl&gt;      &lt;dbl&gt;\n1      21.1 0.00000551\n\n$indus\n# A tibble: 1 × 2\n  statistic  p.value\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      99.8 1.45e-21\n\n$chas\n# A tibble: 1 × 2\n  statistic p.value\n      &lt;dbl&gt;   &lt;dbl&gt;\n1      1.58   0.209\n\n$nox\n# A tibble: 1 × 2\n  statistic  p.value\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      109. 3.75e-23\n\n$rm\n# A tibble: 1 × 2\n  statistic     p.value\n      &lt;dbl&gt;       &lt;dbl&gt;\n1      25.5 0.000000635\n\n$age\n# A tibble: 1 × 2\n  statistic  p.value\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      71.6 2.85e-16\n\n$dis\n# A tibble: 1 × 2\n  statistic  p.value\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      84.9 8.52e-19\n\n$rad\n# A tibble: 1 × 2\n  statistic  p.value\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      324. 2.69e-56\n\n$tax\n# A tibble: 1 × 2\n  statistic  p.value\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      259. 2.36e-47\n\n$ptratio\n# A tibble: 1 × 2\n  statistic  p.value\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      46.3 2.94e-11\n\n$lstat\n# A tibble: 1 × 2\n  statistic  p.value\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      132. 2.65e-27\n\n$medv\n# A tibble: 1 × 2\n  statistic  p.value\n      &lt;dbl&gt;    &lt;dbl&gt;\n1      89.5 1.17e-19\n\n# -&gt; all significant except chas\n\n# fit mlr model\nmod_mlr &lt;- lm( crim ~ ., data = data_boston)\nsummary(mod_mlr)\n\n\nCall:\nlm(formula = crim ~ ., data = data_boston)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.534 -2.248 -0.348  1.087 73.923 \n\nCoefficients:\n              Estimate Std. Error t value         Pr(&gt;|t|)    \n(Intercept) 13.7783938  7.0818258   1.946         0.052271 .  \nzn           0.0457100  0.0187903   2.433         0.015344 *  \nindus       -0.0583501  0.0836351  -0.698         0.485709    \nchas        -0.8253776  1.1833963  -0.697         0.485841    \nnox         -9.9575865  5.2898242  -1.882         0.060370 .  \nrm           0.6289107  0.6070924   1.036         0.300738    \nage         -0.0008483  0.0179482  -0.047         0.962323    \ndis         -1.0122467  0.2824676  -3.584         0.000373 ***\nrad          0.6124653  0.0875358   6.997 0.00000000000859 ***\ntax         -0.0037756  0.0051723  -0.730         0.465757    \nptratio     -0.3040728  0.1863598  -1.632         0.103393    \nlstat        0.1388006  0.0757213   1.833         0.067398 .  \nmedv        -0.2200564  0.0598240  -3.678         0.000261 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.46 on 493 degrees of freedom\nMultiple R-squared:  0.4493,    Adjusted R-squared:  0.4359 \nF-statistic: 33.52 on 12 and 493 DF,  p-value: &lt; 0.00000000000000022\n\n# -&gt; can now only reject for zn, nox, dis, rad, lstat and medv\n\n# extract two sets of coefficients and plot against each other\nbetas_slr &lt;- mods %&gt;% map_dbl(\\(mod) broom::tidy(mod)[2,2] %&gt;% as.numeric)\nbetas_mlr &lt;- coef(mod_mlr)[-1]\nplot(betas_mlr ~ betas_slr)\n\n\n\n\n\n\n\nThe estimated coefficients differ (in particular the estimated coefficient for nox is dramatically different) between the two modelling strategies.\n\n# fit many cubic models models\nmods_cubic &lt;- data_boston %&gt;% \n  select(-c(crim, chas)) %&gt;% \n  map(\\(x) lm(data_boston$crim ~ poly(x, 3)))\nmods_cubic %&gt;% map(\\(mod) broom::tidy(mod))\n\n$zn\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.61     0.372      9.71 1.55e-20\n2 poly(x, 3)1   -38.7      8.37      -4.63 4.70e- 6\n3 poly(x, 3)2    23.9      8.37       2.86 4.42e- 3\n4 poly(x, 3)3   -10.1      8.37      -1.20 2.30e- 1\n\n$indus\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.61     0.330     11.0  3.61e-25\n2 poly(x, 3)1    78.6      7.42      10.6  8.85e-24\n3 poly(x, 3)2   -24.4      7.42      -3.29 1.09e- 3\n4 poly(x, 3)3   -54.1      7.42      -7.29 1.20e-12\n\n$nox\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.61     0.322     11.2  2.74e-26\n2 poly(x, 3)1    81.4      7.23      11.2  2.46e-26\n3 poly(x, 3)2   -28.8      7.23      -3.99 7.74e- 5\n4 poly(x, 3)3   -60.4      7.23      -8.34 6.96e-16\n\n$rm\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.61     0.370     9.76  1.03e-20\n2 poly(x, 3)1   -42.4      8.33     -5.09  5.13e- 7\n3 poly(x, 3)2    26.6      8.33      3.19  1.51e- 3\n4 poly(x, 3)3    -5.51     8.33     -0.662 5.09e- 1\n\n$age\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.61     0.349     10.4  5.92e-23\n2 poly(x, 3)1    68.2      7.84       8.70 4.88e-17\n3 poly(x, 3)2    37.5      7.84       4.78 2.29e- 6\n4 poly(x, 3)3    21.4      7.84       2.72 6.68e- 3\n\n$dis\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.61     0.326     11.1  1.06e-25\n2 poly(x, 3)1   -73.4      7.33     -10.0  1.25e-21\n3 poly(x, 3)2    56.4      7.33       7.69 7.87e-14\n4 poly(x, 3)3   -42.6      7.33      -5.81 1.09e- 8\n\n$rad\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.61     0.297    12.2   5.15e-30\n2 poly(x, 3)1   121.       6.68     18.1   1.05e-56\n3 poly(x, 3)2    17.5      6.68      2.62  9.12e- 3\n4 poly(x, 3)3     4.70     6.68      0.703 4.82e- 1\n\n$tax\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.61     0.305     11.9  8.96e-29\n2 poly(x, 3)1   113.       6.85      16.4  6.98e-49\n3 poly(x, 3)2    32.1      6.85       4.68 3.67e- 6\n4 poly(x, 3)3    -8.00     6.85      -1.17 2.44e- 1\n\n$ptratio\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.61     0.361     10.0  1.27e-21\n2 poly(x, 3)1    56.0      8.12       6.90 1.57e-11\n3 poly(x, 3)2    24.8      8.12       3.05 2.41e- 3\n4 poly(x, 3)3   -22.3      8.12      -2.74 6.30e- 3\n\n$lstat\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.61     0.339     10.7  4.94e-24\n2 poly(x, 3)1    88.1      7.63      11.5  1.68e-27\n3 poly(x, 3)2    15.9      7.63       2.08 3.78e- 2\n4 poly(x, 3)3   -11.6      7.63      -1.52 1.30e- 1\n\n$medv\n# A tibble: 4 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     3.61     0.292     12.4  7.02e-31\n2 poly(x, 3)1   -75.1      6.57     -11.4  4.93e-27\n3 poly(x, 3)2    88.1      6.57      13.4  2.93e-35\n4 poly(x, 3)3   -48.0      6.57      -7.31 1.05e-12\n\n\nSeveral models have a significant cubic term, if not then the quadratic term is significant for the rest.",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>ISLR -- Linear regression</span>"
    ]
  },
  {
    "objectID": "islr-4.html",
    "href": "islr-4.html",
    "title": "\n5  ISLR – Classification\n",
    "section": "",
    "text": "5.1 Notes",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ISLR -- Classification</span>"
    ]
  },
  {
    "objectID": "islr-4.html#notes",
    "href": "islr-4.html#notes",
    "title": "\n5  ISLR – Classification\n",
    "section": "",
    "text": "5.1.1 An overview of classification\nThe linear regression model discussed in earlier assumes that the response variable \\(Y\\) is quantitative. But in many situations, the response variable is instead qualitative. In this chapter, we study approaches for predicting qualitative responses, a process that is known as classification.\nJust as in the regression setting, in the classification setting we have a set of training observations \\((x_1,y_1), \\ldots, (x_n,y_n)\\) that we can use to build a classifier. We want our classifier to perform well not only on the training data, but also on test observations that were not used to train the classifier.\n\n5.1.2 Why not linear regression?\nCould try to code categories to numbers such as\n\\[\nY = \\begin{cases}\n   1  & \\text{if a} \\\\\n   2  & \\text{if b} \\\\\n   3  & \\text{if c} \\\\\n\\end{cases}\n\\] However, this implies an ordering of the outcomes, which means ‘b’ is above ‘a’, ‘c’ is above ‘b’ and the difference between ‘a’ and ‘b’ is the same as the difference between ‘b’ and ‘c’. Typically with categorical variables, order is arbitrary, so it could easily be switched around and lead to a drastically different model. If there is a natural ordering, such as mild, moderate, and severe AND we felt the gap mild and moderate was similar to the gap between moderate and severe, then a 1, 2, 3 coding would be reasonable. Unfortunately, in general there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.\nFor a binary (two level) qualitative response, the situation is better. We can use a dummy variable approach to code the response:\n\\[\nY = \\begin{cases}\n   0   & \\text{if a} \\\\\n   1   & \\text{if b} \\\\\n\\end{cases}\n\\]\nWe could then fit a linear regression to this binary response, and predict ‘b’ if \\(\\hat{Y} &gt; 0.5\\) and ‘a’ otherwise. In this case, even if we flip the coding, the linear regression will produce the same final predictions (so coding doesn’t matter).\nFor a binary response with a 0/1 coding as above, regression by least squares is not completely unreasonable: it can be shown that the \\(X\\hat{\\beta}\\) obtained using linear regression is in fact an estimate of \\(P(Y = 1 \\mid X)\\) in this special case. However, if we use linear regression, some of our estimates might be outside the [0, 1] interval, making them hard to interpret as probabilities!\n\nNevertheless, the predictions provide an ordering and can be interpreted as crude probability estimates. Curiously, it turns out that the classifications that we get if we use linear regression to predict a binary response will be the same as for the linear discriminant analysis (LDA) procedure shown later.\nTo summarize, there are at least two reasons not to perform classification using a regression method: (a) a regression method cannot accommodate a qualitative response with more than two classes; (b) a regression method will not provide meaningful estimates of \\(P(Y \\mid X)\\), even with just two classes. Thus, it is preferable to use a classification method that is truly suited for qualitative response values.\n\n5.1.3 Logistic regression\nConsider the Default data set, where the response default falls into one of two categories, Yes or No. Rather than modeling this response Y directly, logistic regression models the probability that Y belongs to a particular category.\nFor the Default data, logistic regression models the probability of default. For example, the probability of default given balance can be written as\n\\[\nP(\\text{default} = \\text{Yes} \\mid \\text{balance})\n\\]\nThe values of \\(P(\\text{default} = \\text{Yes} \\mid \\text{balance})\\), which we abbreviate \\(p(\\text{balance})\\), will range between 0 and 1. Then for any given value of balance, a prediction can be made for default. For example, one might predict default = Yes for any individual for whom \\(p(\\text{balance}) &gt; 0.5\\). Alternatively, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as \\(p(\\text{balance}) &gt; 0.1\\).\nThe logisitic model\nHow should we model the relationship between \\(p(X) = P(Y = 1 \\mid X)\\) and \\(X\\)? Above, we considered using a linear regression model to represent these probabilities:\n\\[\np(X) = \\beta_0 + \\beta_1 X\n\\]\nIf we use this approach to predict default=Yes using balance, then we obtain the model shown in the left-hand panel of Figure 4.2, which results in the obvious problem of predictions out of bounds. Any time a straight line is fit to a binary response that is coded as 0 or 1, in principle we can always predict \\(p(X) &lt; 0\\) for some values of \\(X\\) and \\(p(X) &gt; 1\\) for others (unless the range of \\(X\\) is limited).\nTo avoid this problem, we must model \\(p(X)\\) using a function that gives outputs between 0 and 1 for all values of X. Many functions meet this description. In logistic regression, we use the logistic function,\n\\[\np(X) = \\frac{\\mathrm{e}^{\\beta_0 + \\beta_1 X}}{1 + \\mathrm{e}^{\\beta_0 + \\beta_1 X}}  = \\frac{1}{1 + \\mathrm{e}^{-\\beta_0 + \\beta_1 X}}\n\\tag{5.1}\\]\nTo fit this model, we use maximum likelihood. The right-hand panel of Figure 4.2 illustrates the fit of the logistic regression model to the Default data. Notice that for low balances we now predict the probability of default as close to, but never below, zero. Likewise, for high balances we predict a default probability close to, but never above, one. The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of \\(X\\), we will obtain a sensible prediction. We also see that the logistic model is better able to capture the range of probabilities than is the linear regression. The average fitted probability in both cases is 0.0333 (averaged over the training data), which is the same as the overall proportion of defaulters in the data set. We can show:\n\\[\n\\begin{align*}\np(X) = p &= \\frac{\\mathrm{e}^{\\beta_0 + \\beta_1 X}}{1 + \\mathrm{e}^{\\beta_0 + \\beta_1 X}} \\quad\\text{for simplicity}\\\\\np(1 + \\mathrm{e}^{\\beta_0 + \\beta_1 X}) &= \\mathrm{e}^{\\beta_0 + \\beta_1 X}\\\\\np + p\\mathrm{e}^{\\beta_0 + \\beta_1 X} &= \\mathrm{e}^{\\beta_0 + \\beta_1 X}\\\\\np + p\\mathrm{e}^{\\beta_0 + \\beta_1 X} - \\mathrm{e}^{\\beta_0 + \\beta_1 X} &= 0\\\\\np + \\mathrm{e}^{\\beta_0 + \\beta_1 X}(p - 1) &= 0\\\\\n\\mathrm{e}^{\\beta_0 + \\beta_1 X}(p - 1) &= -p\\\\\n\\mathrm{e}^{\\beta_0 + \\beta_1 X} &= \\frac{-p}{p - 1}\\\\\n\\mathrm{e}^{\\beta_0 + \\beta_1 X} &= \\frac{p(X)}{1 - p(X)}\\\\\n\\end{align*}\n\\]\nThe quantity \\(p(X) / [1 - p(X)]\\) is called the odds, and can take on any value between 0 and \\(\\infty\\). Values of the odds close to 0 and \\(\\infty\\) indicate very low and very high probabilities of default, respectively.For example, on average 1 in 5 people with an odds of 1/4 will default, since \\(p(X) = 0.2\\) implies an odds of \\(\\frac{0.2}{1 - 0.2} = 1/4\\).\nBy taking the logarithm of both sides, we arrive at the log odds or logit.\n\\[\n\\log\\Big(\\frac{p(X)}{1 - p(X)}\\Big) = \\beta_0 + \\beta_1 X\n\\tag{5.2}\\]\nThus, we see that the logistic regression model \\(p(X) = \\frac{\\mathrm{e}^{\\beta_0 + \\beta_1 X}}{1 + \\mathrm{e}^{\\beta_0 + \\beta_1 X}}\\) has a logit that is linear in \\(X\\).\nNow in a logistic regression model, increasing \\(X\\) by one unit changes the log odds by \\(\\beta_1\\). Equivalently, it multiplies the odds by \\(\\mathrm{e}^{\\beta_1}\\). However, because the relationship between \\(p(X)\\) and \\(X\\) is not a straight line anymore, \\(\\beta_1\\) does not correspond to the change in \\(p(X)\\) associated with a one-unit increase in X. The amount that \\(p(X)\\) changes due to a one-unit change in \\(X\\) depends on the current value of \\(X\\). But regardless of the value of \\(X\\), if \\(\\beta_1\\) is positive then increasing \\(X\\) will be associated with increasing \\(p(X)\\), and if \\(\\beta_1\\) is negative then increasing \\(X\\) will be associated with decreasing \\(p(X)\\).\nEstimating the regression coefficients\nThe coefficients in Equation 5.1 are unknown, and must be estimated based on the available training data. Although we could use (non-linear) least squares to fit the model Equation 5.2, the more general method of maximum likelihood is preferred, since it has better statistical properties. The basic intuition behind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for \\(\\beta_0\\) and \\(\\beta_1\\) such that the predicted probability \\(\\hat{p}(x_i)\\) of default for each individual, using Equation 5.1, corresponds as closely as possible to the individual’s observed default status. In other words, we try to find \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) such that plugging these estimates into the model for \\(p(X)\\), given in Equation 5.1, yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not. This intuition can be formalized using a mathematical equation called a likelihood function:\n\\[\n\\ell(\\beta_0, \\beta_1) = \\prod_{i:y_i = 1} p(x_i) \\prod_{i':y_{i'}=0} (1 - p(x_{i'}))\n\\tag{5.3}\\]\n(note that this notation is the same as the following, just without the powers)\n\\[\n\\ell(\\beta_0, \\beta_1) = \\prod_{i = 1}^n p(x_i)^{y_i} (1 - p(x_i))^{1 - y_i}\n\\]\nThe estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are chosen to maximize this likelihood function. Said another way: to minimize the mis-classification rate, we should predict \\(Y = 1\\) when \\(p \\ge 0.5\\) and \\(Y = 0\\) when \\(p &lt; 0.5\\). This means guessing 1 whenever \\(\\beta_0 + X\\beta\\) is non-negative (as seen in the last version of Equation 5.1, where \\(\\frac{1}{1 + \\mathrm{e}^{-0}} = 1/2\\) is the cutoff point), and 0 otherwise. So logistic regression gives us a linear classifier. The decision boundary separating the two predicted classes is the solution of \\(\\beta_0 + X \\beta_1 = 0\\), which is a point if \\(X\\) is one dimensional, a line if it is two dimensional, etc.\nMaximum likelihood is a very general approach that is used to fit many of the non-linear models that we examine throughout this book. In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood.\n\nMany aspects of the logistic regression output shown in Table 4.1 are similar to the linear regression output in the previous chapter. For example, we can measure the accuracy of the coefficient estimates by computing their standard errors. The z-statistic in Table 4.1 plays the same role as the t-statistic in the linear regression output. For instance, the z-statistic associated with \\(\\beta_1\\) is equal to \\(\\beta_1/SE(\\beta_1)\\), and so a large (absolute) value of the z-statistic indicates evidence against the null hypothesis \\(H_0: \\beta_1 = 0\\). This null hypothesis implies that \\(p(X) = e^{\\beta_0}\\): in other words, that the probability of default does not depend on balance. The estimated intercept in Table 4.1 is typically not of interest; its main purpose is to adjust the average fitted probabilities to the proportion of ones in the data (in this case, the overall default rate).\nMaking predictions\nOnce the coefficients have been estimated, we can compute the probability of default for any given credit card balance.\n\nMultiple logistic regression\nWe now consider the problem of predicting a binary response using multiple predictors. By analogy with the extension from simple to multiple linear regression in the previous chapter, we can generalize Equation 5.2 as follows:\n\\[\n\\log\\Big(\\frac{p(X)}{1 - p(X)}\\Big) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\tag{5.4}\\]\nThis can be rewritten as:\n\\[\np(X) = \\frac{\\mathrm{e}^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1 + \\mathrm{e}^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}\n\\tag{5.5}\\]\nThen we use maximum likelihood method as before. Beware of interpreting the effects (mainly the sign) of coefficients when some important predictors may be missing from your model. Confounding is an issue, just like in MLR.\nMultinomial logisitic regression\nWe sometimes wish to classify a response variable that has more than two classes. It turns out that it is possible to extend the two-class logistic regression approach to the setting of \\(K &gt; 2\\) classes. This extension is sometimes known as multinomial logistic regression. To do this, we first select a single class to serve as the baseline; without loss of generality, we select the \\(K\\)th class for this role. Then we replace the model Equation 5.5 with the model\n\\[\np(Y = k \\mid X = x) = \\frac{\\mathrm{e}^{\\beta_{k0} + \\beta_{k1} X_1 + \\cdots + \\beta_{kp} X_p}}{1 + \\sum_{l = 1}^{K - 1}\\mathrm{e}^{\\beta_{l0} + \\beta_{l1} X_1 + \\cdots + \\beta_{lp} X_p}}\n\\]\nfor \\(k = 1, \\ldots, K-1\\) and\n\\[\np(Y = K \\mid X = x) = \\frac{1}{1 + \\sum_{l = 1}^{K - 1}\\mathrm{e}^{\\beta_{l0} + \\beta_{l1} X_1 + \\cdots + \\beta_{lp} X_p}}\n\\]\nIt is not hard to show that for \\(k = 1, \\ldots, K-1\\),\n\\[\n\\log\\Big(\\frac{P(Y = k \\mid X = x)}{P(Y = K \\mid X = x)}\\Big) = \\beta_{k0} + \\beta_{k1} X_1 + \\cdots + \\beta_{kp} X_p\n\\tag{5.6}\\]\nNotice that this is quite similar to Equation 5.2 and indicates that once again, the log odds between any pair of classes is linear in the features. The decision of which class to have as the baseline is unimportant. If we were to change the baseline, the coefficient estimates would differ between the two fitted models, but the fitted values (predictions), the log odds between any pair of classes, and the other key model outputs will remain the same. Nonetheless, interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline.\n\nTextbook presents an alternative coding for multinomial logistic regression, known as the softmax coding. This is used a lot in machine learning; skipping for now.\n\n5.1.4 Generative models for classification\nLogistic regression involves directly modeling \\(P(Y = k \\mid X = x)\\) using the logistic function, given by eq-logistic-function-multiple for the case of two response classes. In statistical jargon, we model the conditional distribution of the response \\(Y\\), given the predictor(s) \\(X\\). We now consider an alternative and less direct approach to estimating these probabilities. In this new approach, we model the distribution of the predictors \\(X\\) separately in each of the response classes (i.e. for each value of \\(Y\\)). We then use Bayes’ theorem to flip these around into estimates for \\(P(Y = k \\mid X = x)\\). When the distribution of \\(X\\) within each class is assumed to be normal, it turns out that the model is very similar in form to logistic regression.\nWhy do we need another method, when we have logistic regression? There are several reasons:\n\nWhen there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable. The methods that we consider in this section do not suffer from this problem.\nIf the distribution of the predictors \\(X\\) is approximately normal in each of the classes and the sample size is small, then the approaches in this section may be more accurate than logistic regression.\nThe methods in this section can be naturally extended to the case of more than two response classes.\n\nSuppose that we wish to classify an observation into one of \\(K\\) classes, where \\(K \\ge 2\\). In other words, the qualitative response variable \\(Y\\) can take on $ K$ possible distinct and unordered values. Let \\(\\pi_k\\) represent the overall or prior probability that a randomly chosen observation comes from the \\(k\\)th class. Let \\(f_k(X) \\equiv P(X \\mid Y = k)\\) denote the density function of \\(X\\) for an observation that comes from the \\(k\\)th class. In other words, \\(f_k(x)\\) is relatively large if there is a high probability that an observation in the \\(k\\)th class has \\(X \\approx x\\), and \\(f_k(x)\\) is small if it is very unlikely that an observation in the \\(k\\)th class has \\(X \\approx x\\). Then Bayes’ theorem states that\n\\[\nP(Y = k \\mid X = x) = \\frac{\\pi_k f_k(x)}{\\sum_{l = 1}^K \\pi_l f_l(x)}\n\\tag{5.7}\\]\n!!! draw tree picture\nIn accordance with our earlier notation, we will use the abbreviation \\(p_k(x) = P(Y = k \\mid X = x)\\); this is the posterior probability that an observation \\(X = x\\) belongs to the \\(k\\)th class. That is, it is the probability that the observation belongs to the \\(k\\)th class, given the predictor value for that observation.\nThe above equation suggests that instead of directly computing the posterior probability \\(p_k(x)\\) as in Section 5.1.3.1, we can simply plug in estimates of \\(\\pi_k\\) and \\(f_k(x)\\) into Equation 5.7. In general, estimating \\(\\pi_k\\) is easy if we have a random sample from the population: we simply compute the fraction of the training observations that belong to the \\(k\\)th class. However, estimating the density function \\(f_k(x)\\) is much more challenging. As we will see, to estimate \\(f_k(x)\\), we will typically have to make some simplifying assumptions.\nWe know from earlier that the Bayes classifier, which classifies an observation \\(x\\) to the class for which \\(p_k(x)\\) is largest, has the lowest possible error rate out of all classifiers. (Of course, this is only true if all of the terms in Equation 5.7 are correctly specified.) Therefore, if we can find a way to estimate \\(f_k(x)\\), then we can plug it into Equation 5.7 in order to approximate the Bayes classifier.\nIn the following sections, we discuss three classifiers that use different estimates of \\(f_k(x)\\) in Equation 5.7 to approximate the Bayes classifier: linear discriminant analysis, quadratic discriminant analysis, and naive Bayes.\nLinear discrimant analysis for \\(p = 1\\)\n\nFor now, assume that \\(p = 1\\) —- that is, we have only one predictor. We would like to obtain an estimate for \\(f_k(x)\\) that we can plug into Equation 5.7 in order to estimate \\(p_k(x)\\). We will then classify an observation to the class for which \\(p_k(x)\\) is greatest. To estimate \\(f_k(x)\\), we will first make some assumptions about its form.\nIn particular, we assume that \\(f_k(x)\\) is normal or Gaussian. In the one- normal dimensional setting, the normal density takes the form\n\\[\nf_k(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma_k} \\exp\\Big(-\\frac{1}{2\\sigma_k^2}(x - \\mu_k)^2\\Big)\n\\tag{5.8}\\]\nwhere \\(\\mu_k\\) and \\(\\sigma_k^2\\) are the mean and variance parameters for the \\(k\\)th class. For now, let us further assume that \\(\\sigma_1^2 = \\cdots = \\sigma_K^2\\), that is, there is a shared variance term across all \\(K\\) classes, which for simplicity we can denote by \\(\\sigma^2\\). Plugging this into Equation 5.7, we find that\n\\[\np_k(x) = \\frac{\\pi_k \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\big(-\\frac{1}{2\\sigma^2}(x - \\mu_k)^2\\big)}{\\sum_{l = 1}^K \\pi_l \\frac{1}{\\sqrt{2\\pi} \\sigma} \\exp\\big(-\\frac{1}{2\\sigma^2}(x - \\mu_l)^2\\big)}\n\\]\nThe Bayes classifier involves assigning an observation \\(X = x\\) to the class for which the above equation is the largest (recall this Bayes classifier is different than Bayes theorem Equation 5.7, which allows us to manipulate conditional distributions). Taking the log of this and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which\n\\[\n\\delta_k(x) = x \\frac{\\mu_k}{\\sigma^2} - \\frac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)\n\\tag{5.9}\\]\nis largest. For instance, if \\(K = 2\\) and \\(\\pi_1 = \\pi_2\\), then the Bayes classifier assignes an observation to class 1 if \\(2x(\\mu_1 - \\mu_2) &gt; \\mu_1^2 - \\mu_2^2\\) and to class 2 otherwise. The Bayes decision boundary is the point for which \\(\\delta_1(x) = \\delta_2(x)\\); one can show that this amounts to\n\\[\nx = \\frac{\\mu_1^2 - \\mu_2^2}{2(\\mu_1 - \\mu_2)} = \\frac{\\mu_1 + \\mu_2}{2}\n\\tag{5.10}\\]\n\nAn example is shown in the left-hand panel of Figure 4.4. The two normal density functions that are displayed, \\(f_1(x)\\) and \\(f_2(x)\\), represent two distinct classes. The mean and variance parameters for the two density functions are \\(\\mu_1 = −1.25, \\mu_2 = 1.25\\) and \\(\\sigma_1^2 = \\sigma_2^2 = 1\\). The two densities overlap, and so given that \\(X = x\\), there is some uncertainty about the class to which the observation belongs. If we assume that an observation is equally likely to come from either class – that is, \\(\\pi_1 = \\pi_2 = 0.5\\) –then by inspection of @bayes-decision-boundary2, we see that the Bayes classifier assigns the observation to class 1 if \\(x &lt; 0\\) and class 2 otherwise. Note that in this case, we can compute the Bayes classifier because we know that \\(X\\) is drawn from a Gaussian distribution within each class, and we know all of the parameters involved. In a real-life situation, we are not able to calculate the Bayes classifier.\nIn practice, even if we are quite certain of our assumption that \\(X\\) is drawn from a Gaussian distribution within each class, to apply the Bayes classifier we still have to estimate the parameters \\(\\mu_1, \\ldots, \\mu_K, \\pi_1, \\ldots, \\pi_K\\), and \\(\\sigma^2\\). The linear discriminant analysis (LDA) method approximates the Bayes classifier by plugging estimates for \\(\\pi_k\\), \\(\\mu_k\\), and \\(\\sigma^2\\) into @bayes-decision-boundary. In particular, the following estimates are used:\n\\[\n\\begin{align*}\n\\hat{\\mu}_k &= \\frac{1}{n_k} \\sum_{i:y_i = k} x_i\\\\\n\\hat{\\sigma}^2 &= \\frac{1}{n - K} \\sum_{k = 1}^K \\sum_{i:y_i = k} (x_i - \\hat{\\mu}_k)^2\\\\\n\\end{align*}\n\\]\nwhere \\(n\\) is the total number of training observations, and \\(n_k\\) is the number of training observations in the \\(k\\)th class. The estimate for \\(\\mu_k\\) is simply the average of all the training observations from the \\(k\\)th class, while \\(\\sigma^2\\) can be seen as a weighted average of the sample variances for each of the \\(K\\) classes. Sometimes we have knowledge of the class membership probabilities \\(\\pi_1,  \\ldots, \\pi_K\\), which can be used directly. In the absence of any additional information, LDA estimates \\(\\pi_k\\) using the proportion of the training observations that belong to the \\(k\\)th class. In other words,\n\\[\n\\hat{\\pi}_k = n_k / n\n\\]\nThe LDA classifier plugs the estimates above into Equation 5.9 and assigns an observation \\(X = x\\) to the class for which\n\\[\n\\hat{\\delta}_k(x) = x \\frac{\\hat{\\mu}_k}{\\hat{\\sigma}^2} - \\frac{\\hat{\\mu}_k^2}{2\\hat{\\sigma}^2} + \\log(\\hat{\\pi}_k)\n\\]\nis largest. The word linear in the classifier’s name stems from the fact that the discriminant functions \\(\\hat{\\delta}_k(x)\\) above are linear functions of \\(x\\) (as discriminant opposed to a more complex function of \\(x\\)).\nThe right-hand panel of Figure 4.4 displays a histogram of a random sample of 20 observations from each class. In this case, since \\(n_1 = n_2 = 20\\), we have \\(\\hat{\\pi}_1 = \\hat{\\pi}_2\\). As a result, the decision boundary corresponds to the midpoint between the sample means for the two classes, \\((\\hat{\\mu}_1 + \\hat{\\mu}_2) / 2\\).\nTo reiterate, the LDA classifier results from assuming that the observations within each class come from a normal distribution with a class-specific mean and a common variance \\(\\sigma^2\\), and plugging estimates for these parameters into the Bayes classifier. In a later section, we will consider a less stringent set of assumptions, by allowing the observations in the \\(k\\)th class to have a class-specific variance, \\(\\sigma_k^2\\).\nLinear discriminant analysis for \\(p &gt; 1\\)\n\nWe now extend the LDA classifier to the case of multiple predictors. To do this, we will assume that \\(X = (X_1, X_2, \\ldots, X_p)\\) is drawn from a multivariate Gaussian (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix.\nThe multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution, as in Equation 5.8, with some correlation between each pair of predictors.\n\nTo indicate that a \\(p\\)-dimensional random variable \\(X\\) has a multivariate Gaussian distribution, we write \\(X \\sim \\text{N}\\,(\\mu, \\Sigma)\\). Here, \\(E(X) = \\mu\\) is the mean of \\(X\\) (a vector with \\(p\\) components), and \\(\\text{Cov}(X) = \\Sigma\\) is the \\(p \\times p\\) covariance matrix of \\(X\\). Formally, the multivariate Gaussian density is defined as\n\\[\nf(x) = \\frac{1}{(2\\pi)^{p/2} \\lvert \\Sigma \\rvert^{1/2}} \\exp\\Big(-\\frac{1}{2}(x - \\mu)^T \\Sigma^{-1} (x - \\mu)\\Big)\n\\]\nIn the case of \\(p &gt; 1\\) predictors, the LDA classifier assumes that the observations in the \\(k\\)th class are drawn from a multivariate Gaussian distribution \\(N(\\mu_k, \\Sigma)\\), where \\(\\mu_k\\) is a class-specific mean vector, and \\(\\Sigma\\) is a covariance matrix that is common to all \\(K\\) classes. Plugging the density function for the \\(k\\)th class, \\(f_k(X = x)\\), into Equation 5.7 and performing a little bit of algebra reveals that the Bayes classifier assigns an observation \\(X = x\\) to the class for which\n\\[\n\\delta_k(x) = x^T \\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T \\Sigma^{-1}\\mu_k + \\log \\pi_k\n\\tag{5.11}\\]\nis largest. This is the vector / matrix version of Equation 5.9.\nAn example is shown in the left-hand panel of Figure 4.6.\n\nThree equally-sized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix. The three ellipses represent regions that contain 95% of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries. In other words, they represent the set of values x for which \\(\\delta_k(x) = \\delta_l(x)\\); i.e.\n\\[\nx^T \\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T \\Sigma^{-1}\\mu_k = x^T \\Sigma^{-1}\\mu_l - \\frac{1}{2}\\mu_l^T \\Sigma^{-1}\\mu_l\n\\]\nfor \\(k \\ne l\\). (The \\(\\log \\pi_k\\)) term has disappeared because each of the three classes has the same number of training observations). Note that there are three lines representing the Bayes decision boundaries because there are three pairs of classes among the three classes. That is, one Bayes decision boundary separates class 1 from class 2, one separates class 1 from class 3, and one separates class 2 from class 3. These three Bayes decision boundaries divide the predictor space into three regions. The Bayes classifier will classify an observation according to the region in which it is located.\nOnce again, estimate the all the unknown parameters, similarly to how performed with \\(p = 1\\) and plug into Equation 5.11. Then assign the class to that which results in the largest \\(\\hat{\\delta}_k(x)\\). Note that this is still a linear function of \\(x\\)\nResults of classification can be compactly shown in confusion matrices.\n\nFor the default data, the LDA model fit to the 10,000 training samples results in a training error rate of 2.75%. This sounds like a low error rate, but two caveats must be noted.\n\nFirst of all, training error rates will usually be lower than test error rates, which are the real quantity of interest.\nSecond, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that an individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3.33 %. In other words, the trivial null classifier will achieve an error rate that null is only a bit higher than the LDA training set error rate.\n\nIn practice, a binary classifier such as this one can make two types of errors: it can incorrectly assign an individual who defaults to the no default category, or it can incorrectly assign an individual who does not default to the default category. It is often of interest to determine which of these two types of errors are being made. The table reveals that LDA predicted that a total of 104 people would default. Of these people, 81 actually defaulted and 23 did not. Hence only 23 out of 9,667 of the individuals who did not default were incorrectly labeled. This looks like a pretty low error rate! However, of the 333 individuals who defaulted, 252 (or 75.7%) were missed by LDA. So while the overall error rate is low, the error rate among individuals who defaulted is very high.\nAlso, notice that in this example student status is qualitative – thus, the normality assumption made by LDA is clearly violated! However, LDA is often remarkably robust to model violations, as this example shows. Naive Bayes, discussed later, provides an alternative to LDA that does not assume normally distributed predictors.\nClass-specific performance is also important in medicine and biology, where the terms sensitivity and specificity characterize the performance of a classifier or screening test.\n\nSensitivity = true positive % (e.g. true defaulters that are identified; 81/333 = 24.3%)\nSpecificity = true negative % (e.g. true non-defaulters that are identified; 9644/9667 = 1 - 23/9667 = 99.8%)\n\nWhy does LDA do such a poor job of classifying the customers who default? In other words, why does it have such low sensitivity? As we have seen, LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers. That is, the Bayes classifier will yield the smallest possible total number of misclassified observations, regardless of the class from which the errors stem. Some misclassifications will result from incorrectly assigning a customer who does not default to the default class, and others will result from incorrectly assigning a customer who defaults to the non-default class. In contrast, a credit card company might particularly wish to avoid incorrectly classifying an individual who will default, whereas incorrectly classifying an individual who will not default, though still to be avoided, is less problematic. We will now see that it is possible to modify LDA in order to develop a classifier that better meets the credit card company’s needs.\nThe Bayes classifier works by assigning an observation to the class for which the posterior probability \\(p_k(X)\\) is greatest. In the two-class case, this amounts to assigning an observation to the default class if\n\\[\nP(\\text{default} = \\text{Yes} \\mid X = x) &gt; 0.5\n\\]\nThus, the Bayes classifier, and by extension LDA, uses a threshold of 50 % for the posterior probability of default in order to assign an observation to the default class. However, if we are concerned about incorrectly predicting the default status for individuals who default, then we can consider lowering this threshold. For instance, we might label any customer with a posterior probability of default above 20% to the default class:\n\\[\nP(\\text{default} = \\text{Yes} \\mid X = x) &gt; 0.2\n\\]\nThe error rates that result from taking this approach are shown in Table 4.5. Now LDA predicts that 430 individuals will default. Of the 333 individuals who default, LDA correctly predicts all but 138, or 41.4 %. This is a vast improvement over the error rate of 75.7% that resulted from using the threshold of 50%. However, this improvement comes at a cost: now 235 individuals who do not default are incorrectly classified. As a result, the overall error rate has increased slightly to 3.73 %.\n\nFigure 4.7 illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. Various error rates are shown as a function of the threshold value. How can we decide which threshold value is best? Such a decision must be based on domain knowledge, such as detailed information about the costs associated with default.\nIncreasing threshold to say 80% causes:\n\nSensitivity to DECREASE and specificity to INCREASE (good source, for modelling too).\nThis means higher proportion of true negatives, with the cost of lower proportion of true positives.\nPredict more as no (harder to predict as yes), so capture more true no’s, but miss some of the yes’s.\nJust flip below: False positive rate decreases, but the false negative rate increases.\n\nThus, decreasing the threshold:\n\nJust flip above: Sensitivity increases and specificity decreases.\n\nError rates:\n\nDecreases the false negative rate (error rate among defaulters), because more confident in the negatives.\nBut increases the false positive rate (error rate among non-defaulters), because easier to be classified as yes.\nThus increasing the threshold **\n\n\n\n\nThe ROC curve is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. The overall performance of a classifier, summarized over all possible thresholds, is given by the area under the (ROC) curve (AUC). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. We expect a classifier that performs no better than chance to have an AUC of 0.5 (when evaluated on an independent test set not used in model training). ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds. As we have seen above, varying the classifier threshold changes its true positive and false positive rate. These are also called the sensitivity and one minus the specificity of our classifier.\n\nHere is a summary of the terms in this section. To make the connection with the epidemiology literature, we think of “+” as the “disease” that we are trying to detect, and “−” as the “non-disease” state. To make the connection to the classical hypothesis testing literature, we think of “−” as the null hypothesis and “’+” as the alternative (non-null) hypothesis. In the context of the Default data, “+” indicates an individual who defaults, and “−” indicates one who does not.\n\n\nSensitivity = true positive rate\nSpecificity = true negative rate\nQuadratic discriminant analysis\nAs we have discussed, LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a covariance matrix that is common to all \\(K\\) classes. Quadratic discriminant analysis (QDA) provides an alternative approach. Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction. However, unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form \\(X \\sim N(\\mu_k,\\Sigma_k)\\), where \\(\\Sigma_k\\) is a covariance matrix for the \\(k\\)th class. Under this assumption, the Bayes classifier assigns an observation \\(X = x\\) to the class for which\n\nis the largest. Unlike before, the quantity \\(x\\) appears as a quadratic function now.\nWhy does it matter whether or not we assume that the K classes share a common covariance matrix? In other words, why would one prefer LDA to QDA, or vice-versa? The answer lies in the bias-variance trade-off. When there are \\(p\\) predictors, then estimating a covariance matrix requires estimating \\(p(p+1)/2\\) parameters (lower triangle + diagonal). QDA estimates a separate covariance matrix for each class, for a total of \\(Kp(p+1)/2\\) parameters. With 50 predictors this is some multiple of 1,275, which is a lot of parameters. By instead assuming that the \\(K\\) classes share a common covariance matrix, the LDA model becomes linear in \\(x\\), which means there are \\(Kp\\) linear coefficients to estimate. Consequently, LDA is a much less flexible classifier than QDA, and so has substantially lower variance. This can potentially lead to improved prediction performance. But there is a trade-off: if LDA’s assumption that the \\(K\\) classes share a common covariance matrix is badly off, then LDA can suffer from high bias. Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the \\(K\\) classes is clearly untenable.\n\nFigure 4.9 illustrates the performances of LDA and QDA in two scenarios. In the left-hand panel, the two Gaussian classes have a common correlation of 0.7 between \\(X_1\\) and \\(X_2\\). As a result, the Bayes decision boundary is linear and is accurately approximated by the LDA decision boundary. The QDA decision boundary is inferior, because it suffers from higher variance without a corresponding decrease in bias. In contrast, the right-hand panel displays a situation in which the orange class has a correlation of 0.7 between the variables and the blue class has a correlation of −0.7. Now the Bayes decision boundary is quadratic, and so QDA more accurately approximates this boundary than does LDA.\n\nNaive Bayes\nHere, we use Bayes’ theorem to motivate the popular naive Bayes classifier. Recall that Bayes’ theorem Equation 5.7 provides an expression for the posterior probability \\(p_k(x) = P(Y = k \\mid X = x)\\) in terms of \\(\\pi_1, \\ldots, \\pi_K\\) and \\(f_k(x), \\ldots, f_K(x)\\). To use this in practice, we need estimates for for these terms. Estimating the prior probabilities \\(\\pi_k\\) is typically straightforward.\nHowever estimating \\(f_k(x)\\) is more subtle. Recall that \\(f_k(x)\\) is the \\(p\\)-dimensional density function for an observation in the \\(k\\)th class, for \\(k = 1, \\ldots, K\\). In general, estimating a \\(p\\)-dimensional density function is challenging. In LDA, we make a very strong assumption that greatly simplifies the task: we assume that \\(f_k\\) is the density function for a multivariate normal random variable with class-specific mean \\(\\mu_k\\), and shared covariance matrix \\(\\Sigma\\). By contrast, in QDA we change it to have class-specific covariance matrix \\(\\Sigma_k\\). By making these very strong assumptions, we are able to replace the very challenging problem of estimating \\(K\\) \\(p\\)-dimensional density functions with the much simpler problem of estimating \\(K\\) \\(p\\)-dimensional mean vectors and one (in the case of LDA) or \\(K\\) (in the case of QDA) \\((p \\times p)\\)-dimensional covariance matrices.\nThe naive Bayes classifier takes a different tack for estimating \\(f_1(x), \\ldots, f_K(x)\\). Instead of assuming that these functions belong to a particular family of distributions (e.g. multivariate normal), we instead make a single assumption:\nWithin the kth class, the \\(p\\) predictors are independent.\nStated mathematically, this assumption means that for \\(k = 1, \\ldots, K\\),\n\\[\nf_k(x) = f_{k1}(x_1) \\times f_{k2}(x_2) \\times \\cdots \\times f_{kp}(x_p)\n\\tag{5.12}\\]\nwhere \\(f_{kj}\\) is the density function of the \\(j\\)th predictor among observations in the \\(k\\)th class.\nWhy is this assumption so powerful? Essentially, estimating a \\(p\\)-dimensional density function is challenging because we must consider not only the marginal distribution of each predictor – that is, the distribution of each predictor on its own – but also the joint distribution of the predictors – that is, the association between the different predictors. In the case of a multivariate normal distribution, the association between the different predictors is summarized by the off-diagonal elements of the covariance matrix. However, in general, this association can be very hard to characterize, and exceedingly challenging to estimate. But by assuming that the \\(p\\) covariates are independent within each class, we completely eliminate the need to worry about the association between the \\(p\\) predictors, because we have simply assumed that there is no association between the predictors!\nDo we really believe the naive Bayes assumption that the \\(p\\) covariates are independent within each class? In most settings, we do not. But even though this modeling assumption is made for convenience, it often leads to pretty decent results, especially in settings where \\(n\\) is not large enough relative to \\(p\\) for us to effectively estimate the joint distribution of the predictors within each class. In fact, since estimating a joint distribution requires such a huge amount of data, naive Bayes is a good choice in a wide range of settings. Essentially, the naive Bayes assumption introduces some bias, but reduces variance, leading to a classifier that works quite well in practice as a result of the bias-variance trade-off.\nOnce we have made the naive Bayes assumption, we can plug Equation 5.12 into Equation 5.7 to obtain an expression for the posterior probability,\n\\[\nP(Y = k \\mid X = x) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times \\cdots \\times f_{kp}(x_p)}{\\sum_{l = 1}^K \\pi_l \\times f_{l1}(x_1) \\times \\cdots \\times f_{lp}(x_p)}\n\\]\nfor \\(k = 1, \\ldots, K\\).\nTo estimate the one-dimensional density function \\(f_{kj}\\) using training data \\(x_{1j}, \\cdots, x_{nj}\\), we have a few options.\n\nIf \\(X_j\\) is quantitative, then we can assume that \\(X_j \\mid Y = k \\sim N(\\mu_{jk}, \\sigma_{jk}^2)\\). In other words, we assume that within each class, the \\(j\\)th predictor is drawn from a (univariate) normal distribution. While this may sound a bit like QDA, there is one key difference, in that here we are assuming that the predictors are independent; this amounts to QDA with an additional assumption that the class-specific covariance matrix is diagonal.\nIf \\(X_j\\) is quantitative, then another option is to use a non-parametric estimate for \\(f_{kj}\\). A very simple way to do this is by making a histogram for the observations of the \\(j\\)th predictor within each class. Then we can estimate \\(f_{kj}(x_j)\\) as the fraction of the training observations in the \\(k\\)th class that belong to the same histogram bin as \\(x_j\\). Alternatively, we can use a kernel density estimator, which is essentially a smoothed version of a histogram.\nIf \\(X_j\\) is qualitative, then we can simply count the proportion of training observations for the \\(j\\)th predictor corresponding to each class. For instance, suppose that \\(X_j \\in \\{1, 2, 3\\}\\), and we have 100 observations in the \\(k\\)th class. Suppose that the \\(j\\)th predictor takes on values of 1, 2, and 3 in 32, 55, and 13 of those observations, respectively. Then we can estimate \\(f_{kj}\\) as\n\n\\[\n\\hat{f}_{kj}(x_j) =\n\\begin{cases}\n0.32 & \\text{if } x_j = 1\\\\\n0.55 & \\text{if } x_j = 2\\\\\n0.13 & \\text{if } x_j = 3\\\\\n\\end{cases}\n\\]\nJust as with LDA, we can easily adjust the probability threshold for predicting a default. In this example, it should not be too surprising that naive Bayes does not convincingly outperform LDA: this data set has \\(n = 10,000\\) and \\(p = 4\\), and so the reduction in variance resulting from the naive Bayes assumption is not necessarily worthwhile. We expect to see a greater pay-off to using naive Bayes relative to LDA or QDA in instances where \\(p\\) is larger or \\(n\\) is smaller, so that reducing the variance is very important.\n\n5.1.5 A comparison of classification methods\nAn analytical comparison\nWe now perform an analytical (or mathematical) comparison of LDA, QDA, naive Bayes, and logistic regression. We consider these approaches in a setting with K classes, so that we assign an observation to the class that maximizes \\(P(Y = k \\mid X = x)\\). Equivalently, we can set \\(K\\) as the baseline class and assign an observation to the class that maximizes\n\\[\n\\log\\Big(\\frac{P(Y = k \\mid X = x)}{P(Y = K \\mid X = x)}\\Big)\n\\]\nfor \\(k = 1, \\ldots, K\\). Examining the specific form of this equation for each method provides a clear understanding of their similarities and differences.\n\nwhere \\(a_k = \\log\\Big(\\frac{\\pi_k}{\\pi_K}\\Big) - \\frac{1}{2}(\\mu_k + \\mu_K)^T \\Sigma^{-1}(\\mu_k - \\mu_K)\\) and \\(b_{kj}\\) is the \\(j\\)th component of \\(\\Sigma^{-1}(\\mu_k - \\mu_K)\\). Hence LDA, like logistic regression, assumes that the log odds of the posterior probabilities is linear in \\(x\\).\nUsing similar calculations, in the QDA setting this becomes\n\nAgain, as the name suggests, QDA assumes that the log odds of the posterior probabilities is quadratic in \\(x\\).\nFinally with naive Bayes setting, we get\n\nwhere \\(a_k = \\log\\big(\\frac{\\pi_k}{\\pi_K}\\big)\\) and \\(g_{kj}(x_j) = \\log\\big(\\frac{f_{kj}(x_j)}{f_{Kj}(x_j)}\\big)\\). Hence, the right-hand side of above takes the form of a generalized additive model, which will be discussed later.\nInspection of the above results yields the following observations about LDA, QDA, and naive Bayes:\n\nLDA is a special case of QDA with \\(c_{kjl} = 0\\) for all \\(j = 1, \\ldots, p, l = 1, \\ldots, p\\), and \\(k = 1, \\ldots, K\\). (Of course, this is not surprising, since LDA is simply a restricted version of QDA with \\(\\Sigma_1 = \\cdots = \\Sigma_K = \\Sigma\\).)\nAny classifier with a linear decision boundary is a special case of naive Bayes with \\(g_{kj}(x_j) = b_{kj}x_j\\). In particular, this means that LDA is a special case of naive Bayes! This is not at all obvious from the descriptions of LDA and naive Bayes earlier in the chapter, since each method makes very different assumptions: LDA assumes that the features are normally distributed with a common within-class covariance matrix, and naive Bayes instead assumes independence of the features.\nIf we model \\(f_{kj}(x_j)\\) in the naive Bayes classifier using a one-dimensional Gaussian distribution \\(N(\\mu_{kj}, \\sigma_j^2)\\), then we end up with \\(g_{kj}(x_j) = b_{kj}x_j\\) where \\(b_{kj} = (\\mu_{kj} - \\mu_{Kj}) / \\sigma_j^2\\). In this case, naive Bayes is actually a special case of LDA with \\(\\Sigma\\) restricted to be a diagonal matrix with \\(j\\)th diagonal element equal to \\(\\sigma_j^2\\).\nNeither QDA nor naive Bayes is a special case of the other. Naive Bayes can produce a more flexible fit, since any choice can be made for \\(g_{kj}(x_j)\\). However, it is restricted to a purely additive fit, in the sense that in the final derivation, a function of \\(x_j\\) is added to a function of \\(x\\), for \\(j \\ne l\\); however, these terms are never multiplied. By contrast, QDA includes multiplicative terms of the form \\(c_{kjl} x_j x_l\\). Therefore, QDA has the potential to be more accurate in settings where interactions among the predictors are important in discriminating between classes.\n\nNone of these methods uniformly dominates the others: in any setting, the choice of method will depend on the true distribution of the predictors in each of the \\(K\\) classes, as well as other considerations, such as the values of \\(n\\) and \\(p\\). The latter ties into the bias-variance trade-off.\nHow does logistic regression tie into this story? Recall from Equation 5.6 that multinomial logistic regression takes the form\n\\[\n\\log\\Big(\\frac{P(Y = k \\mid X = x)}{P(Y = K \\mid X = x)}\\Big) = \\beta_{k0} + \\sum_{j=1}^p \\beta_{kj} x_j\n\\]\nThis is identical to the linear form of LDA the derivation: in both cases $() $ is a linear function of the predictors. In LDA, the coefficients in this linear function are functions of estimates for \\(\\pi_k\\), \\(\\pi_K\\), \\(\\mu_k\\), \\(\\mu_K\\), and \\(\\Sigma\\) obtained by assuming that \\(X_1,\\ldots,X_p\\) follow a normal distribution within each class. By contrast, in logistic regression, the coefficients are chosen to maximize the likelihood function Equation 5.3. Thus, we expect LDA to outperform logistic regression when the normality assumption (approximately) holds, and we expect logistic regression to perform better when it does not.\nWe close with a brief discussion of \\(K\\)-nearest neighbors (KNN), introduced in earlier. Recall that KNN takes a completely different approach from the classifiers seen in this chapter. In order to make a prediction for an observation \\(X = x\\), the training observations that are closest to \\(x\\) are identified. Then \\(X\\) is assigned to the class to which the plurality of these observations belong. Hence KNN is a completely non-parametric approach: no assumptions are made about the shape of the decision boundary. We make the following observations about KNN:\n\nBecause KNN is completely non-parametric, we can expect this ap- proach to dominate LDA and logistic regression when the decision boundary is highly non-linear, provided that n is very large and p is small.\nIn order to provide accurate classification, KNN requires a lot of observations relative to the number of predictors – that is, \\(n\\) much larger than \\(p\\). This has to do with the fact that KNN is non-parametric, and thus tends to reduce the bias while incurring a lot of variance.\nIn settings where the decision boundary is non-linear but \\(n\\) is only modest, or \\(p\\) is not very small, then QDA may be preferred to KNN. This is because QDA can provide a non-linear decision boundary while taking advantage of a parametric form, which means that it requires a smaller sample size for accurate classification, relative to KNN.\nNote that the of \\(K\\) in KNN is really important and is often chosen via cross-validation.\nUnlike logistic regression, KNN does not tell us which predictors are important: we don’t get a table of coefficients as in Table 4.3.\nAn empirical comparison\nCheckout section 4.5.2 of textbook for some simulation results. This could help when determining which model to use for a particular situation.\n\nWhen the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well.\nWhen the boundaries are moderately non-linear, QDA or naive Bayes may give better results.\nFinally, for much more complicated decision boundaries, a non-parametric approach such as KNN can be superior. But the level of smoothness for a non-parametric approach must be chosen carefully.\n\nIn the next chapter we examine a number of approaches for choosing the correct level of smoothness and, in general, for selecting the best overall method.\nFinally, recall from the previous chapter that in the regression setting we can accommodate a non-linear relationship between the predictors and the response by performing regression using transformations of the predictors. A similar approach could be taken in the classification setting. For instance, we could create a more flexible version of logistic regression by including \\(X^2\\), \\(X^3\\), and even \\(X^4\\) as predictors. This may or may not improve logistic regression’s performance, depending on whether the increase in variance due to the added flexibility is offset by a sufficiently large reduction in bias. We could do the same for LDA. If we added all possible quadratic terms and cross-products to LDA, the form of the model would be the same as the QDA model, although the parameter estimates would be different. This device allows us to move somewhere between an LDA and a QDA model.\n\n5.1.6 Generalized linear models\nIn the previous chapter, we assumed that the response \\(Y\\) is quantitative, and explored the use of least squares linear regression to predict \\(Y\\). Thus far in this chapter, we have instead assumed that \\(Y\\) is qualitative. However, we may sometimes be faced with situations in which \\(Y\\) is neither qualitative nor quantitative, and so neither linear regression nor the classification approaches is applicable.\nThis occurs when we are working with count data (non-negative integers). If we fit linear regression to count data, here are some issues that can occur:\n\nCan predict negative counts. This calls into question our ability to perform meaningful predictions on the data, and it also raises concerns about the accuracy of the coefficient estimates, confidence intervals, and other outputs of the regression model.\nFurthermore, it is reasonable to suspect that with the expected value of the response is small, the variance of the response should be small as well; however, when the expected value of counts is large, the variance should increase as well. This is a major violation of the assumptions of a linear model, which state that \\(Y = \\sum_{j=1}^p X_j \\beta_j + \\epsilon\\), where \\(\\epsilon\\) is a mean-zero error term with variance \\(\\sigma^2\\) that is constant, and not a function of the covariates. Therefore, the heteroscedasticity of the data calls into question the suitability of a linear regression model. This is bad:\n\n\n\nFinally, with a continuous-valued error term, we can get a continuous-valued response, which doesn’t match the scenario.\n\nSome of the problems that arise when fitting a linear regression model to count data can be overcome by transforming the response; for instance, we can fit the model\n\\[\n\\log(Y) = \\sum_{j=1}^p X_j \\beta_j = \\epsilon\n\\]\nTransforming the response avoids the possibility of negative predictions, and it overcomes much of the heteroscedasticity in the untransformed data, as is shown in the right-hand panel of Figure 4.14. However, it is not quite a satisfactory solution, since predictions and inference are made in terms of the log of the response, rather than the response. This leads to challenges in interpretation, e.g. “a one-unit increase in \\(X_j\\) is associated with an increase in the mean of the log of \\(Y\\) by an amount \\(\\beta_j\\)”. Furthermore, a log transformation of the response cannot be applied in settings where the response can take on a value of 0. Thus, while fitting a linear model to a transformation of the response may be an adequate approach for some count-valued data sets, it often leaves something to be desired. We will see in the next section that a Poisson regression model provides a much more natural and elegant approach for this task.\nPoisson regression\nTo overcome the inadequacies of linear regression for analyzing count data, we will make use of an alternative approach, called Poisson regression. Recall if \\(Y \\sim \\text{Poisson}\\,(\\lambda)\\), then\n\\[\nP(Y = k) = \\frac{\\mathrm{e}^{-\\lambda}\\lambda^k}{k!}, \\quad \\text{for } k = 1, 2, \\ldots\n\\tag{5.13}\\]\nHere, \\(\\lambda &gt; 0\\) is the expected value of \\(Y\\), i.e. \\(E(Y)\\). It turns out that \\(\\lambda\\) also equals the variance of \\(Y\\) , i.e. \\(\\lambda = E(Y) = V(Y)\\). This means that if \\(Y\\) follows the Poisson distribution, then the larger the mean of \\(Y\\), the larger its variance.\nThe Poisson distribution is typically used to model counts. To see how we might use the Poisson distribution in practice, let \\(Y\\) denote the number of users of the bike sharing program during a particular hour of the day, under a particular set of weather conditions, and during a particular month of the year. We might model \\(Y\\) as a Poisson distribution with mean \\(E(Y) = \\lambda = 5\\). This means that the probability of no users during this particular hour is \\(P(Y = 0) = \\frac{\\mathrm{e}^{-5} 5^0}{0!} = e^{-5} = 0.0067\\). Of course, in reality, we expect the mean number of users of the bike sharing program, \\(\\lambda = E(Y)\\), to vary as a function of the hour of the day, the month of the year, the weather conditions, and so forth. So rather than modeling the number of bikers, \\(Y\\), as a Poisson distribution with a fixed mean value like \\(\\lambda = 5\\), we would like to allow the mean to vary as a function of the covariates. In particular, we consider the following model for the mean \\(\\lambda = E(Y)\\), which we now write as \\(\\lambda(X_1, \\ldots, X_p)\\) to emphasize that it is a function of the covariates \\(X_1, \\ldots, X_p\\):\n\\[\n\\log(\\lambda(X_1, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\tag{5.14}\\]\nor equivalently\n\\[\n\\lambda(X_1, \\ldots, X_p) = \\mathrm{e}^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}\n\\tag{5.15}\\]\nHere, \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\) are parameters to be estimated. Together, Equation 5.13 and Equation 5.14 define the Poisson regression model. Notice that in Equation 5.14, we take the log of \\(\\lambda(X_1, \\ldots, X_p)\\) to be linear in \\(X_1, \\ldots, X_p\\), in order to ensure that \\(\\lambda(X_1, \\ldots, X_p)\\) takes on nonnegative values for all values of the covariates.\nTo estimate the coefficients \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\), we use the use the same maximum likelihood approach that we adopted for logistic regression. Specifically, given \\(n\\) independent observations from the Poisson regression model, the likelihood takes the form\n\\[\n\\ell(\\beta_0, \\beta_1, \\ldots, \\beta_p) = \\prod_{i = 1}^n \\frac{e^{-\\lambda(x_i)} \\lambda(x_i)^{y_i}}{y_i!}\n\\]\nwhere \\(\\lambda(x_i) = \\mathrm{e}^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}\\) due to Equation 5.15. We estimate the coefficients that maximize the likelihood \\(\\ell(\\beta_0, \\beta_1, \\ldots, \\beta_p)\\), i.e. that make the observed data as likely as possible.\nSome important distinctions between the Poisson regression model and the linear regression model are as follows:\n\nInterpretation: To interpret the coefficients in the Poisson regression model, we must pay close attention to Equation 5.15, which states that an increase in \\(X_j\\) by one unit is associated with a change in \\(E(Y) = \\lambda\\) by a factor of \\(\\mathrm{e}^{\\beta_j}\\). For example, a change in weather from clear to cloudy skies is associated with a change in mean bike usage by a factor of \\(\\exp(−0.08) = 0.923\\), i.e. on average, only 92.3% as many people will use bikes when it is cloudy relative to when it is clear.\nMean-variance relationship: As mentioned earlier, under the Poisson model, \\(\\lambda = E(Y) = V(Y)\\). Thus, by modeling bike usage with a Poisson regression, we implicitly assume that mean bike usage in a given hour equals the variance of bike usage during that hour. By contrast, under a linear regression model, the variance of bike usage always takes on a constant value.\nnonnegative fitted values: There are no negative predictions using the Poisson regression model.\nGeneralized linear models in greater generality\nWe have now discussed three types of regression models: linear, logistic and Poisson. These approaches share some common characteristics:\n\nEach approach uses predictors \\(X_1, \\ldots, X_p\\) to predict a response \\(Y\\). We assume that, conditional on \\(X_1, \\ldots, X_p\\), \\(Y\\) belongs to a certain family of distributions. For linear regression, we typically assume that \\(Y\\) follows a Gaussian or normal distribution. For logistic regression, we assume that \\(Y\\) follows a Bernoulli distribution. Finally, for Poisson regression, we assume that \\(Y\\) follows a Poisson distribution.\nEach approach models the mean of \\(Y\\) as a function of the predictors. In linear regression, the mean of \\(Y\\) takes the form\n\n\\[\nE(Y \\mid X_1, \\ldots, X_p) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\]\ni.e., it is a linear function of the predictors. For logistic regression, the mean instead takes the form\n\\[\n\\begin{align*}\nE(Y \\mid X_1, \\ldots, X_p) &= P(Y = 1 \\mid X_1, \\ldots, X_p)\\\\\n&= \\frac{\\mathrm{e}^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}{1 + \\mathrm{e}^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}}\n\\end{align*}\n\\]\n(this is because mean(Bernoulli) = \\(p\\) = … &lt; logistic function &gt;) while for Poisson regression it takes the form\n\\[\nE(Y \\mid X_1, \\ldots, X_p) = \\lambda(X_1, \\ldots, X_p) = \\mathrm{e}^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}\n\\]\nAll of these above equations can be expressed using a link function, \\(\\eta\\), which applies a transformation to $E(Y X_1, , X_p) $ so that the transformed mean is a linear function of the predictors. That is,\n\\[\n\\eta(E(Y \\mid X_1, \\ldots, X_p)) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p\n\\tag{5.16}\\]\nThe link functions for linear, logistic and Poisson regression are \\(\\eta(\\mu) = \\mu\\), \\(\\eta(\\mu) = \\log(\\mu/(1 − \\mu))\\), and \\(\\eta(\\mu) = \\log(\\mu)\\), respectively.\nThe Gaussian, Bernoulli and Poisson distributions are all members of a wider class of distributions, known as the exponential family. Other well-known members of this family are the exponential distribution, the Gamma distribution, and the negative binomial distribution. In general, we can perform a regression by modeling the response Y as coming from a particular member of the exponential family, and then transforming the mean of the response so that the transformed mean is a linear function of the predictors via Equation 5.16. Any regression approach that follows this very general recipe is known as a generalized linear model (GLM). Thus, linear regression, logistic regression, and Poisson regression are three examples of GLMs. Other examples not covered here include Gamma regression and negative binomial regression.",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ISLR -- Classification</span>"
    ]
  },
  {
    "objectID": "islr-4.html#lab",
    "href": "islr-4.html#lab",
    "title": "\n5  ISLR – Classification\n",
    "section": "\n5.2 Lab",
    "text": "5.2 Lab\n\n5.2.1 Load data\n\n# load data\ndata_stock &lt;- ISLR2::Smarket\ncolnames(data_stock)\n\n[1] \"Year\"      \"Lag1\"      \"Lag2\"      \"Lag3\"      \"Lag4\"      \"Lag5\"     \n[7] \"Volume\"    \"Today\"     \"Direction\"\n\n# view correlations\ndata_stock %&gt;% \n  select(where(is.numeric)) %&gt;% \n  as.matrix %&gt;% \n  cor %&gt;% \n  corrplot::corrplot()\n\n\n\n\n\n\n\n\n5.2.2 Logisitic regression\nLets fit the full model and view some model summaries.\n\n# load packages\nlibrary(magrittr)\n\n# fit logistic regression model\nmod_logreg &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n                  data = data_stock,\n                  family = \"binomial\")\n\n# view model summary\nsummary(mod_logreg)\n\n\nCall:\nglm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + \n    Volume, family = \"binomial\", data = data_stock)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept) -0.126000   0.240736  -0.523    0.601\nLag1        -0.073074   0.050167  -1.457    0.145\nLag2        -0.042301   0.050086  -0.845    0.398\nLag3         0.011085   0.049939   0.222    0.824\nLag4         0.009359   0.049974   0.187    0.851\nLag5         0.010313   0.049511   0.208    0.835\nVolume       0.135441   0.158360   0.855    0.392\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1731.2  on 1249  degrees of freedom\nResidual deviance: 1727.6  on 1243  degrees of freedom\nAIC: 1741.6\n\nNumber of Fisher Scoring iterations: 3\n\n# view coding of response\n# -&gt; thus model is predicting P(Direction = Up | X = x)\ncontrasts(data_stock$Direction)\n\n     Up\nDown  0\nUp    1\n\n# get model fits for probabilities\n# -&gt; base R\n# -&gt; to get the fitted log odds, use type = \"link\"\npreds_logreg &lt;- predict(mod_logreg, type = \"response\")\n# -&gt; broom\n# --&gt; uses the same arguments as type for .fitted\n# --&gt; then classify\npreds_logreg &lt;- broom::augment(mod_logreg, type.predict = \"response\") %&gt;% \n  select(Direction, .fitted) %&gt;% \n  mutate(predicted = if_else(.fitted &gt; 0.5, \"Up\", \"Down\") %&gt;% factor(levels = c(\"Down\", \"Up\")))\n\n# create confusion matrix\n# -&gt; base R\npreds_logreg %$% table(predicted, \n                     Direction,\n                     dnn = c(\"predicted\", \"actual\"))\n\n         actual\npredicted Down  Up\n     Down  145 141\n     Up    457 507\n\n# -&gt; tidymodels\n(c_mat &lt;- yardstick::conf_mat(preds_logreg %&gt;% \n                      mutate(predicted = as.factor(predicted)),\n                    truth = \"Direction\",\n                    estimate = \"predicted\"))\n\n          Truth\nPrediction Down  Up\n      Down  145 141\n      Up    457 507\n\nsummary(c_mat)\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary        0.522 \n 2 kap                  binary        0.0237\n 3 sens                 binary        0.241 \n 4 spec                 binary        0.782 \n 5 ppv                  binary        0.507 \n 6 npv                  binary        0.526 \n 7 mcc                  binary        0.0277\n 8 j_index              binary        0.0233\n 9 bal_accuracy         binary        0.512 \n10 detection_prevalence binary        0.229 \n11 precision            binary        0.507 \n12 recall               binary        0.241 \n13 f_meas               binary        0.327 \n\n# calculate accuracy and error rate = 1 - accuracy\n# -&gt; base R\nmean(preds_logreg$predicted == data_stock$Direction)\n\n[1] 0.5216\n\n1 - mean(preds_logreg$predicted == data_stock$Direction)\n\n[1] 0.4784\n\n\nNow repeat analysis, but use a holdout sample.\n\n# sample data \ndata_train &lt;- data_stock %&gt;% \n  filter(Year &lt; 2005)\ndata_test &lt;- data_stock %&gt;% \n  filter(Year &gt;= 2005)\n\n# fit logistic regression model on training data\nmod_logreg2 &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n                   data = data_train,\n                   family = \"binomial\")\n\n# predict on holdout data\npreds_logreg2 &lt;- broom::augment(mod_logreg2,\n                              newdata = data_test,\n                              type.predict = \"response\") %&gt;% \n  select(Direction, .fitted) %&gt;% \n  mutate(predicted = if_else(.fitted &gt; 0.5, \"Up\", \"Down\") %&gt;% factor(levels = c(\"Down\", \"Up\")))\n\n# view results\npreds_logreg2 %&gt;% \n  yardstick::conf_mat(truth = \"Direction\",\n                      estimate = \"predicted\") #%&gt;% \n\n          Truth\nPrediction Down Up\n      Down   77 97\n      Up     34 44\n\n  #summary(event_level = \"second\")\n\nNow we can do some model selection to get a better model. After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement.\n\n# fit smaller model\nmod_logreg3 &lt;- glm(Direction ~ Lag1 + Lag2,\n                   data = data_train,\n                   family = \"binomial\")\n\n# view model summary\nmod_logreg3 %&gt;% broom::glance()\n\n# A tibble: 1 × 8\n  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;\n1         1383.     997  -691. 1387. 1402.    1381.         995   998\n\n# predict on holdout data\npreds_logreg3 &lt;- broom::augment(mod_logreg3,\n                              newdata = data_test,\n                              type.predict = \"response\") %&gt;% \n  select(Direction, .fitted) %&gt;% \n  mutate(predicted = if_else(.fitted &gt; 0.5, \"Up\", \"Down\") %&gt;% factor(levels = c(\"Down\", \"Up\")))\n\n# view results\npreds_logreg3 %&gt;% \n  yardstick::conf_mat(truth = \"Direction\",\n                      estimate = \"predicted\") %&gt;%\n  summary(event_level = \"second\")\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary        0.560 \n 2 kap                  binary        0.0698\n 3 sens                 binary        0.752 \n 4 spec                 binary        0.315 \n 5 ppv                  binary        0.582 \n 6 npv                  binary        0.5   \n 7 mcc                  binary        0.0744\n 8 j_index              binary        0.0671\n 9 bal_accuracy         binary        0.534 \n10 detection_prevalence binary        0.722 \n11 precision            binary        0.582 \n12 recall               binary        0.752 \n13 f_meas               binary        0.656 \n\n# calculate accuracy of naive approach, which is just predicting yes everyday\n# -&gt; get also numbers from truth confusion matrix\ndata_test %&gt;% \n  summarize(mean(Direction == \"Up\"))\n\n  mean(Direction == \"Up\")\n1               0.5595238\n\n\n\n5.2.3 Linear discriminant analysis\n\n# fit LDA model\nmod_lda &lt;- MASS::lda(Direction ~ Lag1 + Lag2, data = data_train)\n\n# view model\nmod_lda\n\nCall:\nlda(Direction ~ Lag1 + Lag2, data = data_train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\nCoefficients of linear discriminants:\n            LD1\nLag1 -0.6420190\nLag2 -0.5135293\n\n# plot model\n# -&gt; produces plots of the linear discriminants, obtained by computing −0.642 × Lag1 − 0.514 × Lag2 for each of the training observations.\nplot(mod_lda)\n\n\n\n\n\n\n\nThe coefficients of linear discriminants output provides the linear combination of Lag1 and Lag2 that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of X = x in Equation 5.11.\n\nIf −0.642 × Lag1 − 0.514 × Lag2 is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.\n\n# predict\npreds_lda &lt;- predict(mod_lda, newdata = data_test)\n\n# view prediction output\n# -&gt; class prediction\npreds_lda$class %&gt;% head\n\n[1] Up Up Up Up Up Up\nLevels: Down Up\n\n# -&gt; posterior probabilities\npreds_lda$posterior %&gt;% head\n\n       Down        Up\n1 0.4901792 0.5098208\n2 0.4792185 0.5207815\n3 0.4668185 0.5331815\n4 0.4740011 0.5259989\n5 0.4927877 0.5072123\n6 0.4938562 0.5061438\n\n# -&gt; linear discriminants\npreds_lda$x %&gt;% head\n\n          LD1\n1  0.08293096\n2  0.59114102\n3  1.16723063\n4  0.83335022\n5 -0.03792892\n6 -0.08743142\n\n# analyze predictions\ntable(preds_lda$class,\n      data_test$Direction,\n      dnn = c(\"predicted\", \"Direction\"))\n\n         Direction\npredicted Down  Up\n     Down   35  35\n     Up     76 106\n\n# calculate accuracy\nmean(preds_lda$class == data_test$Direction)\n\n[1] 0.5595238\n\n\n\n5.2.4 Quadratic discriminant analysis\n\n# fit QDA model\nmod_qda &lt;- MASS::qda(Direction ~ Lag1 + Lag2, data = data_train)\n\n# view model\n# -&gt; now output does not contain the coefficients of the linear discriminants, because no longer linear\nmod_qda\n\nCall:\nqda(Direction ~ Lag1 + Lag2, data = data_train)\n\nPrior probabilities of groups:\n    Down       Up \n0.491984 0.508016 \n\nGroup means:\n            Lag1        Lag2\nDown  0.04279022  0.03389409\nUp   -0.03954635 -0.03132544\n\n# predict\npreds_qda &lt;- predict(mod_qda, newdata = data_test)\n\n# analyze predictions\ntable(preds_qda$class,\n      data_test$Direction,\n      dnn = c(\"predicted\", \"Direction\"))\n\n         Direction\npredicted Down  Up\n     Down   30  20\n     Up     81 121\n\n# calculate accuracy\nmean(preds_qda$class == data_test$Direction)\n\n[1] 0.5992063\n\n\nA higher accuracy for the test data suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression.\n\n5.2.5 Naive Bayes\n\n# fit model\n# -&gt; by default, this models each quantitative feature with a Gaussian distribution\n# --&gt; but a kernal density method can also be used to estimate the distributions\nmod_nb &lt;- e1071::naiveBayes(Direction ~ Lag1 + Lag2, data = data_train)\n\n# view model output\nmod_nb\n\n\nNaive Bayes Classifier for Discrete Predictors\n\nCall:\nnaiveBayes.default(x = X, y = Y, laplace = laplace)\n\nA-priori probabilities:\nY\n    Down       Up \n0.491984 0.508016 \n\nConditional probabilities:\n      Lag1\nY             [,1]     [,2]\n  Down  0.04279022 1.227446\n  Up   -0.03954635 1.231668\n\n      Lag2\nY             [,1]     [,2]\n  Down  0.03389409 1.239191\n  Up   -0.03132544 1.220765\n\n# predict\npreds_nb &lt;- predict(mod_nb, newdata = data_test)\n\n# analyze predictions\ntable(preds_nb,\n      data_test$Direction,\n      dnn = c(\"predicted\", \"Direction\"))\n\n         Direction\npredicted Down  Up\n     Down   28  20\n     Up     83 121\n\n# calculate accuracy\nmean(preds_nb == data_test$Direction)\n\n[1] 0.5912698\n\n\nNaive Bayes performs very well on this data, with accurate predictions over 59% of the time. This is slightly worse than QDA, but much better than LDA.\n\n5.2.6 KNN\n\n# fit model\n# -&gt; this function forms prediction in a single command (not in two steps like the other models: fit then predict)\n# -&gt; NOTE: if there is a tie (say k = 2 and one observation from each class), then R randomly breaks the tie\n# --&gt; so need to set seed if want reproducibility\npreds_knn1 &lt;- class::knn(train = data_train %&gt;% select(Lag1, Lag2) %&gt;% as.matrix, # predictors of train set as matrix\n                      test = data_test %&gt;% select(Lag1, Lag2) %&gt;% as.matrix, # predictors of test set as matrix\n                      cl = data_train$Direction, # truth for train set\n                      k = 1)\n\n# analyze predictions\ntable(preds_knn1,\n      data_test$Direction,\n      dnn = c(\"predicted\", \"Direction\"))\n\n         Direction\npredicted Down Up\n     Down   43 58\n     Up     68 83\n\n# calculate accuracy\nmean(preds_knn1 == data_test$Direction)\n\n[1] 0.5\n\n# overfitting...\n\n# refit with better k\npreds_knn3 &lt;- class::knn(train = data_train %&gt;% select(Lag1, Lag2) %&gt;% as.matrix, # predictors of train set as matrix\n                      test = data_test %&gt;% select(Lag1, Lag2) %&gt;% as.matrix, # predictors of test set as matrix\n                      cl = data_train$Direction, # truth for train set\n                      k = 3)\n\n# analyze predictions\ntable(preds_knn3,\n      data_test$Direction,\n      dnn = c(\"predicted\", \"Direction\"))\n\n         Direction\npredicted Down Up\n     Down   48 55\n     Up     63 86\n\n# calculate accuracy\nmean(preds_knn3 == data_test$Direction)\n\n[1] 0.531746\n\n\nThe results have improved slightly. But increasing \\(K\\) further turns out to provide no further improvements.\nIt appears that for this data, QDA provides the best results of the methods that we have examined so far.\nNotes about KNN in general:\n\nBecause the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the distance between the observations, and hence on the KNN classifier, than variables that are on a small scale.\nFor instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of $1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. Same for scale of the response, changing units from dollars to cents will lead to different classification results.\nA good way to handle this problem is to standardize the data so that all variables are given a mean of zero and a standard deviation of one. Can use scale() to accomplish this.\n\nWhen using classifying methods in general, can look at the naive approach of guessing all positives as a baseline.\n\n5.2.7 Poisson regression\nFirst, try to fit a linear regression model.\n\n# load data\ndata_bike &lt;- ISLR2::Bikeshare\n\n# fit linear regression model\nmod_lr &lt;- lm(bikers ~ mnth + hr + workingday + temp + weathersit, data = data_bike)\n\n# view model summary\nsummary(mod_lr)\n\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = data_bike)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                          Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)                -68.632      5.307 -12.932 &lt; 0.0000000000000002 ***\nmnthFeb                      6.845      4.287   1.597             0.110398    \nmnthMarch                   16.551      4.301   3.848             0.000120 ***\nmnthApril                   41.425      4.972   8.331 &lt; 0.0000000000000002 ***\nmnthMay                     72.557      5.641  12.862 &lt; 0.0000000000000002 ***\nmnthJune                    67.819      6.544  10.364 &lt; 0.0000000000000002 ***\nmnthJuly                    45.324      7.081   6.401  0.00000000016282293 ***\nmnthAug                     53.243      6.640   8.019  0.00000000000000121 ***\nmnthSept                    66.678      5.925  11.254 &lt; 0.0000000000000002 ***\nmnthOct                     75.834      4.950  15.319 &lt; 0.0000000000000002 ***\nmnthNov                     60.310      4.610  13.083 &lt; 0.0000000000000002 ***\nmnthDec                     46.458      4.271  10.878 &lt; 0.0000000000000002 ***\nhr1                        -14.579      5.699  -2.558             0.010536 *  \nhr2                        -21.579      5.733  -3.764             0.000168 ***\nhr3                        -31.141      5.778  -5.389  0.00000007260801066 ***\nhr4                        -36.908      5.802  -6.361  0.00000000021092958 ***\nhr5                        -24.135      5.737  -4.207  0.00002611246755715 ***\nhr6                         20.600      5.704   3.612             0.000306 ***\nhr7                        120.093      5.693  21.095 &lt; 0.0000000000000002 ***\nhr8                        223.662      5.690  39.310 &lt; 0.0000000000000002 ***\nhr9                        120.582      5.693  21.182 &lt; 0.0000000000000002 ***\nhr10                        83.801      5.705  14.689 &lt; 0.0000000000000002 ***\nhr11                       105.423      5.722  18.424 &lt; 0.0000000000000002 ***\nhr12                       137.284      5.740  23.916 &lt; 0.0000000000000002 ***\nhr13                       136.036      5.760  23.617 &lt; 0.0000000000000002 ***\nhr14                       126.636      5.776  21.923 &lt; 0.0000000000000002 ***\nhr15                       132.087      5.780  22.852 &lt; 0.0000000000000002 ***\nhr16                       178.521      5.772  30.927 &lt; 0.0000000000000002 ***\nhr17                       296.267      5.749  51.537 &lt; 0.0000000000000002 ***\nhr18                       269.441      5.736  46.976 &lt; 0.0000000000000002 ***\nhr19                       186.256      5.714  32.596 &lt; 0.0000000000000002 ***\nhr20                       125.549      5.704  22.012 &lt; 0.0000000000000002 ***\nhr21                        87.554      5.693  15.378 &lt; 0.0000000000000002 ***\nhr22                        59.123      5.689  10.392 &lt; 0.0000000000000002 ***\nhr23                        26.838      5.688   4.719  0.00000241267941359 ***\nworkingday                   1.270      1.784   0.711             0.476810    \ntemp                       157.209     10.261  15.321 &lt; 0.0000000000000002 ***\nweathersitcloudy/misty     -12.890      1.964  -6.562  0.00000000005600358 ***\nweathersitlight rain/snow  -66.494      2.965 -22.425 &lt; 0.0000000000000002 ***\nweathersitheavy rain/snow -109.745     76.667  -1.431             0.152341    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 0.00000000000000022\n\nhead(coef(mod_lr))\n\n(Intercept)     mnthFeb   mnthMarch   mnthApril     mnthMay    mnthJune \n -68.631704    6.845203   16.551438   41.424907   72.557084   67.818749 \n\n# view predictions #check negative ones\nsum(predict(mod))\n\nError in UseMethod(\"predict\"): no applicable method for 'predict' applied to an object of class \"function\"\n\n\nNow we can change contrasts so can get a coefficient estimate for every level of the predictors.\n\nContrasts are an attribute of the column.\nDefault coding is dummy variable (aka treatment contrasts), where coefficients are set relative to the reference level.\nFor sum contrasts, all contrasts sum to 0 for each dummy variable and the reference level is in fact the grand mean. Also, now coefficients are the difference relative to the grand mean (which is now the intercept).\nLastly for sum contrasts, the last coefficient isn’t given in the output, but it can be easily calculated: it is the negative sum of the coefficient estimates for all of the other levels.\n\n\n# set new contrasts\ncontrasts(data_bike$hr) &lt;- \"contr.sum\"\ncontrasts(data_bike$mnth) &lt;- \"contr.sum\"\n\n# refit model\nmod_lr2 &lt;- lm(bikers ~ mnth + hr + workingday + temp + weathersit, data = data_bike)\n\n# view model summary\nsummary(mod_lr2)\n\n\nCall:\nlm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    data = data_bike)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-299.00  -45.70   -6.23   41.08  425.29 \n\nCoefficients:\n                           Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)                 73.5974     5.1322  14.340 &lt; 0.0000000000000002 ***\nmnth1                      -46.0871     4.0855 -11.281 &lt; 0.0000000000000002 ***\nmnth2                      -39.2419     3.5391 -11.088 &lt; 0.0000000000000002 ***\nmnth3                      -29.5357     3.1552  -9.361 &lt; 0.0000000000000002 ***\nmnth4                       -4.6622     2.7406  -1.701              0.08895 .  \nmnth5                       26.4700     2.8508   9.285 &lt; 0.0000000000000002 ***\nmnth6                       21.7317     3.4651   6.272   0.0000000003747098 ***\nmnth7                       -0.7626     3.9084  -0.195              0.84530    \nmnth8                        7.1560     3.5347   2.024              0.04295 *  \nmnth9                       20.5912     3.0456   6.761   0.0000000000146005 ***\nmnth10                      29.7472     2.6995  11.019 &lt; 0.0000000000000002 ***\nmnth11                      14.2229     2.8604   4.972   0.0000006740476467 ***\nhr1                        -96.1420     3.9554 -24.307 &lt; 0.0000000000000002 ***\nhr2                       -110.7213     3.9662 -27.916 &lt; 0.0000000000000002 ***\nhr3                       -117.7212     4.0165 -29.310 &lt; 0.0000000000000002 ***\nhr4                       -127.2828     4.0808 -31.191 &lt; 0.0000000000000002 ***\nhr5                       -133.0495     4.1168 -32.319 &lt; 0.0000000000000002 ***\nhr6                       -120.2775     4.0370 -29.794 &lt; 0.0000000000000002 ***\nhr7                        -75.5424     3.9916 -18.925 &lt; 0.0000000000000002 ***\nhr8                         23.9511     3.9686   6.035   0.0000000016537185 ***\nhr9                        127.5199     3.9500  32.284 &lt; 0.0000000000000002 ***\nhr10                        24.4399     3.9360   6.209   0.0000000005566528 ***\nhr11                       -12.3407     3.9361  -3.135              0.00172 ** \nhr12                         9.2814     3.9447   2.353              0.01865 *  \nhr13                        41.1417     3.9571  10.397 &lt; 0.0000000000000002 ***\nhr14                        39.8939     3.9750  10.036 &lt; 0.0000000000000002 ***\nhr15                        30.4940     3.9910   7.641   0.0000000000000239 ***\nhr16                        35.9445     3.9949   8.998 &lt; 0.0000000000000002 ***\nhr17                        82.3786     3.9883  20.655 &lt; 0.0000000000000002 ***\nhr18                       200.1249     3.9638  50.488 &lt; 0.0000000000000002 ***\nhr19                       173.2989     3.9561  43.806 &lt; 0.0000000000000002 ***\nhr20                        90.1138     3.9400  22.872 &lt; 0.0000000000000002 ***\nhr21                        29.4071     3.9362   7.471   0.0000000000000874 ***\nhr22                        -8.5883     3.9332  -2.184              0.02902 *  \nhr23                       -37.0194     3.9344  -9.409 &lt; 0.0000000000000002 ***\nworkingday                   1.2696     1.7845   0.711              0.47681    \ntemp                       157.2094    10.2612  15.321 &lt; 0.0000000000000002 ***\nweathersitcloudy/misty     -12.8903     1.9643  -6.562   0.0000000000560036 ***\nweathersitlight rain/snow  -66.4944     2.9652 -22.425 &lt; 0.0000000000000002 ***\nweathersitheavy rain/snow -109.7446    76.6674  -1.431              0.15234    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 76.5 on 8605 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 \nF-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 0.00000000000000022\n\ncoef(mod_lr2) %&gt;% {c(head(.), tail(.))}\n\n              (Intercept)                     mnth1                     mnth2 \n                73.597428                -46.087090                -39.241888 \n                    mnth3                     mnth4                     mnth5 \n               -29.535652                 -4.662183                 26.469993 \n                     hr23                workingday                      temp \n               -37.019399                  1.269601                157.209366 \n   weathersitcloudy/misty weathersitlight rain/snow weathersitheavy rain/snow \n               -12.890266                -66.494365               -109.744577 \n\n# create plot of the coefficients for one of the factor variables\ncoef(mod_lr2)[2:12] %&gt;% \n  {c(., -sum(.))} %&gt;% \n  plot(type = \"b\", xlab = \"Month\", ylab = \"Coefficient\")\n\n\n\n\n\n\n\nNow we can fit a Poisson regression model.\n\n# fit poisson regression model\nmod_pois &lt;- glm(bikers ~ mnth + hr + workingday + temp + weathersit,\n                data = data_bike,\n                family = \"poisson\")\n\n# view model summary\nsummary(mod_pois)\n\n\nCall:\nglm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, \n    family = \"poisson\", data = data_bike)\n\nCoefficients:\n                           Estimate Std. Error  z value             Pr(&gt;|z|)\n(Intercept)                4.118245   0.006021  683.964 &lt; 0.0000000000000002\nmnth1                     -0.670170   0.005907 -113.445 &lt; 0.0000000000000002\nmnth2                     -0.444124   0.004860  -91.379 &lt; 0.0000000000000002\nmnth3                     -0.293733   0.004144  -70.886 &lt; 0.0000000000000002\nmnth4                      0.021523   0.003125    6.888   0.0000000000056631\nmnth5                      0.240471   0.002916   82.462 &lt; 0.0000000000000002\nmnth6                      0.223235   0.003554   62.818 &lt; 0.0000000000000002\nmnth7                      0.103617   0.004125   25.121 &lt; 0.0000000000000002\nmnth8                      0.151171   0.003662   41.281 &lt; 0.0000000000000002\nmnth9                      0.233493   0.003102   75.281 &lt; 0.0000000000000002\nmnth10                     0.267573   0.002785   96.091 &lt; 0.0000000000000002\nmnth11                     0.150264   0.003180   47.248 &lt; 0.0000000000000002\nhr1                       -0.754386   0.007879  -95.744 &lt; 0.0000000000000002\nhr2                       -1.225979   0.009953 -123.173 &lt; 0.0000000000000002\nhr3                       -1.563147   0.011869 -131.702 &lt; 0.0000000000000002\nhr4                       -2.198304   0.016424 -133.846 &lt; 0.0000000000000002\nhr5                       -2.830484   0.022538 -125.586 &lt; 0.0000000000000002\nhr6                       -1.814657   0.013464 -134.775 &lt; 0.0000000000000002\nhr7                       -0.429888   0.006896  -62.341 &lt; 0.0000000000000002\nhr8                        0.575181   0.004406  130.544 &lt; 0.0000000000000002\nhr9                        1.076927   0.003563  302.220 &lt; 0.0000000000000002\nhr10                       0.581769   0.004286  135.727 &lt; 0.0000000000000002\nhr11                       0.336852   0.004720   71.372 &lt; 0.0000000000000002\nhr12                       0.494121   0.004392  112.494 &lt; 0.0000000000000002\nhr13                       0.679642   0.004069  167.040 &lt; 0.0000000000000002\nhr14                       0.673565   0.004089  164.722 &lt; 0.0000000000000002\nhr15                       0.624910   0.004178  149.570 &lt; 0.0000000000000002\nhr16                       0.653763   0.004132  158.205 &lt; 0.0000000000000002\nhr17                       0.874301   0.003784  231.040 &lt; 0.0000000000000002\nhr18                       1.294635   0.003254  397.848 &lt; 0.0000000000000002\nhr19                       1.212281   0.003321  365.084 &lt; 0.0000000000000002\nhr20                       0.914022   0.003700  247.065 &lt; 0.0000000000000002\nhr21                       0.616201   0.004191  147.045 &lt; 0.0000000000000002\nhr22                       0.364181   0.004659   78.173 &lt; 0.0000000000000002\nhr23                       0.117493   0.005225   22.488 &lt; 0.0000000000000002\nworkingday                 0.014665   0.001955    7.502   0.0000000000000627\ntemp                       0.785292   0.011475   68.434 &lt; 0.0000000000000002\nweathersitcloudy/misty    -0.075231   0.002179  -34.528 &lt; 0.0000000000000002\nweathersitlight rain/snow -0.575800   0.004058 -141.905 &lt; 0.0000000000000002\nweathersitheavy rain/snow -0.926287   0.166782   -5.554   0.0000000279379459\n                             \n(Intercept)               ***\nmnth1                     ***\nmnth2                     ***\nmnth3                     ***\nmnth4                     ***\nmnth5                     ***\nmnth6                     ***\nmnth7                     ***\nmnth8                     ***\nmnth9                     ***\nmnth10                    ***\nmnth11                    ***\nhr1                       ***\nhr2                       ***\nhr3                       ***\nhr4                       ***\nhr5                       ***\nhr6                       ***\nhr7                       ***\nhr8                       ***\nhr9                       ***\nhr10                      ***\nhr11                      ***\nhr12                      ***\nhr13                      ***\nhr14                      ***\nhr15                      ***\nhr16                      ***\nhr17                      ***\nhr18                      ***\nhr19                      ***\nhr20                      ***\nhr21                      ***\nhr22                      ***\nhr23                      ***\nworkingday                ***\ntemp                      ***\nweathersitcloudy/misty    ***\nweathersitlight rain/snow ***\nweathersitheavy rain/snow ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 1052921  on 8644  degrees of freedom\nResidual deviance:  228041  on 8605  degrees of freedom\nAIC: 281159\n\nNumber of Fisher Scoring iterations: 5\n\n# plot estimated coefficients (still using sum contrasts)\ncoef(mod_pois)[2:12] %&gt;% \n  {c(., -sum(.))} %&gt;% \n  plot(type = \"b\", xlab = \"Month\", ylab = \"Coefficient\")\n\n\n\n\n\n\n# make predictions\npreds_pois &lt;- predict(mod_pois, type = \"response\")",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ISLR -- Classification</span>"
    ]
  },
  {
    "objectID": "islr-4.html#exercises",
    "href": "islr-4.html#exercises",
    "title": "\n5  ISLR – Classification\n",
    "section": "\n5.3 Exercises",
    "text": "5.3 Exercises\n\n5.3.1 Conceptual\nQuestion 1\n\nUsing a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.\n\n&lt; already showed in notes &gt;\nQuestion 2\n\nIt was stated in the text that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest. Prove that this is the case. In other words, under the assumption that the observations in the \\(k\\)th class are drawn from a \\(N(\\mu_k,\\sigma^2)\\) distribution, the Bayes’ classifier assigns an observation to the class for which the discriminant function is maximized.\n\n\nQuestion 3\n\nThis problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where \\(p = 1\\); i.e. there is only one feature.\n\n\nSuppose that we have \\(K\\) classes, and that if an observation belongs to the kth class then \\(X\\) comes from a one-dimensional normal distribution, \\(X \\sim N(\\mu_k,\\sigma^2_k)\\). Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is not linear. Argue that it is in fact quadratic.\n\n\nHint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that \\(\\sigma_1^2 = ... = \\sigma_K^2\\).\n\n\nQuestion 4\n\nWhen the number of features \\(p\\) is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the curse of dimensionality, and it ties into the fact that non-parametric approaches often perform poorly when \\(p\\) is large. We will now investigate this curse.\n\n\n\nSuppose that we have a set of observations, each with measurements on \\(p = 1\\) feature, \\(X\\). We assume that \\(X\\) is uniformly (evenly) distributed on \\([0, 1]\\). Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10% of the range of \\(X\\) closest to that test observation. For instance, in order to predict the response for a test observation with \\(X = 0.6\\), we will use observations in the range \\([0.55, 0.65]\\). On average, what fraction of the available observations will we use to make the prediction?\n\n\nFor values in \\([0,0.05]\\), we use less than 10% of observations (between 5% and 10%, 7.5% on average), similarly with values in \\([0.95,1]\\). For values in \\([0.05,0.95]\\) we use 10% of available observations. The (weighted) average is then \\(7.5 \\times 0.1 + 10 \\times 0.9 = 9.75\\%\\).\n\n\nNow suppose that we have a set of observations, each with measurements on \\(p = 2\\) features, \\(X_1\\) and \\(X_2\\). We assume that \\((X_1, X_2)\\) are uniformly distributed on \\([0, 1] \\times [0, 1]\\). We wish to predict a test observation’s response using only observations that are within 10% of the range of \\(X_1\\) and within 10% of the range of \\(X_2\\) closest to that test observation. For instance, in order to predict the response for a test observation with \\(X_1 = 0.6\\) and \\(X_2 = 0.35\\), we will use observations in the range \\([0.55, 0.65]\\) for \\(X_1\\) and in the range \\([0.3, 0.4]\\) for \\(X_2\\). On average, what fraction of the available observations will we use to make the prediction?\n\n\nSince we need the observation to be within range for \\(X_1\\) and \\(X_2\\) we square 9.75% = \\(0.0975^2 \\times 100 = 0.95\\%\\)\n\n\nNow suppose that we have a set of observations on \\(p = 100\\) features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10% of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?\n\n\nSimilar to above, we use: \\(0.0975^{100} \\times 100 = 8 \\times 10^{-100}\\%\\), essentially zero.\n\n\nUsing your answers to parts (a)–(c), argue that a drawback of KNN when \\(p\\) is large is that there are very few training observations “near” any given test observation.\n\n\nAs \\(p\\) increases, the fraction of observations near any given point rapidly approaches zero. For instance, even if you use 50% of the nearest observations for each \\(p\\), with \\(p = 10\\), only \\(0.5^{10} \\times 100 \\approx 0.1\\%\\) points are “near”.\n\n\nNow suppose that we wish to make a prediction for a test observation by creating a \\(p\\)-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For $p = $1,2, and 100, what is the length of each side of the hypercube? Comment on your answer.\n\n\nWhen \\(p = 1\\), clearly the length is 0.1. When \\(p = 2\\), we need the value \\(l\\) such that \\(l^2 = 0.1\\), so \\(l = \\sqrt{0.1} \\approx 0.32\\). With \\(p\\) variables, \\(l = 0.1^{1/p}\\), so in the case of \\(p = 100\\), \\(l = 0.977\\). Therefore, the length of each side of the hypercube rapidly approaches 1 (or 100%) of the range of each \\(p\\).\nQuestion 5\n\nWe now examine the differences between LDA and QDA.\n\n\n\nIf the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nQDA because because of overfitting, will always perform better on the training set. But because the decision boundary is linear, on the testing set LDA will perform better.\n\n\nIf the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?\n\n\nQDA because because it is a more flexible model will perform better. And because the decision boundary is non-linear, on the testing set QDA will still perform better.\n\n\nIn general, as the sample size \\(n\\) increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?\n\n\nImprove, will have more data points to take into account the added parameters from LDA (less overfitting), more data to pick up on smaller effects.\n\n\nTrue or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.\n\n\nNot necessarily. The decrease in error rate from flexibility may not offset the increase in bias due to overfitting.\nQuestion 6\n\nSuppose we collect data for a group of students in a statistics class with variables \\(X_1 =\\) hours studied, \\(X_2 =\\) undergrad GPA, and \\(Y =\\) receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat\\beta_0 = -6\\), \\(\\hat\\beta_1= 0.05\\), \\(\\hat\\beta_2 = 1\\).\n\n\n\nEstimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class.\n\n\n\n\nHow many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?\n\n\n\nQuestion 7\n\nSuppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on \\(X\\), last year’s percent profit. We examine a large number of companies and discover that the mean value of \\(X\\) for companies that issued a dividend was \\(\\bar{X} = 10\\), while the mean for those that didn’t was \\(\\bar{X} = 0\\). In addition, the variance of \\(X\\) for these two sets of companies was \\(\\hat{\\sigma}^2 = 36\\). Finally, 80% of companies issued dividends. Assuming that \\(X\\) follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was \\(X = 4\\) last year.\n\n\nHint: Recall that the density function for a normal random variable is \\(f(x) =\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-(x-\\mu)^2/2\\sigma^2}\\). You will need to use Bayes’ theorem.\n\n\nQuestion 8\n\nSuppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. \\(K = 1\\)) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?\n\nFor \\(K = 1\\), performance on the training set is perfect and the error rate is zero, implying a test error rate of 36%. Logistic regression outperforms 1-nearest neighbor on the test set and therefore should be preferred.\nQuestion 9\n\nThis problem has to do with odds.\n\n\n\nOn average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\n\n\n\n\nSuppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?\n\n\n\nQuestion 10\n\nEquation 4.32 derived an expression for \\(\\log(\\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})\\) in the setting where \\(p &gt; 1\\), so that the mean for the \\(k\\)th class, \\(\\mu_k\\), is a \\(p\\)-dimensional vector, and the shared covariance \\(\\Sigma\\) is a \\(p \\times p\\) matrix. However, in the setting with \\(p = 1\\), (4.32) takes a simpler form, since the means \\(\\mu_1, ..., \\mu_k\\) and the variance \\(\\sigma^2\\) are scalars. In this simpler setting, repeat the calculation in (4.32), and provide expressions for \\(a_k\\) and \\(b_{kj}\\) in terms of \\(\\pi_k, \\pi_K, \\mu_k, \\mu_K,\\) and \\(\\sigma^2\\).\n\n\nQuestion 11\n\nWork out the detailed forms of \\(a_k\\), \\(b_{kj}\\), and \\(b_{kjl}\\) in (4.33). Your answer should involve \\(\\pi_k\\), \\(\\pi_K\\), \\(\\mu_k\\), \\(\\mu_K\\), \\(\\Sigma_k\\), and \\(\\Sigma_K\\).\n\n&lt; skipping &gt;\nQuestion 12\n\nSuppose that you wish to classify an observation \\(X \\in \\mathbb{R}\\) into apples and oranges. You fit a logistic regression model and find that\n\n\n\\[\n\\hat{Pr}(Y=orange|X=x) =\n\\frac{\\exp(\\hat\\beta_0 + \\hat\\beta_1x)}{1 + \\exp(\\hat\\beta_0 + \\hat\\beta_1x)}\n\\]\n\n\nYour friend fits a logistic regression model to the same data using the softmax formulation in (4.13), and finds that\n\n\n\\[\n\\hat{Pr}(Y=orange|X=x) =\n\\frac{\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x)}\n{\\exp(\\hat\\alpha_{orange0} + \\hat\\alpha_{orange1}x) + \\exp(\\hat\\alpha_{apple0} + \\hat\\alpha_{apple1}x)}\n\\]\n\n\n\nWhat is the log odds of orange versus apple in your model?\n\n\n\n\nWhat is the log odds of orange versus apple in your friend’s model?\n\n\n\n\nSuppose that in your model, \\(\\hat\\beta_0 = 2\\) and \\(\\hat\\beta_1 = −1\\). What are the coefficient estimates in your friend’s model? Be as specific as possible.\n\n\n\n\nNow suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates \\(\\hat\\alpha_{orange0} = 1.2\\), \\(\\hat\\alpha_{orange1} = −2\\), \\(\\hat\\alpha_{apple0} = 3\\), \\(\\hat\\alpha_{apple1} = 0.6\\). What are the coefficient estimates in your model?\n\n\n\n\nFinally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer.\n\n\n\n\n5.3.2 Applied\nQuestion 13\nEDA\n\n# load data\ndata_weekly &lt;- ISLR2::Weekly\n\n# table for the response\ndata_weekly %&gt;% \n  summarize(.by = Direction,\n            n = n()) %&gt;% \n  gt::gt() # try gtSummary to add total row\n\n\n\n\n\nDirection\nn\n\n\n\nDown\n484\n\n\nUp\n605\n\n\n\n\n\n# correlation plot\ndata_weekly %&gt;% \n  select(where(is.numeric)) %&gt;% \n  as.matrix %&gt;% \n  cor %&gt;% \n  corrplot::corrplot()\n\n\n\n\n\n\n# density plots of numeric predictors\ndata_weekly %&gt;% \n  pivot_longer(cols = starts_with(\"Lag\"),\n               names_to = \"lag\",\n               values_to = \"value\") %&gt;% \n  ggplot() + \n  geom_density(aes(x = value,\n                   color = lag)) + \n  facet_grid(Direction ~ .)\n\n\n\n\n\n\n# line plot of number of increases and decreases per year\ndata_weekly %&gt;% \n  group_by(Year, Direction) %&gt;% \n  summarize(n = n()) %&gt;% \n  ggplot(aes(x = Year,\n                y = n,\n                color = Direction)) + \n  geom_line() + \n  geom_smooth(se = FALSE)\n\n\n\n\n\n\n# histograms of other predictors\n# -&gt; some by class of response\nhist(data_weekly$Today)\n\n\n\n\n\n\ndata_weekly %&gt;% \n  split(.$Direction) %&gt;% \n  map(\\(df) hist(df$Volume))\n\n\n\n\n\n\n\n\n\n\n\n\n\n$Down\n$breaks\n [1]  0  1  2  3  4  5  6  7  8  9 10\n\n$counts\n [1] 230 132  39  19  30  19  11   3   0   1\n\n$density\n [1] 0.475206612 0.272727273 0.080578512 0.039256198 0.061983471 0.039256198\n [7] 0.022727273 0.006198347 0.000000000 0.002066116\n\n$mids\n [1] 0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5\n\n$xname\n[1] \"df$Volume\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n$Up\n$breaks\n [1] 0 1 2 3 4 5 6 7 8 9\n\n$counts\n[1] 314 138  52  29  31  23  11   6   1\n\n$density\n[1] 0.519008264 0.228099174 0.085950413 0.047933884 0.051239669 0.038016529\n[7] 0.018181818 0.009917355 0.001652893\n\n$mids\n[1] 0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5\n\n$xname\n[1] \"df$Volume\"\n\n$equidist\n[1] TRUE\n\nattr(,\"class\")\n[1] \"histogram\"\n\n\nObservations\n\nBalanced-ish response\nFluctuation in number of weeks with Up/Down by year, no real pattern though\nLag variables are roughly normal within class; Volume is not (right skewed)\nYear and Volume have a strong, positive correlation\n\nNow we can fit a logistic regression model\n\n# fit logistic regression model\nmod_logreg &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,\n                  data = data_weekly,\n                  family = \"binomial\")\n\n# view model summary\nbroom::tidy(mod_logreg)\n\n# A tibble: 7 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   0.267     0.0859     3.11  0.00190\n2 Lag1         -0.0413    0.0264    -1.56  0.118  \n3 Lag2          0.0584    0.0269     2.18  0.0296 \n4 Lag3         -0.0161    0.0267    -0.602 0.547  \n5 Lag4         -0.0278    0.0265    -1.05  0.294  \n6 Lag5         -0.0145    0.0264    -0.549 0.583  \n7 Volume       -0.0227    0.0369    -0.616 0.538  \n\n\nOnly Lag2 is significant.\nAnalyze predictions on training data.\n\n# calculate confusion matrix\nmod_logreg %&gt;% \n  broom::augment(type.predict = \"response\") %&gt;% \n  mutate(predicted = if_else(.fitted &gt; 0.5, \"Up\", \"Down\") %&gt;% as.factor) %&gt;% \n  yardstick::conf_mat(truth = \"Direction\",\n                      estimate = \"predicted\") %&gt;%\n  summary()\n\n# A tibble: 13 × 3\n   .metric              .estimator .estimate\n   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;\n 1 accuracy             binary        0.561 \n 2 kap                  binary        0.0350\n 3 sens                 binary        0.112 \n 4 spec                 binary        0.921 \n 5 ppv                  binary        0.529 \n 6 npv                  binary        0.564 \n 7 mcc                  binary        0.0550\n 8 j_index              binary        0.0322\n 9 bal_accuracy         binary        0.516 \n10 detection_prevalence binary        0.0937\n11 precision            binary        0.529 \n12 recall               binary        0.112 \n13 f_meas               binary        0.184 \n\n\nModel is misclassifying the true “Down”s at a high rate; model is not specific.\n\n# split data\ndata_train &lt;- data_weekly %&gt;% \n  filter(Year &lt;= 2008)\ndata_test &lt;- data_weekly %&gt;% \n  filter(Year &gt; 2008)\n\n# define function to fit different types of models\n# -&gt; NOTE: only works with first order models\nfit_model &lt;- function(model = c(\"logreg\", \"lda\", \"qda\", \"knn\", \"nb\"), formula, response_levels, df_train, df_test, threshold = 0.5, k = 1){\n  \n  # set items\n  model = match.arg(model)\n  \n  # knn does model and prediction in one step\n  if(identical(model, \"knn\")) {\n    \n    # extract string of predictors (and format as vector) and response from formula\n    x = formula %&gt;% str_sub(.,\n                            start =  str_locate(., \"~\")[1]+2,\n                            end = -1) %&gt;% \n      data.frame(x = .) %&gt;% \n      separate_wider_delim(cols = x,\n                           delim = \" + \",\n                           names_sep = \"\") %&gt;% \n      reduce(.f = c)\n    y = formula %&gt;% str_sub(.,\n                            start = 1,\n                            end = str_locate(., \"~\")[1]-2) \n    \n    # fit model and calculate predictions\n    pred_class = class::knn(train = df_train %&gt;% select(any_of(x)) %&gt;% as.matrix,\n                            test = df_test %&gt;% select(any_of(x)) %&gt;% as.matrix,\n                            cl = df_train %&gt;% pull(any_of(y)),\n                            k = k) %&gt;% \n      data.frame(pred_class = .)\n    \n    # initialize empty dataframe for returning\n    pred_prob = data.frame(pred_prob = rep(NA, length(pred_class)))\n    \n  }\n  else{\n    \n    # set item\n    formula = as.formula(formula)\n    \n    # fit model\n    mod = if(identical(model, \"logreg\")){\n      \n      glm(formula, data = df_train, family = \"binomial\")\n      \n    }\n    else if(identical(model, \"lda\")){\n      \n      MASS::lda(formula, data = df_train)\n      \n    }\n    else if(identical(model, \"qda\")){\n      \n      MASS::qda(formula, data = df_train)\n      \n    }\n    else if(identical(model, \"nb\")){\n      \n      e1071::naiveBayes(formula, data = df_train)\n\n    }\n    else{\n      \n      e1071::naiveBayes(formula, data = df_train)\n    }\n    \n    # make predictions\n    pred_prob = if(identical(model, \"logreg\")){\n      \n      predict(mod, type = \"response\", newdata = df_test)\n      \n    }\n    else if(model %in% c(\"lda\", \"qda\")){\n      \n      predict(mod, newdata = df_test)$posterior[,response_levels[2]]\n      \n    }\n    else{\n      \n      predict(mod, type = \"raw\", newdata = df_test)[,response_levels[2]]\n\n    }\n    \n    # classify based on predicted probability and threshold\n    pred_class = ifelse(pred_prob &gt; threshold, 1, 0) %&gt;% \n      data.frame(pred_class = .) %&gt;% \n      mutate(pred_class = case_when(pred_class == 1 ~ response_levels[2],\n                                    .default = response_levels[1]) %&gt;% factor(levels = response_levels))\n  \n  }\n  \n  bind_cols(pred_prob = pred_prob, pred_class) %&gt;% return\n  \n}\n\n# initialize items\nmodels &lt;- c(\"logreg\", \"lda\", \"qda\", \"knn\", \"nb\")  \nformula &lt;- \"Direction ~ Lag2\"\nk &lt;- 1\n\n# define function to calculate confusion matrix / summary statistics\n# -&gt; NOTE: preds is output of fit_model()\ncalc_conf_mat &lt;- function(preds, df_test, truth, estimate) {\n  \n  df_test %&gt;% \n      bind_cols(preds) %&gt;% \n      yardstick::conf_mat(truth = truth,\n                          estimate = estimate)\n     \n}\n\n# fit models and assess quality\n# -&gt; good enough, could loop over a set of formulas\n# -&gt; just manually change formula / k above and look at results\n# -&gt; change event_level if needed\nmodels %&gt;% \n  set_names(x = ., nm = .) %&gt;% \n  map(\\(model) fit_model(model = model, formula = formula, response_levels = c(\"Down\", \"Up\"), df_train = data_train, df_test = data_test, threshold = 0.5, k = k)) %&gt;% \n  map(\\(preds) calc_conf_mat(preds, df_test = data_test, truth = \"Direction\", estimate = \"pred_class\")) %&gt;% \n  map(\\(conf_mat) summary(conf_mat, event_level = \"second\")[1,]) %&gt;% \n  reduce(bind_rows) %&gt;% \n  bind_cols(data.frame(model = models,\n                       formula = formula)) %&gt;% \n  select(model, formula, .estimate) %&gt;% \n  mutate(k = ifelse(model == \"knn\", k, NA),\n         .after = formula)\n\n# A tibble: 5 × 4\n  model  formula              k .estimate\n  &lt;chr&gt;  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n1 logreg Direction ~ Lag2    NA     0.625\n2 lda    Direction ~ Lag2    NA     0.625\n3 qda    Direction ~ Lag2    NA     0.587\n4 knn    Direction ~ Lag2     1     0.5  \n5 nb     Direction ~ Lag2    NA     0.587\n\n\nLogistic regression and LDA are the best performing.\n&lt; Didn’t experiment too much with different predictor sets / transformations &gt;\nQuestion 14\n\n# load data and modify\ndata_mpg &lt;- ISLR2::Auto %&gt;% \n  mutate(mpg01 = ifelse(mpg &lt;= median(mpg), 0, 1) %&gt;% as.factor)\n\n# comparative boxplots of the response against each numeric X\nnms_x &lt;- data_mpg %&gt;% \n  select(where(is.numeric), -mpg) %&gt;% \n  colnames\nmap2(data_mpg %&gt;% select(where(is.numeric), -mpg), nms_x, function(x, nm) {\n  boxplot(x ~ mpg01, main = nm, data = data_mpg)\n})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$cylinders\n$cylinders$stats\n     [,1] [,2]\n[1,]    3    4\n[2,]    6    4\n[3,]    8    4\n[4,]    8    4\n[5,]    8    4\n\n$cylinders$n\n[1] 196 196\n\n$cylinders$conf\n         [,1] [,2]\n[1,] 7.774286    4\n[2,] 8.225714    4\n\n$cylinders$out\n [1] 6 6 5 8 8 6 6 5 6 3 6 6 6 6 8 6 6\n\n$cylinders$group\n [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n\n$cylinders$names\n[1] \"0\" \"1\"\n\n\n$displacement\n$displacement$stats\n     [,1] [,2]\n[1,]   70   68\n[2,]  225   91\n[3,]  261  105\n[4,]  350  134\n[5,]  455  198\n\n$displacement$n\n[1] 196 196\n\n$displacement$conf\n         [,1]     [,2]\n[1,] 246.8929 100.1471\n[2,] 275.1071 109.8529\n\n$displacement$out\n[1] 200 350 260 350 262\n\n$displacement$group\n[1] 2 2 2 2 2\n\n$displacement$names\n[1] \"0\" \"1\"\n\n\n$horsepower\n$horsepower$stats\n     [,1]  [,2]\n[1,]   72  46.0\n[2,]  100  67.0\n[3,]  125  76.5\n[4,]  150  90.0\n[5,]  225 120.0\n\n$horsepower$n\n[1] 196 196\n\n$horsepower$conf\n         [,1]     [,2]\n[1,] 119.3571 73.90429\n[2,] 130.6429 79.09571\n\n$horsepower$out\n[1] 230 125 132\n\n$horsepower$group\n[1] 1 2 2\n\n$horsepower$names\n[1] \"0\" \"1\"\n\n\n$weight\n$weight$stats\n       [,1] [,2]\n[1,] 2124.0 1613\n[2,] 3139.5 2045\n[3,] 3607.0 2229\n[4,] 4159.5 2610\n[5,] 5140.0 3420\n\n$weight$n\n[1] 196 196\n\n$weight$conf\n         [,1]     [,2]\n[1,] 3491.886 2165.236\n[2,] 3722.114 2292.764\n\n$weight$out\n[1] 3530 3900 3725\n\n$weight$group\n[1] 2 2 2\n\n$weight$names\n[1] \"0\" \"1\"\n\n\n$acceleration\n$acceleration$stats\n     [,1]  [,2]\n[1,]  8.0 11.30\n[2,] 12.9 14.70\n[3,] 14.5 16.20\n[4,] 16.3 17.95\n[5,] 21.0 22.20\n\n$acceleration$n\n[1] 196 196\n\n$acceleration$conf\n         [,1]     [,2]\n[1,] 14.11629 15.83321\n[2,] 14.88371 16.56679\n\n$acceleration$out\n[1] 21.9 23.5 24.8 23.7 24.6\n\n$acceleration$group\n[1] 1 2 2 2 2\n\n$acceleration$names\n[1] \"0\" \"1\"\n\n\n$year\n$year$stats\n     [,1] [,2]\n[1,]   70   70\n[2,]   72   75\n[3,]   74   78\n[4,]   77   81\n[5,]   82   82\n\n$year$n\n[1] 196 196\n\n$year$conf\n         [,1]     [,2]\n[1,] 73.43571 77.32286\n[2,] 74.56429 78.67714\n\n$year$out\nnumeric(0)\n\n$year$group\nnumeric(0)\n\n$year$names\n[1] \"0\" \"1\"\n\n\n$origin\n$origin$stats\n     [,1] [,2]\n[1,]    1    1\n[2,]    1    1\n[3,]    1    2\n[4,]    1    3\n[5,]    1    3\n\n$origin$n\n[1] 196 196\n\n$origin$conf\n     [,1]     [,2]\n[1,]    1 1.774286\n[2,]    1 2.225714\n\n$origin$out\n [1] 3 2 2 2 3 3 3 2 2 3 2 2 2 3 2 3 2 3 3 2 2 2 2\n\n$origin$group\n [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n\n$origin$names\n[1] \"0\" \"1\"\n\n# scatterplot matrix\npairs(data_mpg %&gt;% select(where(is.numeric)))\n\n\n\n\n\n\n\nDisplacement, horsepower, weight, and year appear to have a relationship with mpg.\nNow we can split into train and test sets.\n\n# make test / train split\nsplit_mpg &lt;- rsample::initial_split(data = data_mpg, prop = .8)\n\n# save train and data\ndata_train &lt;- split_mpg %&gt;% rsample::training()\ndata_test &lt;- split_mpg %&gt;% rsample::testing()\n\n# fit all models and get error rates\nmodels &lt;- c(\"logreg\", \"lda\", \"qda\", \"knn\", \"nb\")\nk &lt;- 7\nmodels %&gt;% \n  set_names(x = ., nm = .) %&gt;% \n  map(\\(model) fit_model(model = model, formula = \"mpg01 ~ displacement + horsepower + weight + year\", response_levels = c(\"0\", \"1\"), df_train = data_train, df_test = data_test, threshold = 0.5, k = k)) %&gt;% \n  map(\\(preds) calc_conf_mat(preds, df_test = data_test, truth = \"mpg01\", estimate = \"pred_class\")) %&gt;% \n  map(\\(conf_mat) summary(conf_mat, event_level = \"second\")[1,]) %&gt;% # pull accuracy (first row)\n  reduce(bind_rows) %&gt;% \n  mutate(error_rate = 1 - .estimate) %&gt;%  # add error rates\n  bind_cols(data.frame(model = models)) %&gt;% \n  select(model, accuracy = .estimate, error_rate) %&gt;% # rename summary of interest\n  mutate(k = ifelse(model == \"knn\", k, NA),\n         .after = model)\n\n# A tibble: 5 × 4\n  model      k accuracy error_rate\n  &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 logreg    NA    0.886     0.114 \n2 lda       NA    0.911     0.0886\n3 qda       NA    0.873     0.127 \n4 knn        7    0.873     0.127 \n5 nb        NA    0.899     0.101 \n\n\nFor the models tested here, \\(k = 7\\) appears to perform best. QDA has a lower error rate overall than LDA and logistic regression, but only slightly.\nThese results suggests the decision boundary is slightly more complicated than linear.\nQuestion 15\n&lt; function writing &gt;\nQuestion 16\n&lt; another model fitting problem with a different dataset, save for learning tidy models &gt;",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>ISLR -- Classification</span>"
    ]
  },
  {
    "objectID": "partA-prob.html",
    "href": "partA-prob.html",
    "title": "Probability Models",
    "section": "",
    "text": "This section contains notes for Part A of the syllabus: Probability Models.",
    "crumbs": [
      "Probability Models"
    ]
  },
  {
    "objectID": "partB-stats.html",
    "href": "partB-stats.html",
    "title": "Statistics",
    "section": "",
    "text": "This section contains notes for Part B of the syllabus: Statistics.",
    "crumbs": [
      "Statistics"
    ]
  },
  {
    "objectID": "partC-glm.html",
    "href": "partC-glm.html",
    "title": "Extended Linear Models",
    "section": "",
    "text": "This section contains notes for Part C of the syllabus: Extended Linear Models.",
    "crumbs": [
      "Extended Linear Models"
    ]
  },
  {
    "objectID": "islr-5.html",
    "href": "islr-5.html",
    "title": "\n6  ISLR – Resampling methods\n",
    "section": "",
    "text": "6.1 Notes\nResampling methods are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model. For example, in order to estimate the variability of a linear regression fit, we can repeatedly draw different samples from the training data, fit a linear regression to each new sample, and then examine the extent to which the resulting fits differ. Such an approach may allow us to obtain information that would not be available from fitting the model only once using the original training sample.\nIn this chapter, we discuss two of the most commonly used resampling methods, cross-validation and the bootstrap. Both methods are important tools in the practical application of many statistical learning procedures. For example, cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility. The process of evaluating a model’s performance is known as model assessment, whereas the process of selecting the proper level of flexibility for a model is known as model selection. The bootstrap is used in several contexts, most commonly to provide a measure of accuracy of a parameter estimate or of a given statistical learning method.",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ISLR -- Resampling methods</span>"
    ]
  },
  {
    "objectID": "islr-5.html#notes",
    "href": "islr-5.html#notes",
    "title": "\n6  ISLR – Resampling methods\n",
    "section": "",
    "text": "6.1.1 Cross-validation\nGiven a data set, the use of a particular statistical learning method is warranted if it results in a low test error. The test error can be easily calculated if a designated test set is available. Unfortunately, this is usually not the case. The training error rate can easily be found, but it dramatically underestimates the test error rate.\nIn the absence of a very large designated test set that can be used to directly estimate the test error rate, a number of techniques can be used to estimate this quantity using the available training data. Some methods make a mathematical adjustment to the training error rate in order to estimate the test error rate. These are discussed in later chapters. In this section, we instead consider a class of methods that estimate the test error rate by holding out a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.\nThe validation set approach\n\n\nThe validation set approach is conceptually simple and is easy to imple- ment. But it has two potential drawbacks:\n\nAs is shown in the right-hand panel of Figure 5.2, the validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\nIn the validation approach, only a subset of the observations – those that are included in the training set rather than in the validation set – are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the validation set error rate may tend to overestimate the test error rate for the model fit on the entire data set.\n\nIn the coming subsections, we will present cross-validation, a refinement of the validation set approach that addresses these two issues.\nLeave-one-out-cross-validation\n\nThe LOOCV estimate for the test MSE is the average of these \\(n\\) test error estimates:\n\\[\n\\text{CV}_{(n)} = \\frac{1}{n} \\sum_{i = 1}^n \\text{MSE}_i\n\\]\nLOOCV has a couple of major advantages over the validation set approach.\n\nFirst, it has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain \\(n − 1\\) observations, almost as many as are in the entire data set. This is in contrast to the validation set approach, in which the training set is typically around half the size of the original data set. Consequently, the LOOCV approach tends not to overestimate the test error rate as much as the validation set approach does.\nSecond, in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.\n\n\nLOOCV has the potential to be expensive to implement, since the model has to be fit \\(n\\) times. This can be very time consuming if \\(n\\) is large, and if each individual model is slow to fit. With least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:\n\\[\n\\text{CV}_{(n)} = \\frac{1}{n} \\sum_{i = 1}^n \\big(\\frac{y_i - \\hat{y}_i}{1 - h_i}\\big)^2\n\\]\nwhere \\(\\hat{y}_i\\) is the \\(i\\)th fitted value from the original least squares fit, and \\(h_i\\) is the leverage defined Equation 4.1. This is like the ordinary MSE, except the \\(i\\)th residual is divided by \\(1 − h_i\\). The leverage lies between \\(1/n\\) and 1, and reflects the amount that an observation influences its own fit. Hence the residuals for high-leverage points are inflated in this formula by exactly the right amount for this equality to hold.\nLOOCV is a very general method, and can be used with any kind of predictive modeling.\n\n\\(k\\)-fold cross-validation\n\nThe k-fold CV estimate is computed by averaging these values,\n\\[\n\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{i = 1}^k \\text{MSE}_i\n\\]\nIt is not hard to see that LOOCV is a special case of \\(k\\)-fold CV in which \\(k\\) is set to equal \\(n\\). In practice, one typically performs \\(k\\)-fold CV using \\(k = 5\\) or \\(k = 10\\). The obvious advantage of using \\(k = 5\\) or \\(k = 10\\) rather than \\(k = n\\) is the computational benefits? And cross-validation is a very general approach that can be applied to almost any statistical learning method. The next section describes the non-computational benefits that exist as well.\nWhen we examine real data, we do not know the true test MSE, and so it is difficult to determine the accuracy of the cross-validation estimate. However, if we examine simulated data, then we can compute the true test MSE, and can thereby evaluate the accuracy of our cross-validation results.\n\nIn all three plots, the two cross-validation estimates are very similar.\nWhen we perform cross-validation, our goal might be to determine how well a given statistical learning procedure can be expected to perform on independent data; in this case, the actual estimate of the test MSE is of interest. But at other times we are interested only in the location of the minimum point in the estimated test MSE curve. This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error. For this purpose, the location of the minimum point in the estimated test MSE curve is important, but the actual value of the estimated test MSE is not. We find in Figure 5.6 that despite the fact that they sometimes underestimate the true test MSE, all of the CV curves come close to identifying the correct level of flexibility – that is, the flexibility level corresponding to the smallest test MSE.\nBias-variance trade-off for \\(k\\)-fold cross-validation\nThere is potentially more important advantage of \\(k\\)-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV. This has to do with a bias-variance trade-off.\nIt was mentioned earlier that the validation set approach can lead to overestimates of the test error rate, since in this approach the training set used to fit the statistical learning method contains only half the observations of the entire data set. Using this logic, it is not hard to see that LOOCV will give approximately unbiased estimates of the test error, since each training set contains \\(n - 1\\) observations, which is almost as many as the number of observations in the full data set. And performing \\(k\\)-fold CV for, say, \\(k = 5\\) or \\(k = 10\\) will lead to an intermediate level of bias, since each training set contains approximately \\((k − 1)n/k\\) observations – fewer than in the LOOCV approach, but substantially more than in the validation set approach. Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to \\(k\\)-fold CV.\nHowever, we know that bias is not the only source for concern in an estimating procedure; we must also consider the procedure’s variance. It turns out that LOOCV has higher variance than does \\(k\\)-fold CV with \\(k &lt; n\\). Why is this the case? When we perform LOOCV, we are in effect averaging the outputs of \\(n\\) fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, when we perform \\(k\\)-fold CV with \\(k &lt; n\\), we are averaging the outputs of \\(k\\) fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from \\(k\\)-fold CV.\nTo summarize, there is a bias-variance trade-off associated with the choice of \\(k\\) in \\(k\\)-fold cross-validation. Typically, given these considerations, one performs \\(k\\)-fold cross-validation using \\(k = 5\\) or \\(k = 10\\), as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.\nCross-validation on classification problems\nCross-validation can also be a very useful approach in the classification setting when \\(Y\\) is qualitative. For instance, in the classification setting, the LOOCV error rate takes the form (the k-fold CV error rate and validation set error rates are defined analogously):\n\\[\n\\text{CV}_{(n)} = \\frac{1}{n} \\sum_{i = 1}^n I(y_i \\ne \\hat{y}_i)\n\\]\nAs an example, we fit various logistic regression models. Since this is simulated data, we can compute the true test error rate, which takes a value of 0.201 and so is substantially larger than the Bayes error rate of 0.133. Clearly logistic regression does not have enough flexibility to model the Bayes decision boundary in this setting. We can easily extend logistic regression to obtain a non-linear decision boundary by using polynomial functions of the predictors, as we did in the regression setting. For example, we can fit a quadratic logistic regression model, given by\n\\[\n\\log\\big(\\frac{p}{1 - p}\\big) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_1^2 + \\beta_3 X_2 + \\beta_4 X_2^2\n\\]\n\nIn practice, for real data, the Bayes decision boundary and the test er- ror rates are unknown. So how might we decide between the four logistic regression models displayed in Figure 5.7? We can use cross-validation in order to make this decision.\nAs we have seen previously, the training error tends to decrease as the flexibility of the fit increases. (The figure indicates that though the training error rate doesn’t quite decrease monotonically, it tends to decrease on the whole as the model complexity increases.) In contrast, the test error displays a characteristic U-shape. The 10-fold CV error rate provides a pretty good approximation to the test error rate. While it somewhat underestimates the error rate, it reaches a minimum when fourth-order polynomials are used, which is very close to the minimum of the test curve, which occurs when third-order polynomials are used. On the right, Again the training error rate declines as the method becomes more flexible, and so we see that the training error rate cannot be used to select the optimal value for \\(K\\). Though the cross-validation error curve slightly underestimates the test error rate, it takes on a minimum very close to the best value for \\(K\\).\n\n\n6.1.2 The bootstrap\nThe bootstrap is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method. As a simple example, the bootstrap can be used to estimate the standard errors of the coefficients from a linear regression fit. In the specific case of linear regression, this is not particularly useful, since we saw know that standard statistical software such as R outputs such standard errors automatically. However, the power of the bootstrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software.\nWith simulated data, can generate new samples and estimate the sampling distribution of the statistic of interest very easily and get the standard error. In practice, however, the procedure for estimating \\(SE( &lt; \\text{statistic} &gt;)\\) cannot be applied, because for real data we cannot generate new samples. However, the bootstrap approach allows us to use a computer to emulate the process of obtaining new sample sets, so that we can estimate the variability of our statistic without generating additional samples. Rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set.\n\n\nAgain, the boxplots have similar spreads, indicating that the bootstrap approach can be used to effectively estimate the variability associated with our statistic.\nBootstrap estimates:\n\nOf location are biased\nOf spread are fairly accurate\nBetter description\nOne of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required.",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ISLR -- Resampling methods</span>"
    ]
  },
  {
    "objectID": "islr-5.html#lab",
    "href": "islr-5.html#lab",
    "title": "\n6  ISLR – Resampling methods\n",
    "section": "\n6.2 Lab",
    "text": "6.2 Lab\nNOTE: Will learn tidymodels after taking MAS I exam\n\n6.2.1 The validation set approach\n\n# load data\ndata_auto &lt;- ISLR2::Auto\n\n# split into training and testing\ndata_split &lt;- data_auto %&gt;% rsample::initial_split(prop = .7)\ndata_train &lt;- data_split %&gt;% rsample::training()\ndata_test &lt;- data_split %&gt;% rsample::testing()\n\n# fit various degrees of polynomial models\nmods_lm &lt;- c(1:4) %&gt;% \n  map(\\(power) lm(mpg ~ poly(horsepower, power), data = data_train))\n  \n# make predictions\npreds_lm &lt;- mods_lm %&gt;% \n  map(\\(mod) predict(mod, newdata = data_test))\n\n# calculate mse\npreds_lm %&gt;% \n  map(\\(preds) yardstick::rmse_vec(truth = data_test$mpg, estimate = preds) %&gt;% raise_to_power(2))\n\n[[1]]\n[1] 21.56111\n\n[[2]]\n[1] 18.38995\n\n[[3]]\n[1] 18.56034\n\n[[4]]\n[1] 18.4975\n\n\n\n6.2.2 Leave-one-out cross-validation\nThe LOOCV estimate can be automatically computed for any generalized linear model using the glm() and boot::cv.glm() functions; and by default family = \"gaussian\", which means it does linear regression.\n\n# fit model\nmod_glm &lt;- glm(mpg ~ horsepower, data = data_auto)\n\n# obtain LOOCV estimate\ncv_err &lt;- boot::cv.glm(data_auto, mod_glm)\n\ncv_err$delta gives two numbers: &lt; first &gt; = LOOCV estimate, &lt; second &gt; = bias-adjusted CV error estimate (for when using \\(k &lt; n\\))\n\n# extract results\n# -&gt; two numbers are identical here\ncv_err$delta\n\n[1] 24.23151 24.23114\n\n\n\n# perform LOOCV for different polynomial models and obtain the CV error estimates\nc(1:4) %&gt;% \n  map(function(power, df = data_auto) {\n    glm(mpg ~ poly(horsepower, power), data = df) %&gt;% \n      boot::cv.glm(df, .) %&gt;% \n      .$delta %&gt;% \n      .[1]\n  }) \n\n[[1]]\n[1] 24.23151\n\n[[2]]\n[1] 19.24821\n\n[[3]]\n[1] 19.33498\n\n[[4]]\n[1] 19.42443\n\n\n\n6.2.3 \\(k\\)-fold cross-validation\nThe cv.glm() function can also be used to implement \\(k\\)-fold CV.\n\n# perform k-fold CV for different polynomial models and obtain the CV error estimates\n# -&gt; just have to specify K = k\nc(1:4) %&gt;% \n  map(function(power, df = data_auto) {\n    glm(mpg ~ poly(horsepower, power), data = df) %&gt;% \n      boot::cv.glm(df, ., K = 10) %&gt;% \n      .$delta %&gt;% \n      .[1]\n  }) \n\n[[1]]\n[1] 24.16933\n\n[[2]]\n[1] 19.21619\n\n[[3]]\n[1] 19.27685\n\n[[4]]\n[1] 19.44821\n\n\n\n6.2.4 The bootstrap\n\nAlready created some examples here",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ISLR -- Resampling methods</span>"
    ]
  },
  {
    "objectID": "islr-5.html#exercises",
    "href": "islr-5.html#exercises",
    "title": "\n6  ISLR – Resampling methods\n",
    "section": "\n6.3 Exercises",
    "text": "6.3 Exercises\n\n6.3.1 Conceptual\nQuestion 1\n\nUsing basic statistical properties of the variance, as well as single-variable calculus, derive (5.6). In other words, prove that \\(\\alpha\\) given by (5.6) does indeed minimize \\(Var(\\alpha X + (1 - \\alpha)Y)\\).\n\n\nQuestion 2\n\nWe will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.\n\n\n\nWhat is the probability that the first bootstrap observation is not the \\(j\\)th observation from the original sample? Justify your answer.\n\n\n\\[\n1 - 1/n\n\\]\n\n\nWhat is the probability that the second bootstrap observation is not the \\(j\\)th observation from the original sample?\n\n\n\\[\n1 - 1/n\n\\]\n\n\nArgue that the probability that the \\(j\\)th observation is not in the bootstrap sample is \\((1 - 1/n)^n\\).\n\n\n\\(n\\) independent events -&gt; Multliply the probabilities\n\n\nWhen \\(n = 5, 100, 10000\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample?\n\n\n\\[\n1 - (1 - 1/n)^n\n\\]\n\nboot_prob &lt;- function(n) {\n  1 - (1 - 1 / n)^n\n}\n\nc(5, 100, 10000) %&gt;% \n  map(\\(n) boot_prob(n))\n\n[[1]]\n[1] 0.67232\n\n[[2]]\n[1] 0.6339677\n\n[[3]]\n[1] 0.632139\n\n\n\n\nCreate a plot that displays, for each integer value of \\(n\\) from 1 to 100, the probability that the \\(j\\)th observation is in the bootstrap sample. Comment on what you observe.\n\n\n\ndata.frame(n = c(1:100)) %&gt;% \n  mutate(prob = boot_prob(n)) %$%\n  plot(x = n, y = prob, type = \"o\")\n\n\n\n\n\n\n\nApproaches a limit.\n\n\nWe will now investigate numerically the probability that a bootstrap sample of size \\(n = 100\\) contains the \\(j\\)th observation. Here \\(j = 4\\). We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.\n\n\n\nstore &lt;- rep (NA, 10000)\nfor (i in 1:10000) {\n  store[i] &lt;- sum(sample(1:100, rep = TRUE) == 4) &gt; 0\n}\nmean(store)\n\n[1] 0.631\n\n\n\nComment on the results obtained.\n\nThe probability of including \\(4\\) when resampling numbers \\(1,...,100\\) is close to \\(1 - (1 - 1/100)^{100}\\).\nQuestion 3\n\n\nWe now review \\(k\\)-fold cross-validation.\n\n\n\n\nExplain how \\(k\\)-fold cross-validation is implemented.\n\n\nSteps:\n\nData is randomply split into \\(k\\) subsections, called folds.\nModel is fit on all but 1 of the subsections, and is used to predict data in the remaining section.\nFinal error estimate is averaged across all models.\n\n\n\nWhat are the advantages and disadvantages of \\(k\\)-fold cross-validation relative to:\nThe validation set approach?\n\n\nLOOCV?\n\n\n\n\\(k\\)-fold is way less biased because more data is used to fit the model (overestimates test error rate relative to using the entire dataset to fit the model); does not depend on the split as heavily as the validation approach\n\\(k\\)-fold is computationally easier and final results (error estimates) are less variable (won’t get as big of difference if fit on different set of data)\nQuestion 4\n\nSuppose that we use some statistical learning method to make a prediction for the response \\(Y\\) for a particular value of the predictor \\(X\\). Carefully describe how we might estimate the standard deviation of our prediction.\n\n\nCreate 10,000 bootstrap samples\nFit the same model on each bootstrap sample\nPredict the new observation using each model.\nCalculate standard deviation of predictions\n\n6.3.2 Applied\n!!! TO BE DONE ONCE LEARN TIDYMODELS\nQuestion 5\nQuestion 6\nQuestion 7\nQuestion 8\nQuestion 9",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>ISLR -- Resampling methods</span>"
    ]
  },
  {
    "objectID": "glm-1.html",
    "href": "glm-1.html",
    "title": "\n7  GLM – Introduction\n",
    "section": "",
    "text": "7.1 Notes",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>GLM -- Introduction</span>"
    ]
  },
  {
    "objectID": "glm-1.html#problems",
    "href": "glm-1.html#problems",
    "title": "\n7  GLM – Introduction\n",
    "section": "\n7.2 Problems",
    "text": "7.2 Problems\n\nlibrary(tidyverse)\n\n# Q6) c)\n# -&gt; numerically maximize log likelihood function\n\n# load data\ndata_progeny &lt;- data.frame(females = c(18,31,34,33,27,33,28,23,33,12,19,25,14,4,22,7),\n                           males = c(11,22,27,29,24,29,25,26,38,14,23,31,20,6,34,12))\n\n# define log likelihood function (with respect to theta)\n# -&gt; df = [y, n]\nloglik &lt;- function(df, theta) {\n  sum(df$y) * log(theta) + sum(df$n - df$y) * log(1 - theta)\n}\n\n# calculate values\nthetas &lt;- seq(from = 0.001, to = 0.9999, by = 0.001)\nlogliks &lt;- thetas %&gt;% map(\\(theta) loglik(data_progeny %&gt;% mutate(n = females + males) %&gt;% select(y = females, n), theta))\n\n# get results\nplot(x = thetas, y = logliks)\n\n\n\n\n\n\nthetas[which.max(logliks)] # close to theoretical result of ~0.4946\n\n[1] 0.495",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>GLM -- Introduction</span>"
    ]
  },
  {
    "objectID": "glm-2.html",
    "href": "glm-2.html",
    "title": "\n8  GLM – Model fitting\n",
    "section": "",
    "text": "8.1 Notes",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>GLM -- Model fitting</span>"
    ]
  },
  {
    "objectID": "glm-2.html#problems",
    "href": "glm-2.html#problems",
    "title": "\n8  GLM – Model fitting\n",
    "section": "\n8.2 Problems",
    "text": "8.2 Problems\n\n# load packages\nlibrary(tidyverse)\nlibrary(magrittr)\n\n# Q1) setup\n\n# load data\ndata_seeds &lt;- data.frame(treatment = c(4.81,5.36 ,4.17,3.48,4.41,4.69,3.59,4.44,5.87,4.89,3.83,4.71,6.03,5.48,4.98,4.32,4.90,5.15,5.75,6.34),\n                           control = c(4.17,4.66,3.05,5.58,5.18,3.66,4.01,4.50,6.11,3.90,4.10,4.61,5.17,5.62,3.57,4.53,5.33,6.05,5.59,5.14)) %&gt;% \n  pivot_longer(cols = everything(), names_to = \"group\", values_to = \"weight\") %&gt;% \n  arrange(group)\n\n# a) EDA\n\n# summary statistics and plots\ndata_seeds %&gt;% \n  summarize(.by = group,\n             n = n(),\n             mean = mean(weight),\n             sd = sd(weight))\n\n# A tibble: 2 × 4\n  group         n  mean    sd\n  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 control      20  4.73 0.864\n2 treatment    20  4.86 0.791\n\ndata_seeds %$% boxplot(weight ~ group, horizontal = TRUE)\n\n\n\n\n\n\nggplot(data = data_seeds,\n       aes(x = weight,\n           color = group)) + \n  geom_density()\n\n\n\n\n\n\n# b) unpaird t-test\nt.test(x = data_seeds %&gt;% filter(group == \"treatment\") %&gt;% pull(weight),\n       y = data_seeds %&gt;% filter(group == \"control\") %&gt;% pull(weight),\n       conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  data_seeds %&gt;% filter(group == \"treatment\") %&gt;% pull(weight) and data_seeds %&gt;% filter(group == \"control\") %&gt;% pull(weight)\nt = 0.50985, df = 37.711, p-value = 0.6131\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.3967069  0.6637069\nsample estimates:\nmean of x mean of y \n   4.8600    4.7265 \n\n# c) SSE estimates\nS_0 &lt;- sum((data_seeds$weight - mean(data_seeds$weight))^2)\ny_bar1 &lt;- mean(data_seeds[which(data_seeds$group == \"treatment\"),]$weight)\ny_bar2 &lt;- mean(data_seeds[which(data_seeds$group == \"control\"),]$weight)\nS_1 &lt;- sum((data_seeds$weight - c(rep(y_bar2, 20), rep(y_bar1, 20)))^2) # order based on how sorted\n\n# g) F-test\nmod_full &lt;- lm(weight ~ group, data = data_seeds)\nmod_reduced &lt;- lm(weight ~ 1, data = data_seeds) \nanova(mod_full, mod_reduced)\n\nAnalysis of Variance Table\n\nModel 1: weight ~ group\nModel 2: weight ~ 1\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     38 26.053                           \n2     39 26.232 -1  -0.17822 0.2599 0.6131\n\n# -&gt; manual\nF_star &lt;- (S_0 - S_1) / (S_1 / 38)\n\n# i) residual analysis \nmod_reduced %&gt;% plot(which = 2)\n\n\n\n\n\n\nmod_reduced %&gt;% residuals %&gt;% hist(main = \"hist of residuals\")\n\n\n\n\n\n\n\n\n# Q2 setup\n\n# load data\ndata_weights &lt;- data.frame(id = 1:20,\n                           before = c(100.8,102.0,105.9,108.0,92.0,116.7,110.2,135.0,123.5,95.0,105.0,85.0,107.2,80.0,115.1,103.5,82.0,101.5,103.5,93.0),\n                           after = c(97.0,107.5,97.0,108.0,84.0,111.5,102.5,127.5,118.5,94.2,105.0,82.4,98.2,83.6,115.0,103.0,80.0,101.5,102.6,93.0))\n\n# summary stats\ndata_weights %&gt;% \n  summarize(across(c(before, after), list(mean = mean, sd = sd)))\n\n  before_mean before_sd after_mean after_sd\n1     103.245   13.5173      100.6 12.47452\n\n# a) unpaired t-test\nt.test(x = data_weights$before,\n       y = data_weights$after,\n       conf.level = 0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  data_weights$before and data_weights$after\nt = 0.64309, df = 37.758, p-value = 0.5241\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.683035 10.973035\nsample estimates:\nmean of x mean of y \n  103.245   100.600 \n\n# a) paired t-test\n\n# calculate difference\ndata_weights %&lt;&gt;% \n  mutate(diff = before - after)\ndata_weights %&gt;% \n  summarize(mean(diff),\n            sd(diff))\n\n  mean(diff) sd(diff)\n1      2.645 4.116651\n\nt.test(x = data_weights$before,\n       y = data_weights$after,\n       paired = TRUE,\n       conf.level = 0.95)\n\n\n    Paired t-test\n\ndata:  data_weights$before and data_weights$after\nt = 2.8734, df = 19, p-value = 0.00973\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n 0.718348 4.571652\nsample estimates:\nmean difference \n          2.645 \n\n# or equivalently\nt.test(x = data_weights$diff,\n       conf.level = 0.95)\n\n\n    One Sample t-test\n\ndata:  data_weights$diff\nt = 2.8734, df = 19, p-value = 0.00973\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.718348 4.571652\nsample estimates:\nmean of x \n    2.645",
    "crumbs": [
      "Extended Linear Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>GLM -- Model fitting</span>"
    ]
  }
]