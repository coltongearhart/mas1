<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.545">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>MAS I - 3&nbsp; Statistical learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./islr-3.html" rel="next">
<link href="./partC-glm.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./partC-glm.html">Extended Linear Models</a></li><li class="breadcrumb-item"><a href="./islr-2.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Statistical learning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><header id="title-block-header" class="quarto-title-block default page-columns page-full"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./partC-glm.html">Extended Linear Models</a></li><li class="breadcrumb-item"><a href="./islr-2.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Statistical learning</span></a></li></ol></nav><div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">
<span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Statistical learning</span>
</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MAS I</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./partA-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./placeholder.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Placeholder</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./partB-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./placeholder.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Placeholder</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./partC-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Extended Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./islr-2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./islr-3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear regression</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#notes" id="toc-notes" class="nav-link active" data-scroll-target="#notes"><span class="header-section-number">3.1</span> Notes</a>
  <ul>
<li>
<a href="#what-is-statistical-learning" id="toc-what-is-statistical-learning" class="nav-link" data-scroll-target="#what-is-statistical-learning"><span class="header-section-number">3.1.1</span> What is statistical learning?</a>
  <ul class="collapse">
<li><a href="#why-estimate-f" id="toc-why-estimate-f" class="nav-link" data-scroll-target="#why-estimate-f">Why estimate <span class="math inline">\(f\)</span>?</a></li>
  <li><a href="#how-do-we-estimate-f" id="toc-how-do-we-estimate-f" class="nav-link" data-scroll-target="#how-do-we-estimate-f">How do we estimate <span class="math inline">\(f\)</span>?</a></li>
  <li><a href="#the-trade-off-between-prediction-accuracy-and-model-interpretability" id="toc-the-trade-off-between-prediction-accuracy-and-model-interpretability" class="nav-link" data-scroll-target="#the-trade-off-between-prediction-accuracy-and-model-interpretability">The trade-off between prediction accuracy and model interpretability</a></li>
  <li><a href="#supervised-vs-unsupervised-learning" id="toc-supervised-vs-unsupervised-learning" class="nav-link" data-scroll-target="#supervised-vs-unsupervised-learning">Supervised vs unsupervised learning</a></li>
  <li><a href="#regression-vs-classification-problems" id="toc-regression-vs-classification-problems" class="nav-link" data-scroll-target="#regression-vs-classification-problems">Regression vs classification problems</a></li>
  </ul>
</li>
  <li>
<a href="#assessing-model-accuracy" id="toc-assessing-model-accuracy" class="nav-link" data-scroll-target="#assessing-model-accuracy"><span class="header-section-number">3.1.2</span> Assessing model accuracy</a>
  <ul class="collapse">
<li><a href="#measuring-the-quality-of-fit" id="toc-measuring-the-quality-of-fit" class="nav-link" data-scroll-target="#measuring-the-quality-of-fit">Measuring the quality of fit</a></li>
  <li><a href="#the-bias-variance-trade-off" id="toc-the-bias-variance-trade-off" class="nav-link" data-scroll-target="#the-bias-variance-trade-off">The bias-variance trade-off</a></li>
  <li><a href="#the-classification-setting" id="toc-the-classification-setting" class="nav-link" data-scroll-target="#the-classification-setting">The classification setting</a></li>
  </ul>
</li>
  </ul>
</li>
  <li><a href="#lab" id="toc-lab" class="nav-link" data-scroll-target="#lab"><span class="header-section-number">3.2</span> Lab</a></li>
  <li>
<a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">3.3</span> Exercises</a>
  <ul>
<li>
<a href="#conceptual" id="toc-conceptual" class="nav-link" data-scroll-target="#conceptual"><span class="header-section-number">3.3.1</span> Conceptual</a>
  <ul class="collapse">
<li><a href="#question-1" id="toc-question-1" class="nav-link" data-scroll-target="#question-1">Question 1</a></li>
  <li><a href="#question-2" id="toc-question-2" class="nav-link" data-scroll-target="#question-2">Question 2</a></li>
  <li><a href="#question-3" id="toc-question-3" class="nav-link" data-scroll-target="#question-3">Question 3</a></li>
  <li><a href="#question-4" id="toc-question-4" class="nav-link" data-scroll-target="#question-4">Question 4</a></li>
  <li><a href="#question-5" id="toc-question-5" class="nav-link" data-scroll-target="#question-5">Question 5</a></li>
  <li><a href="#question-6" id="toc-question-6" class="nav-link" data-scroll-target="#question-6">Question 6</a></li>
  <li><a href="#question-7" id="toc-question-7" class="nav-link" data-scroll-target="#question-7">Question 7</a></li>
  </ul>
</li>
  <li>
<a href="#applied" id="toc-applied" class="nav-link" data-scroll-target="#applied"><span class="header-section-number">3.3.2</span> Applied</a>
  <ul class="collapse">
<li><a href="#question-8" id="toc-question-8" class="nav-link" data-scroll-target="#question-8">Question 8</a></li>
  <li><a href="#question-9" id="toc-question-9" class="nav-link" data-scroll-target="#question-9">Question 9</a></li>
  <li><a href="#question-10" id="toc-question-10" class="nav-link" data-scroll-target="#question-10">Question 10</a></li>
  </ul>
</li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content"><section id="notes" class="level2" data-number="3.1"><h2 data-number="3.1" class="anchored" data-anchor-id="notes">
<span class="header-section-number">3.1</span> Notes</h2>
<section id="what-is-statistical-learning" class="level3" data-number="3.1.1"><h3 data-number="3.1.1" class="anchored" data-anchor-id="what-is-statistical-learning">
<span class="header-section-number">3.1.1</span> What is statistical learning?</h3>
<p>Suppose that we observe a quantitative response <span class="math inline">\(Y\)</span> and <span class="math inline">\(p\)</span> different predictors, <span class="math inline">\(X_1, X_2, \ldots , X_p\)</span>. We assume that there is some relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X = (X_1, X_2, \ldots, Xp)\)</span>, which can be written in the very general form</p>
<p><span class="math display">\[
Y = f(X) + \epsilon
\]</span></p>
<p>Here <span class="math inline">\(f\)</span> is some fixed but unknown function of <span class="math inline">\(X_1, \ldots , X_p\)</span>, and <span class="math inline">\(\epsilon\)</span> is a random error term, which is independent of <span class="math inline">\(X\)</span> and has mean zero. In this formulation, <span class="math inline">\(f\)</span> represents the systematic information that <span class="math inline">\(X\)</span> provides about <span class="math inline">\(Y\)</span>.</p>
<p><strong>In essence, statistical learning refers to a set of approaches for estimating <span class="math inline">\(f\)</span>.</strong></p>
<section id="why-estimate-f" class="level4"><h4 class="anchored" data-anchor-id="why-estimate-f">Why estimate <span class="math inline">\(f\)</span>?</h4>
<p>There are two main reasons that we may wish to estimate <span class="math inline">\(f\)</span>: <em>prediction</em> and <em>inference</em>.</p>
<p><strong>Prediction</strong></p>
<p>In many situations, a set of inputs <span class="math inline">\(X\)</span> are readily available, but the output <span class="math inline">\(Y\)</span> cannot be easily obtained. In this setting, since the error term averages to zero, we can predict <span class="math inline">\(Y\)</span> using</p>
<p><span class="math display">\[
\hat{Y} = \hat{f}(X),
\]</span></p>
<p>where <span class="math inline">\(\hat{f}\)</span> represents our estimate for <span class="math inline">\(f\)</span> , and <span class="math inline">\(\hat{Y}\)</span> represents the resulting prediction for <span class="math inline">\(Y\)</span>. In this setting, <span class="math inline">\(\hat{f}\)</span> is often treated as a <em>black box</em>, in the sense that one is not typically concerned with the exact form of <span class="math inline">\(\hat{f}\)</span>, provided that it yields accurate predictions for <span class="math inline">\(Y\)</span>.</p>
<p>The accuracy of <span class="math inline">\(\hat{Y}\)</span> as a prediction for <span class="math inline">\(Y\)</span> depends on two quantities, which we will call the <em>reducible error</em> and the <em>irreducible error</em>. In general, <span class="math inline">\(\hat{f}\)</span> will not be a perfect estimate for <span class="math inline">\(f\)</span>, and this inaccuracy will introduce some error. This error is <em>reducible</em> because we can potentially improve the accuracy of <span class="math inline">\(\hat{f}\)</span> by using the most appropriate statistical learning technique to estimate <span class="math inline">\(f\)</span>. However, even if it were possible to form a perfect estimate for <span class="math inline">\(f\)</span>, so that our estimated response took the form <span class="math inline">\(\hat{Y} = f(X)\)</span>, our prediction would still have some error in it! This is because <span class="math inline">\(Y\)</span> is also a function of <span class="math inline">\(\epsilon\)</span>, which, by definition, cannot be predicted using <span class="math inline">\(X\)</span>. Therefore, variability associated with <span class="math inline">\(\epsilon\)</span> also affects the accuracy of our predictions. This is known as the <em>irreducible error</em>, because no matter how well we estimate <span class="math inline">\(f\)</span>, we cannot reduce the error introduced by <span class="math inline">\(\epsilon\)</span>.</p>
<p>Why is the irreducible error larger than zero? The quantity <span class="math inline">\(\epsilon\)</span> may contain unmeasured variables that are useful in predicting <span class="math inline">\(Y\)</span>: since we don’t measure them, <span class="math inline">\(f\)</span> cannot use them for its prediction. The quantity <span class="math inline">\(\epsilon\)</span> may also contain unmeasurable variation.</p>
<p>Consider a given estimate <span class="math inline">\(\hat{f}\)</span> and a set of predictors <span class="math inline">\(X\)</span>, which yields the prediction <span class="math inline">\(\hat{Y} = \hat{f}(X)\)</span>. Assume for a moment that both <span class="math inline">\(\hat{f}\)</span> and <span class="math inline">\(X\)</span> are fixed, so that the only variability comes from <span class="math inline">\(\epsilon\)</span>. Then, we can say:</p>
<p><img src="files/images/2-expected-value.png" class="img-fluid" style="width:50.0%"></p>
<p>where <span class="math inline">\(E(Y − \hat{Y})^2\)</span> represents the average, or <em>expected value</em>, of the squared difference between the predicted and actual value of <span class="math inline">\(Y\)</span>, and <span class="math inline">\(Var(\epsilon)\)</span> represents the variance associated with the error term <span class="math inline">\(\epsilon\)</span>.</p>
<p>The focus of this book is on techniques for estimating <span class="math inline">\(f\)</span> with the aim of minimizing the reducible error. It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for <span class="math inline">\(Y\)</span>. This bound is almost always unknown in practice.</p>
<p><strong>Inference</strong></p>
<p>We are often interested in understanding the association between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1, \ldots , X_p\)</span>. In this situation we wish to estimate <span class="math inline">\(f\)</span>, but our goal is not necessarily to make predictions for <span class="math inline">\(Y\)</span>. Now <span class="math inline">\(\hat{f}\)</span> cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:</p>
<ul>
<li><p><em>Which predictors are associated with the response?</em> It is often the case that only a small fraction of the available predictors are substantially associated with <span class="math inline">\(Y\)</span>. Identifying the few important predictors among a large set of possible variables can be extremely useful, depending on the application.</p></li>
<li><p><em>What is the relationship between the response and each predictor?</em> Some predictors may have a positive relationship with <span class="math inline">\(Y\)</span>, in the sense that larger values of the predictor are associated with larger values of <span class="math inline">\(Y\)</span>. Other predictors may have the opposite relationship. Depending on the complexity of <span class="math inline">\(f\)</span>, the relationship between the response and a given predictor may also depend on the values of the other predictors.</p></li>
<li><p><em>Can the relationship between <span class="math inline">\(Y\)</span> and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?</em> Historically, most methods for estimating <span class="math inline">\(f\)</span> have taken a linear form. In some situations, such an assumption is reasonable or even desirable. But often the true relationship is more complicated, in which case a linear model may not provide an accurate representation of the relationship between the input and output variables.</p></li>
</ul></section><section id="how-do-we-estimate-f" class="level4"><h4 class="anchored" data-anchor-id="how-do-we-estimate-f">How do we estimate <span class="math inline">\(f\)</span>?</h4>
<p>Throughout this book, we explore many linear and non-linear approaches for estimating <span class="math inline">\(f\)</span>. However, these methods generally share certain characteristics. Here is an overview.</p>
<p>Note we will always assume that we have observed a set of <span class="math inline">\(n\)</span> different data points, called the <em>training data</em> because we will use these observations to train, or teach, our method how to estimate <span class="math inline">\(f\)</span>.</p>
<p>Our goal is to apply a statistical learning method to the training data in order to estimate the unknown function <span class="math inline">\(f\)</span>. In other words, we want to find a function <span class="math inline">\(\hat{f}\)</span> such that <span class="math inline">\(Y \approx \hat{f}(X)\)</span> for any observation <span class="math inline">\((X,Y)\)</span>. Broadly speaking, most statistical learning methods for this task can be characterized as either <em>parametric</em> or <em>non-parametric</em>. We now briefly discuss these two types of approaches.</p>
<p><strong>Parametric</strong></p>
<p>Parametric methods involve a two-step model-based approach.</p>
<p>Step 1</p>
<ul>
<li>First, we make an assumption about the functional form, or shape, of <span class="math inline">\(f\)</span>. For example, one very simple assumption is that <span class="math inline">\(f\)</span> is linear in <span class="math inline">\(X\)</span>:</li>
</ul>
<p><span class="math display">\[
f(X) = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p
\]</span></p>
<ul>
<li>This is a <em>linear model</em>. Once we have assumed that <span class="math inline">\(f\)</span> is linear, the problem of estimating <span class="math inline">\(f\)</span> is greatly simplified. Instead of having to estimate an entirely arbitrary <span class="math inline">\(p\)</span>-dimensional function <span class="math inline">\(f(X)\)</span>, one only needs to estimate the p+1 coefficients <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span>.</li>
</ul>
<p>Step 2</p>
<ul>
<li>After a model has been selected, we need a procedure that uses the training data to fit or train the model. In the case of the linear model, we need to estimate the parameters <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span>.That is,we want to find values of these parameters such that</li>
</ul>
<p><span class="math display">\[
Y \approx \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p
\]</span></p>
<ul>
<li>The most common approach to fitting the model above is referred to as <em>(ordinary) least squares</em>. However, least squares is one of many possible ways to fit the linear model.</li>
</ul>
<p>The model-based approach just described is referred to as <em>parametric</em>; <strong>it reduces the problem of estimating <span class="math inline">\(f\)</span> down to one of estimating a set of parameters</strong>. Assuming a parametric form for <span class="math inline">\(f\)</span> simplifies the problem of estimating <span class="math inline">\(f\)</span> because it is generally much easier to estimate a set of parameters, such as <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span> in the linear model, than it is to fit an entirely arbitrary function <span class="math inline">\(f\)</span>. The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of <span class="math inline">\(f\)</span>. If the chosen model is too far from the true <span class="math inline">\(f\)</span>, then our estimate will be poor. We can try to address this problem by choosing flexible models that can fit many different possible functional forms for <span class="math inline">\(f\)</span>. But in general, fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as <em>overfitting</em> the data, which essentially means they follow the errors, or noise, too closely.</p>
<p><strong>Non-parametric</strong></p>
<p><strong>Non-parametric methods do not make explicit assumptions about the functional form of <span class="math inline">\(f\)</span></strong>. Instead they seek an estimate of <span class="math inline">\(f\)</span> that gets as close to the data points as possible without being too rough or wiggly. Such approaches can have a major advantage over parametric approaches: by avoiding the assumption of a particular functional form for <span class="math inline">\(f\)</span>, they have the potential to accurately fit a wider range of possible shapes for <span class="math inline">\(f\)</span>.</p>
<p>Any parametric approach brings with it the possibility that the functional form used to estimate <span class="math inline">\(f\)</span> is very different from the true <span class="math inline">\(f\)</span>, in which case the resulting model will not fit the data well. In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of <span class="math inline">\(f\)</span> is made. But non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating <span class="math inline">\(f\)</span> to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for <span class="math inline">\(f\)</span>.</p>
<p>Be careful of overfitting though, non-parametric methods can fit the data perfectly if complex enough, which causes the fit obtained to not yield accurate estimates of the response on new observations that were not part of the original training data set.</p>
<p><img src="files/images/2-perfect-fit.png" class="img-fluid" style="width:50.0%"></p>
</section><section id="the-trade-off-between-prediction-accuracy-and-model-interpretability" class="level4"><h4 class="anchored" data-anchor-id="the-trade-off-between-prediction-accuracy-and-model-interpretability">The trade-off between prediction accuracy and model interpretability</h4>
<p>Of the many methods that we examine in this book, some are less flexible, or more restrictive, in the sense that they can produce just a relatively small range of shapes to estimate <span class="math inline">\(f\)</span>. For example, linear regression is a relatively inflexible approach, because it can only generate linear functions (smooth lines / planes). Other methods are considerably more flexible because they can generate a much wider range of possible shapes to estimate <span class="math inline">\(f\)</span>.</p>
<p>One might reasonably ask the following question: <em>why would we ever choose to use a more restrictive method instead of a very flexible approach?</em> There are several reasons that we might prefer a more restrictive model. If we are mainly interested in inference, then restrictive models are much more interpretable. For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X_1, X_2, \ldots, X_p\)</span>. In contrast, very flexible approaches, such as the splines and boosting methods can lead to such complicated estimates of <span class="math inline">\(f\)</span> that it is difficult to understand how any individual predictor is associated with the response.</p>
<p>Here is an illustration of the trade-off between flexibility and interpretability for some of the methods that we cover in this book.</p>
<p><img src="files/images/2-flex-vs-inter.png" class="img-fluid" style="width:50.0%"></p>
<p>Least squares linear regression is relatively inflexible but is quite interpretable. The <em>lasso</em> relies upon the linear model but uses an alternative fitting procedure for estimating the coefficients <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span>. The new procedure is more restrictive in estimating the coefficients, and sets a number of them to exactly zero. Hence in this sense the lasso is a less flexible approach than linear regression. It is also more interpretable than linear regression, because in the final model the response variable will only be related to a small subset of the predictors — namely, those with nonzero coefficient estimates.</p>
<p><em>Generalized additive models</em> (GAMs), discussed in Chapter 7, instead extend the linear model to allow for certain non-linear relationships. Consequently, GAMs are more flexible than linear regression. They are also somewhat less interpretable than linear regression, because the relationship between each predictor and the response is now modeled using a curve. Finally, fully non-linear methods such as <em>bagging</em>, <em>boosting</em>, <em>support vector machines</em> with non-linear kernels, and <em>neural networks</em> (deep learning) are highly flexible approaches that are harder to interpret.</p>
<p>We have established that when inference is the goal, there are clear advantages to using simple and relatively inflexible statistical learning methods. In some settings, however, we are only interested in prediction, and the interpretability of the predictive model is simply not of interest; our sole requirement for the algorithm is that it predict accurately (i.e.&nbsp;interpretability is not a concern). In this setting, we might expect that it will be best to use the most flexible model available. Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for overfitting in highly flexible methods.</p>
</section><section id="supervised-vs-unsupervised-learning" class="level4"><h4 class="anchored" data-anchor-id="supervised-vs-unsupervised-learning">Supervised vs unsupervised learning</h4>
<p>Most statistical learning problems fall in to one of two categories: <em>supervised</em> or <em>unsupervised</em>. The examples that we have discussed so far in this chapter all fall into the supervised learning domain. For each observation of the predictor measurement(s) <span class="math inline">\(x_i, i = 1, \ldots , n\)</span> there is an associated response measurement <span class="math inline">\(y_i\)</span>. We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference). Many classical statistical learning methods such as linear regression and logistic regression, as well as more modern approaches such as GAM, boosting, and support vector machines, operate in the supervised learning domain. The vast majority of this book is devoted to this setting.</p>
<p>By contrast, unsupervised learning describes the somewhat more challenging situation in which for every observation <span class="math inline">\(i = 1, \ldots, n\)</span>, we observe a vector of measurements <span class="math inline">\(x_i\)</span> but no associated response <span class="math inline">\(y_i\)</span>. It is not possible to fit a linear regression model, since there is no response variable to predict. In this setting, we are in some sense working blind; the situation is referred to as <em>unsupervised</em> because we lack a response variable that can supervise our analysis. What sort of statistical analysis is possible? We can seek to understand the relationships between the variables or between the observations. One statistical learning tool that we may use in this setting is <em>cluster analysis</em>, or clustering. The goal of cluster analysis is to ascertain, on the basis of <span class="math inline">\(x_1, \ldots, x_n\)</span>, whether the observations fall into relatively distinct groups.</p>
<p><img src="files/images/2-clustering.png" class="img-fluid" style="width:50.0%"></p>
<p>However, in practice the group memberships are unknown, and the goal is to determine the group to which each observation belongs. A clustering method could not be expected to assign all of the overlapping points to their correct group.</p>
<p>In the example above, there are only two variables, and so one can simply visually inspect the scatterplots of the observations in order to identify clusters. However, in practice, we often encounter data sets that contain many more than two variables. In this case, we cannot easily plot the observations. For instance, if there are <span class="math inline">\(p\)</span> variables in our data set, then <span class="math inline">\(p(p − 1)/2\)</span> distinct scatterplots can be made, and visual inspection is simply not a viable way to identify clusters. For this reason, automated clustering methods are important.</p>
<p>Many problems fall naturally into the supervised or unsupervised learning paradigms. However, sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut. For instance, suppose that we have a set of <span class="math inline">\(n\)</span> observations. For <span class="math inline">\(m\)</span> of the observations, where <span class="math inline">\(m &lt; n\)</span>, we have both predictor measurements and a response measurement. For the remaining <span class="math inline">\(n − m\)</span> observations, we have predictor measurements but no response measurement. Such a scenario can arise if the predictors can be measured relatively cheaply but the corresponding responses are much more expensive to collect. We refer to this setting as a <em>semi-supervised learning</em> problem. In this setting, we wish to use a statistical learning method that can incorporate the <span class="math inline">\(m\)</span> observations for which response measurements are available as well as the <span class="math inline">\(n − m\)</span> observations for which they are not. Although this is an interesting topic, it is beyond the scope of this book.</p>
</section><section id="regression-vs-classification-problems" class="level4"><h4 class="anchored" data-anchor-id="regression-vs-classification-problems">Regression vs classification problems</h4>
<p>We tend to refer to problems with a quantitative response as <em>regression</em> problems, while those involving a qualitative response are often referred to as <em>classification</em> problems. However, the distinction is not always that crisp. Least squares linear regression is used with a quantitative response, whereas logistic regression is typically used with a qualitative (two-class, or <em>binary</em>) response. Thus, despite its name, logistic regression is a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as <span class="math inline">\(K\)</span>-nearest neighbors and boosting, can be used in the case of either quantitative or qualitative responses.</p>
<p>We tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative; i.e.&nbsp;we might use linear regression when quantitative and logistic regression when qualitative. However, whether the <em>predictors</em> are qualitative or quantitative is generally considered less important. Most of the statistical learning methods discussed in this book can be applied regardless of the predictor variable type, provided that any qualitative predictors are properly <em>coded</em> before the analysis is performed.</p>
</section></section><section id="assessing-model-accuracy" class="level3" data-number="3.1.2"><h3 data-number="3.1.2" class="anchored" data-anchor-id="assessing-model-accuracy">
<span class="header-section-number">3.1.2</span> Assessing model accuracy</h3>
<p>One of the key aims of this book is to introduce the reader to a wide range of statistical learning methods that extend far beyond the standard linear regression approach. Why is it necessary to introduce so many different statistical learning approaches, rather than just a single <em>best</em> method? <em>There is no free lunch in statistics:</em> <strong>no one method dominates all others over all possible data sets</strong>. On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. Hence it is an important task to decide for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.</p>
<p>Now, we discuss some of the most important concepts that arise in selecting a statistical learning procedure for a specific data set.</p>
<section id="measuring-the-quality-of-fit" class="level4"><h4 class="anchored" data-anchor-id="measuring-the-quality-of-fit">Measuring the quality of fit</h4>
<p>In order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the <em>mean squared error</em> (MSE), given by</p>
<p><span class="math display">\[
MSE = \frac{1}{n} \sum_{i = 1}^n (y_i - \hat{f}(x_i))^2
\]</span> The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially.</p>
<p>This MSE is computed using the training data that was used to fit the model, and so should more accurately be referred to as the <em>training MSE</em>. But in general, we do not really care how well the method works training on the training data. Rather, <em>we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data</em>. To state it more mathematically, suppose that we fit our statistical learn- ing method on our training observations <span class="math inline">\(\{(x_1,y_1), (x_2,y_2), \ldots, (x_n,y_n)\}\)</span>, and we obtain the estimate <span class="math inline">\(\hat{f}\)</span>. We can then compute <span class="math inline">\(\hat{f}(x_1), \hat{f}(x_2), \ldots , \hat{f}(x_n)\)</span>. If these are approximately equal to <span class="math inline">\(y_1, y_2, \ldots, y_n\)</span>, then the training MSE is small. However, we are really not interested in whether <span class="math inline">\(\hat{f}(x_i) \approx y_i\)</span>; instead, we want to know whether <span class="math inline">\(\hat{f}(x_0)\)</span> is approximately equal to <span class="math inline">\(y_0\)</span>, where <span class="math inline">\((x_0,y_0)\)</span> is a <em>previously unseen test observation not used to train the statistical learning method</em>. We want to choose the method that gives the lowest <em>test MSE</em>, as opposed to the lowest training MSE. In other words, if we had a large number of test observations, we could compute the average squared prediction error for these test observations <span class="math inline">\((x_0,y_0)\)</span>:</p>
<p><span class="math display">\[
\text{Ave}(y_0 - \hat{f}(x_0))^2
\]</span></p>
<p>We’d like to select the model for which this quantity is as small as possible.</p>
<p>How can we go about trying to select a method that minimizes the test MSE? In some settings, we may have a test data set available. We can then simply evaluate the above formula on the test observations, and select the learning method for which the test MSE is smallest. But what if no test observations are available? In that case, one might imagine simply selecting a statistical learning method that minimizes the training MSE. This seems like it might be a sensible approach, since the training MSE and the test MSE appear to be closely related. Unfortunately, there is a fundamental problem with this strategy: there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but the test MSE is often much larger.</p>
<p>The figure below illustrates this concept. In the left-hand panel, we have generated observations from some <span class="math inline">\(f(X)\)</span> with the true <span class="math inline">\(f\)</span> given by the black curve. The orange, blue and green curves illustrate three possible estimates for <span class="math inline">\(f\)</span> obtained using methods with increasing levels of flexibility.</p>
<p><img src="files/images/2-training-vs-test-mse.png" class="img-fluid" style="width:50.0%"></p>
<p>The blue and green curves were produced using <em>smoothing splines</em>. It is clear that as the level of flexibility increases, the curves fit the observed data more closely. The green curve is the most flexible and matches the data very well; however, we observe that it fits the true <span class="math inline">\(f\)</span> (shown in black) poorly because it is too wiggly.</p>
<p>On the right panel, the grey curve displays the average training MSE as a function of flexibility, or more formally the <em>degrees of freedom</em>, for a number of smoothing splines. The degrees of freedom is a quantity that summarizes the flexibility of a curve. A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve (i.e.&nbsp;more error dfs; a more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve). The training MSE declines monotonically as flexibility increases. In this example the true <span class="math inline">\(f\)</span> is non-linear, and so the orange linear fit is not flexible enough to estimate f well. The green curve has the lowest training MSE of all three methods, since it corresponds to the most flexible of the three curves fit in the left-hand panel.</p>
<p>In this example, we know the true function <span class="math inline">\(f\)</span>, and so we can also compute the test MSE over a very large test set, as a function of flexibility. (Of course, in general <span class="math inline">\(f\)</span> is unknown, so this will not be possible. The horizontal dashed line indicates <span class="math inline">\(\text{Var}(\epsilon)\)</span>, the irreducible error, which corresponds to the lowest achievable test MSE among all possible methods. <strong>As the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a <em>U-shape</em> in the test MSE (red line)</strong>. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be <em>overfitting</em> the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function <span class="math inline">\(f\)</span>. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data. <strong>Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.</strong></p>
<p>Here is another example that displays the same patterns. However, because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the orange least squares fit is substantially better than the highly flexible green curve.</p>
<p><img src="files/images/2-training-vs-test-mse-2.png" class="img-fluid" style="width:50.0%"></p>
<p>Finally, here is another example in which <span class="math inline">\(f\)</span> is highly non-linear. Now there is a rapid decrease in both curves before the test MSE starts to increase slowly.</p>
<p><img src="files/images/2-training-vs-test-mse-3.png" class="img-fluid" style="width:50.0%"></p>
<p>In practice, one can usually compute the training MSE with relative ease, but estimating test MSE is considerably more difficult because usually no test data are available. As the previous three examples illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably among data sets. Throughout this book, we discuss a variety of approaches that can be used in practice to estimate this minimum point. One important method is <em>cross-validation</em>, which is a method for estimating test MSE using the training data.</p>
</section><section id="the-bias-variance-trade-off" class="level4"><h4 class="anchored" data-anchor-id="the-bias-variance-trade-off">The bias-variance trade-off</h4>
<p>The U-shape observed in the test MSE curves turns out to be the result of two competing properties of statistical learning methods. It is possible to show that the expected test MSE, for a given value <span class="math inline">\(x_0\)</span>, can always be decomposed into the sum of three fundamental quantities: the <em>variance</em> of <span class="math inline">\(\hat{f}(x_0)\)</span>, the squared <em>bias</em> of <span class="math inline">\(f(x_0)\)</span> and the variance of the error terms <span class="math inline">\(\epsilon\)</span>. That is,</p>
<p><span class="math display">\[
E\big(y_o - \hat{f}(x_0)\big)^2 = \text{Var}(\hat{f}(x_0)) + [\text{Bias}(\hat{f}(x_0))]^2 + \text{Var}(\epsilon)
\]</span></p>
<!-- ??? proof of this -->
<p>Here, the notation <span class="math inline">\(E\big(y_o - \hat{f}(x_0)\big)^2\)</span> defines the <em>expected test MSE</em> at <span class="math inline">\(x_0\)</span>, and refers to the average test MSE that we would obtain if we repeatedly estimated <span class="math inline">\(f\)</span> using a large number of training sets, and tested each at <span class="math inline">\(x_0\)</span>. The overall expected test MSE can be computed by averaging <span class="math inline">\(E\big(y_o - \hat{f}(x_0)\big)^2\)</span> over all possible values of <span class="math inline">\(x_0\)</span> in the test set.</p>
<p>This equation tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves <em>low variance</em> and <em>low bias</em>. Note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative. Hence, we see that the expected test MSE can never lie below <span class="math inline">\(\text{Var}(\epsilon)\)</span>, the irreducible error.</p>
<p>What do we mean by the <em>variance</em> and <em>bias</em> of a statistical learning method? <em>Variance</em> refers to the amount by which <span class="math inline">\(\hat{f}\)</span> would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different <span class="math inline">\(\hat{f}\)</span>. But ideally the estimate for f should not vary too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in <span class="math inline">\(\hat{f}\)</span>. <strong>In general, more flexible statistical methods have higher variance</strong>.</p>
<p>On the other hand, <em>bias</em> refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. <strong>Generally, more flexible methods result in less bias.</strong></p>
<p><strong>As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease.</strong> The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases. For the previous examples, here are the variance, bias curves:</p>
<p><img src="files/images/2-variance-bias.png" class="img-fluid" style="width:50.0%"></p>
<p>This shows that variance is an increasing function of flexibility, and bias is a decreasing function. The goal is to find the balance point. This trade-off is one of the most important recurring themes in this book.</p>
<p><strong>In a real-life situation in which <span class="math inline">\(f\)</span> is unobserved, it is generally not possible to explicitly compute the test MSE, bias, or variance for a statistical learning method. Nevertheless, one should always keep the bias-variance trade-off in mind.</strong> In this book we explore methods that are extremely flexible and hence can essentially eliminate bias. However, this does not guarantee that they will outperform a much simpler method such as linear regression. To take an extreme example, suppose that the true <span class="math inline">\(f\)</span> is linear. In this situation linear regression will have no bias, making it very hard for a more flexible method to compete. In contrast, if the true <span class="math inline">\(f\)</span> is highly non-linear and we have an ample number of training observations, then we may do better using a highly flexible approach. Later, we discuss cross-validation, which is a way to estimate the test MSE using the training data.</p>
</section><section id="the-classification-setting" class="level4"><h4 class="anchored" data-anchor-id="the-classification-setting">The classification setting</h4>
<p>Thus far, our discussion of model accuracy has been focused on the regression setting. But many of the concepts that we have encountered, such as the bias-variance trade-off, transfer over to the classification setting with only some modifications due to the fact that <span class="math inline">\(y_i\)</span> is no longer quantitative. Suppose that we seek to estimate <span class="math inline">\(f\)</span> on the basis of training observations <span class="math inline">\(\{(x_1,y_1), \ldots, (x_n,y_n)\}\)</span>, where now <span class="math inline">\(y_1, \ldots , y_n\)</span> are qualitative. The most common approach for quantifying the accuracy of our estimate <span class="math inline">\(\hat{f}\)</span> is the <em>training error rate</em>, the proportion of mistakes that are made if we apply our estimate <span class="math inline">\(\hat{f}\)</span> to the training observations:</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i = 1}^n I(y_i \ne \hat{y}_i)
\]</span></p>
<p>Here <span class="math inline">\(\hat{y}_i\)</span> is the predicted class label for the <span class="math inline">\(i\)</span>th observation using <span class="math inline">\(\hat{f}\)</span>. And <span class="math inline">\(I(y_i \ne \hat{y}_i))\)</span> is an indicator variable that equals 1 if <span class="math inline">\(y_i \ne \hat{y}_i\)</span> and zero if <span class="math inline">\(y_i = \hat{y}_i\)</span> (i.e.&nbsp;indicator if wrong). Thus, the above equation computes the fraction of incorrect classifications.</p>
<p>The <em>test error rate</em> associated with a set of test observations of the form <span class="math inline">\((x_0, y_0)\)</span> is given by</p>
<p><span class="math display">\[
\text{Ave}(I(y_0 \ne \hat{y}_0))
\]</span></p>
<p>A good classifier is one for which the test error is smallest.</p>
<p><strong>The Bayes classifier</strong></p>
<p>It is possible to show (though the proof is outside of the scope of this book) that the test error rate given in is minimized, on average, by a very simple classifier that <em>assigns each observation to the most likely class, given its predictor values</em>. In other words, we should simply assign a test observation with predictor vector <span class="math inline">\(x_0\)</span> to the class <span class="math inline">\(j\)</span> for which</p>
<p><span class="math display">\[
\text{Pr}(Y = j \mid X = x_0)
\]</span></p>
<p>is the largest. Note that the above formula is a <em>conditional probability</em>: it is the probability that <span class="math inline">\(Y = j\)</span>, given the observed predictor vector <span class="math inline">\(x_0\)</span>. This very simple classifier is called the <em>Bayes classifier</em>. In a two-class problem where there are only two possible response values, say class 1 or class 2, the Bayes classifier corresponds to predicting class one if <span class="math inline">\(\text{Pr}(Y = j \mid X = x_0) &gt; 0.5\)</span>, and class two otherwise.</p>
<p>Here is an example:</p>
<p><img src="files/images/2-bayes-classifier.png" class="img-fluid" style="width:50.0%"></p>
<p>For each value of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, there is a different probability of the response being orange or blue. Since this is simulated data, we know how the data were generated and we can calculate the conditional probabilities for each value of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. The purple dashed line represents the points where the probability is exactly 50%. This is called the <em>Bayes decision boundary</em>.</p>
<p>The Bayes classifier produces the lowest possible test error rate, called the <em>Bayes error rate</em>. Since the Bayes classifier will always choose the class for which the above formula is largest, the error rate will be <span class="math inline">\(1 − \text{max}_j \, \text{Pr}(Y = j \mid X = x_0)\)</span> at <span class="math inline">\(X = x_0\)</span>. In general, the overall Bayes error rate is given by</p>
<p><span class="math display">\[
1 - E\big(\underset{j}{\text{max}} \, \text{Pr}(Y = j \mid X)\big)
\]</span></p>
<p>where the expectation averages the probability over all possible values of <span class="math inline">\(X\)</span>. <strong>The Bayes error rate is analogous to the irreducible error, discussed earlier.</strong></p>
<!-- ??? how does this work -->
<p><strong><span class="math inline">\(K\)</span>-Nearest neighbors</strong></p>
<p>In theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods.</p>
<!-- ??? why impossible -->
<p>Many approaches attempt to estimate the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, and then classify a given observation to the class with highest <em>estimated</em> probability. One such method is the <em>K-nearest neighbors</em> (KNN) classifier. Given a positive integer <span class="math inline">\(K\)</span> and a test observation <span class="math inline">\(x_0\)</span>, the KNN classifier first identifies the <span class="math inline">\(K\)</span> points in the training data that are closest to <span class="math inline">\(x_0\)</span>, represented by <span class="math inline">\(\cal{N}_0\)</span>. It then estimates the conditional probability for class <span class="math inline">\(j\)</span> as the fraction of points in <span class="math inline">\(\cal{N}_0\)</span> whose response values equal <span class="math inline">\(j\)</span>:</p>
<p><span class="math display">\[
\text{Pr}(Y = j \mid X = x_0) = \frac{1}{K}\sum_{i \in \cal{N}_0} I(y_i = j)
\]</span></p>
<p>Finally, KNN classifies the test observation <span class="math inline">\(x_0\)</span> to the class with the largest probability from the equation above.</p>
<p>Here is an example of the KNN approach:</p>
<p><img src="files/images/2-knn.png" class="img-fluid" style="width:50.0%"></p>
<p>Despite the fact that it is a very simple approach, KNN can often produce classifiers that are surprisingly close to the optimal Bayes classifier. Note that even though the true distribution is not known by the KNN classifier, the KNN decision boundary is very close to that of the Bayes classifier.</p>
<p><img src="files/images/2-knn-2.png" class="img-fluid" style="width:50.0%"></p>
<p>However, the choice of <span class="math inline">\(K\)</span> has a drastic effect on the KNN classifier obtained.</p>
<p><img src="files/images/2-knn-3.png" class="img-fluid" style="width:50.0%"></p>
<p>When <span class="math inline">\(K = 1\)</span>, the decision boundary is overly flexible and finds patterns in the data that don’t correspond to the Bayes decision boundary. This corresponds to a classifier that has low bias but very high variance. As <span class="math inline">\(K\)</span> grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier.</p>
<p>Just as in the regression setting, there is not a strong relationship between the training error rate and the test error rate. With <span class="math inline">\(K = 1\)</span>, the KNN training error rate is 0, but the test error rate may be quite high. <strong>In general, as we use more flexible classification methods, the training error rate will decline but the test error rate may not.</strong></p>
<p><img src="files/images/2-knn-results.png" class="img-fluid" style="width:50.0%"></p>
<p>As <span class="math inline">\(1/K\)</span> increases, the method becomes more flexible. As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately <span class="math inline">\(K = 10\)</span>) before increasing again when the method becomes excessively flexible and overfits.</p>
<p><strong>In both the regression and classification settings, choosing the correct level of flexibility is critical to the success of any statistical learning method. The bias-variance trade-off, and the resulting U-shape in the test error, can make this a difficult task.</strong></p>
</section></section></section><section id="lab" class="level2" data-number="3.2"><h2 data-number="3.2" class="anchored" data-anchor-id="lab">
<span class="header-section-number">3.2</span> Lab</h2>
<p>&lt; just basic R commands &gt;</p>
</section><section id="exercises" class="level2" data-number="3.3"><h2 data-number="3.3" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">3.3</span> Exercises</h2>
<section id="conceptual" class="level3" data-number="3.3.1"><h3 data-number="3.3.1" class="anchored" data-anchor-id="conceptual">
<span class="header-section-number">3.3.1</span> Conceptual</h3>
<section id="question-1" class="level4"><h4 class="anchored" data-anchor-id="question-1">Question 1</h4>
<blockquote class="blockquote">
<p>For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.</p>
<ol type="a">
<li>The sample size <span class="math inline">\(n\)</span> is extremely large, and the number of predictors <span class="math inline">\(p\)</span> is small.</li>
</ol>
</blockquote>
<p>A flexible model will perform better than an inflexible model because there are ample degrees of freedom to estimate many parameters and still have many leftover for error df.</p>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>The number of predictors <span class="math inline">\(p\)</span> is extremely large, and the number of observations <span class="math inline">\(n\)</span> is small.</li>
</ol>
</blockquote>
<p>An inflexible will be better because there is a high chance of some predictors being randomly associated.</p>
<blockquote class="blockquote">
<ol start="3" type="a">
<li>The relationship between the predictors and response is highly non-linear.</li>
</ol>
</blockquote>
<p>A flexible model will perform better because it can pick up on the non-linear trends better.</p>
<blockquote class="blockquote">
<ol start="4" type="a">
<li>The variance of the error terms, i.e.&nbsp;<span class="math inline">\(\sigma^2 = \text{Var}(\epsilon)\)</span>, is extremely high.</li>
</ol>
</blockquote>
<p>Inflexible model will be better because the flexible model will pick up on the noise a lot better even though it is not related to the relationship to the <span class="math inline">\(X\)</span>s.</p>
</section><section id="question-2" class="level4"><h4 class="anchored" data-anchor-id="question-2">Question 2</h4>
<blockquote class="blockquote">
<p>Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>.</p>
<ol type="a">
<li>We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.</li>
</ol>
</blockquote>
<p>Regression; inference; <span class="math inline">\(n = 500, p = 3\)</span></p>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.</li>
</ol>
</blockquote>
<p>Classification; prediction; <span class="math inline">\(n = 20, p = 13\)</span></p>
<blockquote class="blockquote">
<ol start="3" type="a">
<li>We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.</li>
</ol>
</blockquote>
<p>Regression; prediction; <span class="math inline">\(n = 52, p = 3\)</span>.</p>
</section><section id="question-3" class="level4"><h4 class="anchored" data-anchor-id="question-3">Question 3</h4>
<blockquote class="blockquote">
<p>We now revisit the bias-variance decomposition.</p>
<ol type="a">
<li>Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.</li>
</ol>
</blockquote>
<p>&lt; !!! get from ipad &gt;</p>
<p><img src="files/images/2-ex-3-sol.png" class="img-fluid" style="width:50.0%"></p>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>Explain why each of the five curves has the shape displayed in part (a).</li>
</ol>
</blockquote>
<ul>
<li><p>Bias: Decreases until leveling off as flexibility increases (nears zero)</p></li>
<li><p>Variance: Increases faster as flexibility increases</p></li>
<li><p>Bayes (irreducible error): Constant, unrelated to flexibility (<span class="math inline">\(X\)</span>)</p></li>
<li><p>Training error rate: Decrease monotonically as flexibility increases</p></li>
<li><p>Testing error rate: U-shaped with minimum where bias and variance are simultaneously minimized</p></li>
</ul></section><section id="question-4" class="level4"><h4 class="anchored" data-anchor-id="question-4">Question 4</h4>
<blockquote class="blockquote">
<p>You will now think of some real-life applications for statistical learning.</p>
<ol type="a">
<li>Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.</li>
</ol>
</blockquote>
<ul>
<li><p>Insurance, predict whether a policyholder will have a claim or not. Response is binary, predictors are variables such as credit score, age of business, type of business, etc. Goal is prediction, want to see if a new customer is likely to have a claim.</p></li>
<li><p>&lt; good enough &gt;</p></li>
</ul>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.</li>
</ol>
</blockquote>
<p>Insurance, predict the amount of a claim. Response is claim size in dollars, same predictors as in a.</p>
<blockquote class="blockquote">
<ol start="3" type="a">
<li>Describe three real-life applications in which cluster analysis might be useful.</li>
</ol>
</blockquote>
<p>Insurance, determining if a business is low, medium or high risk. Same predictors as a.</p>
</section><section id="question-5" class="level4"><h4 class="anchored" data-anchor-id="question-5">Question 5</h4>
<blockquote class="blockquote">
<p>What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?</p>
</blockquote>
<ul>
<li>Flexible models can pick up on non-linear patterns in regression and are more useful in prediction when we are only concerned with accuracy, especially with large <span class="math inline">\(n\)</span>; and they are less biased, however more variable. A less flexible approach is better for inference when we want to understand the relationship(s) between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>; more bias, but less variable.</li>
</ul></section><section id="question-6" class="level4"><h4 class="anchored" data-anchor-id="question-6">Question 6</h4>
<blockquote class="blockquote">
<p>Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?</p>
</blockquote>
<p>Parametric approach assumes a functional form of the response and one needs to only estimate parameters, which is a simpler task. And because of the assumption, the model is more interpretable. However a non-parametric method just fits the data as best as possible. Non-parametric methods can be less biased, but are more variable, especially as the complexity increases and more data is needed because of the more parameters being estimated.</p>
</section><section id="question-7" class="level4"><h4 class="anchored" data-anchor-id="question-7">Question 7</h4>
<blockquote class="blockquote">
<p>The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.</p>
</blockquote>
<p><img src="files/images/2-ex-7-table.png" class="img-fluid" style="width:50.0%"></p>
<blockquote class="blockquote">
<p>Suppose we wish to use this data set to make a prediction for <span class="math inline">\(Y\)</span> when <span class="math inline">\(X_1 = X_2 = X_3 = 0\)</span> using <span class="math inline">\(K\)</span>-nearest neighbors.</p>
</blockquote>
<blockquote class="blockquote">
<ol type="a">
<li>Compute the Euclidean distance between each observation and the test point, <span class="math inline">\(X_1 = X_2 = X_3 = 0\)</span>.</li>
</ol>
</blockquote>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># read in data</span></span>
<span><span class="va">df</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span></span>
<span>  x1 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">2</span>, <span class="fl">0</span>, <span class="fl">0</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  x2 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  x3 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0</span>, <span class="fl">3</span>, <span class="fl">2</span>, <span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span>,</span>
<span>  y <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Red"</span>, <span class="st">"Red"</span>, <span class="st">"Red"</span>, <span class="st">"Green"</span>, <span class="st">"Green"</span>, <span class="st">"Red"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="co"># define function to calculate euclidean distance </span></span>
<span><span class="co"># -&gt; sum function calculates the sum of the squares of absolute difference between  corresponding elements of vec_1 and vec_2 </span></span>
<span><span class="va">calc_euc_dist</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">vec_1</span>, <span class="va">vec_2</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu">subtract</span><span class="op">(</span><span class="va">vec_1</span>, <span class="va">vec_2</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">raise_to_power</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="va">sum</span> <span class="op">%&gt;%</span> <span class="va">sqrt</span></span>
<span><span class="op">}</span> </span>
<span></span>
<span><span class="co"># calculate euclidean distance for each observation</span></span>
<span><span class="va">df</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>euc_dist <span class="op">=</span> <span class="fu">calc_euc_dist</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>,</span>
<span>         .keep <span class="op">=</span> <span class="st">"none"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 6 × 1
# Rowwise: 
  euc_dist
     &lt;dbl&gt;
1     3   
2     2   
3     3.16
4     2.24
5     1.41
6     1.73</code></pre>
</div>
</div>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>What is our prediction with K = 1? Why?</li>
</ol>
</blockquote>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">df</span> <span class="op">%&gt;%</span></span>
<span>  <span class="fu">rowwise</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>euc_dist <span class="op">=</span> <span class="fu">calc_euc_dist</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">x1</span>, <span class="va">x2</span>, <span class="va">x3</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span><span class="op">)</span><span class="op">)</span>,</span>
<span>         .keep <span class="op">=</span> <span class="st">"unused"</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">ungroup</span><span class="op">(</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">slice_min</span><span class="op">(</span>order_by <span class="op">=</span> <span class="va">euc_dist</span>, n <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
  y     euc_dist
  &lt;chr&gt;    &lt;dbl&gt;
1 Green     1.41</code></pre>
</div>
</div>
<p>Green is our prediction</p>
<blockquote class="blockquote">
<ol start="3" type="a">
<li>What is our prediction with K = 3? Why?</li>
</ol>
</blockquote>
<div class="cell">
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># turn above into a function</span></span>
<span></span>
<span><span class="co"># define function to do KNN prediction for a particular test point</span></span>
<span><span class="va">algorithm_knn</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">df</span>, <span class="va">y</span>, <span class="va">x0</span>, <span class="va">k</span> <span class="op">=</span> <span class="fl">1</span>, <span class="va">final_result</span> <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span></span>
<span> </span>
<span>  <span class="co"># get column names</span></span>
<span>  <span class="va">nms</span> <span class="op">=</span> <span class="va">df</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="va">colnames</span></span>
<span>  </span>
<span>  <span class="co"># standardized column names</span></span>
<span>  <span class="co"># -&gt; place response variable first and renamed</span></span>
<span>  <span class="co"># -&gt; followed by explanatory variables renamed x1, x2, ... (unnecessary)</span></span>
<span>  <span class="va">df_z</span> <span class="op">&lt;-</span> <span class="va">df</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">select</span><span class="op">(</span>y <span class="op">=</span> <span class="fu">any_of</span><span class="op">(</span><span class="va">y</span><span class="op">)</span>, <span class="fu">any_of</span><span class="op">(</span><span class="va">nms</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.html">which</a></span><span class="op">(</span><span class="va">nms</span> <span class="op">!=</span> <span class="va">y</span><span class="op">)</span><span class="op">]</span><span class="op">)</span><span class="op">)</span> <span class="co">#%&gt;% </span></span>
<span>    <span class="co">#rename_with(~ paste0("x", 1:(ncol(df)-1)), .cols = any_of(nms[which(nms != y)]))</span></span>
<span>   </span>
<span>  <span class="co"># calculate euclidean distances</span></span>
<span>  <span class="va">euc_dist</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">)</span></span>
<span>  <span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">df_z</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>    <span class="va">euc_dist</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">=</span> <span class="fu">calc_euc_dist</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/vector.html">as.vector</a></span><span class="op">(</span><span class="va">df_z</span><span class="op">[</span><span class="va">i</span>, <span class="fl">2</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">df_z</span><span class="op">)</span><span class="op">]</span>, mode <span class="op">=</span> <span class="st">"double"</span><span class="op">)</span>, <span class="va">x0</span><span class="op">)</span> </span>
<span>    </span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="co"># create dataframe of results</span></span>
<span>  <span class="va">df_dist</span> <span class="op">=</span> <span class="va">df_z</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">select</span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">bind_cols</span><span class="op">(</span>euc_dist <span class="op">=</span> <span class="va">euc_dist</span><span class="op">)</span></span>
<span>    </span>
<span>  <span class="co"># display results</span></span>
<span>  <span class="va">results</span> <span class="op">=</span> <span class="va">df_dist</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">slice_min</span><span class="op">(</span>order_by <span class="op">=</span> <span class="va">euc_dist</span>, n <span class="op">=</span> <span class="va">k</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>    <span class="fu">summarize</span><span class="op">(</span>.by <span class="op">=</span> <span class="va">y</span>, </span>
<span>              est_prob <span class="op">=</span> <span class="fu">n</span><span class="op">(</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># conditionally simplify results</span></span>
<span>  <span class="kw">if</span><span class="op">(</span><span class="va">final_result</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">results</span> <span class="op">%&lt;&gt;%</span> </span>
<span>      <span class="fu">slice_min</span><span class="op">(</span>order_by <span class="op">=</span> <span class="va">est_prob</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>      <span class="fu">select</span><span class="op">(</span><span class="va">y</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>      <span class="va">as.character</span></span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="kw"><a href="https://rdrr.io/r/base/function.html">return</a></span><span class="op">(</span><span class="va">results</span><span class="op">)</span></span>
<span>  </span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># test function on K = 1</span></span>
<span><span class="fu">algorithm_knn</span><span class="op">(</span><span class="va">df</span>, y <span class="op">=</span> <span class="st">"y"</span>, x0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span><span class="op">)</span>, k <span class="op">=</span> <span class="fl">1</span>, final_result <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 2
  y     est_prob
  &lt;chr&gt;    &lt;dbl&gt;
1 Green        1</code></pre>
</div>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># now for K = 3</span></span>
<span><span class="fu">algorithm_knn</span><span class="op">(</span><span class="va">df</span>, y <span class="op">=</span> <span class="st">"y"</span>, x0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span><span class="op">)</span>, k <span class="op">=</span> <span class="fl">3</span>, final_result <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 2 × 2
  y     est_prob
  &lt;chr&gt;    &lt;dbl&gt;
1 Green    0.333
2 Red      0.667</code></pre>
</div>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu">algorithm_knn</span><span class="op">(</span><span class="va">df</span>, y <span class="op">=</span> <span class="st">"y"</span>, x0 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">3</span><span class="op">)</span>, k <span class="op">=</span> <span class="fl">3</span>, final_result <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Green"</code></pre>
</div>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># test a set of points</span></span>
<span><span class="va">x0</span> <span class="op">&lt;-</span> <span class="fu">tibble</span><span class="op">(</span></span>
<span>  x1 <span class="op">=</span> <span class="fu">extraDistr</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/extraDistr/man/DiscreteUniform.html">rdunif</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">5</span>, min <span class="op">=</span> <span class="fl">0</span>, max <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  x2 <span class="op">=</span> <span class="fu">extraDistr</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/extraDistr/man/DiscreteUniform.html">rdunif</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">5</span>, min <span class="op">=</span> <span class="fl">0</span>, max <span class="op">=</span> <span class="fl">3</span><span class="op">)</span>,</span>
<span>  x3 <span class="op">=</span> <span class="fu">extraDistr</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/extraDistr/man/DiscreteUniform.html">rdunif</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fl">5</span>, min <span class="op">=</span> <span class="fl">0</span>, max <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">results</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">x0</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">results</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">=</span> <span class="fu">algorithm_knn</span><span class="op">(</span><span class="va">df</span>, y <span class="op">=</span> <span class="st">"y"</span>, x0 <span class="op">=</span> <span class="va">x0</span><span class="op">[</span><span class="va">i</span>,<span class="op">]</span>, k <span class="op">=</span> <span class="fl">3</span>, final_result <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="va">results</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Red"   "Green" "Green" "Green" "Red"  </code></pre>
</div>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># note if results include a vector of two classes, then there was a tie. Not sure how to break ties, but could research and code something...</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<blockquote class="blockquote">
<ol start="4" type="a">
<li>If the Bayes decision boundary in this problem is highly non- linear, then would we expect the best value for K to be large or small? Why?</li>
</ol>
</blockquote>
<p>Small so that the model is more flexible (high <span class="math inline">\(k\)</span> leads to linear boundaries due to averaging).</p>
</section></section><section id="applied" class="level3" data-number="3.3.2"><h3 data-number="3.3.2" class="anchored" data-anchor-id="applied">
<span class="header-section-number">3.3.2</span> Applied</h3>
<section id="question-8" class="level4"><h4 class="anchored" data-anchor-id="question-8">Question 8</h4>
<div class="cell">
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load data</span></span>
<span><span class="va">data_college</span> <span class="op">&lt;-</span> <span class="fu">ISLR2</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ISLR2/man/College.html">College</a></span></span>
<span></span>
<span><span class="co"># summarize numeric variables</span></span>
<span><span class="va">data_college</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="fu">where</span><span class="op">(</span><span class="va">is.numeric</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">summary</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>      Apps           Accept          Enroll       Top10perc       Top25perc    
 Min.   :   81   Min.   :   72   Min.   :  35   Min.   : 1.00   Min.   :  9.0  
 1st Qu.:  776   1st Qu.:  604   1st Qu.: 242   1st Qu.:15.00   1st Qu.: 41.0  
 Median : 1558   Median : 1110   Median : 434   Median :23.00   Median : 54.0  
 Mean   : 3002   Mean   : 2019   Mean   : 780   Mean   :27.56   Mean   : 55.8  
 3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902   3rd Qu.:35.00   3rd Qu.: 69.0  
 Max.   :48094   Max.   :26330   Max.   :6392   Max.   :96.00   Max.   :100.0  
  F.Undergrad     P.Undergrad         Outstate       Room.Board  
 Min.   :  139   Min.   :    1.0   Min.   : 2340   Min.   :1780  
 1st Qu.:  992   1st Qu.:   95.0   1st Qu.: 7320   1st Qu.:3597  
 Median : 1707   Median :  353.0   Median : 9990   Median :4200  
 Mean   : 3700   Mean   :  855.3   Mean   :10441   Mean   :4358  
 3rd Qu.: 4005   3rd Qu.:  967.0   3rd Qu.:12925   3rd Qu.:5050  
 Max.   :31643   Max.   :21836.0   Max.   :21700   Max.   :8124  
     Books           Personal         PhD            Terminal    
 Min.   :  96.0   Min.   : 250   Min.   :  8.00   Min.   : 24.0  
 1st Qu.: 470.0   1st Qu.: 850   1st Qu.: 62.00   1st Qu.: 71.0  
 Median : 500.0   Median :1200   Median : 75.00   Median : 82.0  
 Mean   : 549.4   Mean   :1341   Mean   : 72.66   Mean   : 79.7  
 3rd Qu.: 600.0   3rd Qu.:1700   3rd Qu.: 85.00   3rd Qu.: 92.0  
 Max.   :2340.0   Max.   :6800   Max.   :103.00   Max.   :100.0  
   S.F.Ratio      perc.alumni        Expend        Grad.Rate     
 Min.   : 2.50   Min.   : 0.00   Min.   : 3186   Min.   : 10.00  
 1st Qu.:11.50   1st Qu.:13.00   1st Qu.: 6751   1st Qu.: 53.00  
 Median :13.60   Median :21.00   Median : 8377   Median : 65.00  
 Mean   :14.09   Mean   :22.74   Mean   : 9660   Mean   : 65.46  
 3rd Qu.:16.50   3rd Qu.:31.00   3rd Qu.:10830   3rd Qu.: 78.00  
 Max.   :39.80   Max.   :64.00   Max.   :56233   Max.   :118.00  </code></pre>
</div>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># scatterplot matrix</span></span>
<span><span class="va">data_college</span><span class="op">[</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span> <span class="op">%&gt;%</span> <span class="va">pairs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-2_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># side-by-side boxplots</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/boxplot.html">boxplot</a></span><span class="op">(</span><span class="va">Outstate</span> <span class="op">~</span> <span class="va">Private</span>, data <span class="op">=</span> <span class="va">data_college</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-2_files/figure-html/unnamed-chunk-5-2.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># create new variable and summarize</span></span>
<span><span class="va">data_college</span> <span class="op">%&lt;&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>Elite <span class="op">=</span> <span class="fu">case_when</span><span class="op">(</span></span>
<span>    <span class="va">Top10perc</span> <span class="op">&gt;</span> <span class="fl">50</span> <span class="op">~</span> <span class="st">"Yes"</span>,</span>
<span>    .default <span class="op">=</span> <span class="st">"No"</span></span>
<span>  <span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">data_college</span><span class="op">$</span><span class="va">Elite</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
 No Yes 
699  78 </code></pre>
</div>
</div>
</section><section id="question-9" class="level4"><h4 class="anchored" data-anchor-id="question-9">Question 9</h4>
<div class="cell">
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load data</span></span>
<span><span class="va">data_auto</span> <span class="op">&lt;-</span> <span class="fu">ISLR2</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ISLR2/man/Auto.html">Auto</a></span> </span>
<span></span>
<span><span class="co"># determine variable types</span></span>
<span><span class="va">data_auto</span> <span class="op">%&gt;%</span> <span class="fu">map_chr</span><span class="op">(</span><span class="va">class</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         mpg    cylinders displacement   horsepower       weight acceleration 
   "numeric"    "integer"    "numeric"    "integer"    "integer"    "numeric" 
        year       origin         name 
   "integer"    "integer"     "factor" </code></pre>
</div>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># find range of each quantitative predictor</span></span>
<span><span class="va">data_auto</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="fu">where</span><span class="op">(</span><span class="va">is.numeric</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">reframe</span><span class="op">(</span><span class="fu">across</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span>, <span class="va">range</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   mpg cylinders displacement horsepower weight acceleration year origin
1  9.0         3           68         46   1613          8.0   70      1
2 46.6         8          455        230   5140         24.8   82      3</code></pre>
</div>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># find mean and st dev of each quantitative predictor</span></span>
<span><span class="co"># -&gt; then format nicely</span></span>
<span><span class="va">data_auto</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="fu">where</span><span class="op">(</span><span class="va">is.numeric</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">summarize</span><span class="op">(</span><span class="fu">across</span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">ncol</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>mean <span class="op">=</span> <span class="va">mean</span>, sd <span class="op">=</span> <span class="va">sd</span><span class="op">)</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">pivot_longer</span><span class="op">(</span><span class="fu">everything</span><span class="op">(</span><span class="op">)</span>,</span>
<span>               names_to <span class="op">=</span> <span class="st">"var"</span>,</span>
<span>               values_to <span class="op">=</span> <span class="st">"val"</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">separate_wider_delim</span><span class="op">(</span>cols <span class="op">=</span> <span class="va">var</span>,</span>
<span>                       delim <span class="op">=</span> <span class="st">"_"</span>,</span>
<span>                       names <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"var"</span>, <span class="st">"fun"</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">pivot_wider</span><span class="op">(</span>names_from <span class="op">=</span> <span class="st">"fun"</span>,</span>
<span>              values_from <span class="op">=</span> <span class="st">"val"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 8 × 3
  var             mean      sd
  &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;
1 mpg            23.4    7.81 
2 cylinders       5.47   1.71 
3 displacement  194.   105.   
4 horsepower    104.    38.5  
5 weight       2978.   849.   
6 acceleration   15.5    2.76 
7 year           76.0    3.68 
8 origin          1.58   0.806</code></pre>
</div>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># EDA</span></span>
<span><span class="co"># -&gt; trying to predict mpg</span></span>
<span><span class="va">data_auto</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="fu">where</span><span class="op">(</span><span class="va">is.numeric</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">pairs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-2_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># useful variables</span></span>
<span><span class="co"># -&gt; all predictors appear to be correlated with the response (some have non-linear relationship), however there are several that are highly correlated among themselves</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section><section id="question-10" class="level4"><h4 class="anchored" data-anchor-id="question-10">Question 10</h4>
<div class="cell">
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load data</span></span>
<span><span class="va">data_boston</span> <span class="op">&lt;-</span> <span class="fu">ISLR2</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ISLR2/man/Boston.html">Boston</a></span></span>
<span></span>
<span><span class="co"># &lt; ... easy stuff ... &gt; </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


<!-- -->

</section></section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./partC-glm.html" class="pagination-link  aria-label=" extended="" linear="" models="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Extended Linear Models</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./islr-3.html" class="pagination-link" aria-label="<span class='chapter-number'>4</span>&nbsp; <span class='chapter-title'>Linear regression</span>">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Linear regression</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb29" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># Statistical learning</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: load-prereqs</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a><span class="co"># knitr options</span></span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"_common.R"</span>)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a><span class="fu">## Notes</span></span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a><span class="fu">### What is statistical learning?</span></span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>Suppose that we observe a quantitative response $Y$ and $p$ different predictors, $X_1, X_2, \ldots , X_p$. We assume that there is some relationship between $Y$ and $X = (X_1, X_2, \ldots, Xp)$, which can be written in the very general form</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>Y = f(X) + \epsilon</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>Here $f$ is some fixed but unknown function of $X_1, \ldots , X_p$, and $\epsilon$ is a random error term, which is independent of $X$ and has mean zero. In this formulation, $f$ represents the systematic information that $X$ provides about $Y$.</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>**In essence, statistical learning refers to a set of approaches for estimating $f$.**</span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Why estimate $f$?</span></span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>There are two main reasons that we may wish to estimate $f$: *prediction*</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>and *inference*.</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>**Prediction**</span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>In many situations, a set of inputs $X$ are readily available, but the output $Y$ cannot be easily obtained. In this setting, since the error term averages to zero, we can predict $Y$ using</span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>\hat{Y} = \hat{f}(X),</span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>where $\hat{f}$ represents our estimate for $f$ , and $\hat{Y}$ represents the resulting prediction for $Y$. In this setting, $\hat{f}$  is often treated as a *black box*, in the sense that one is not typically concerned with the exact form of $\hat{f}$, provided that</span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>it yields accurate predictions for $Y$.</span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>The accuracy of $\hat{Y}$ as a prediction for $Y$ depends on two quantities, which we will call the *reducible error* and the *irreducible error*. In general, $\hat{f}$ will not be a perfect estimate for $f$, and this inaccuracy will introduce some error. This error is *reducible* because we can potentially improve the accuracy of $\hat{f}$ by using the most appropriate statistical learning technique to estimate $f$. However, even if it were possible to form a perfect estimate for $f$, so that our estimated response took the form $\hat{Y} = f(X)$, our prediction would still have some error in it! This is because $Y$ is also a function of $\epsilon$, which, by definition, cannot be predicted using $X$. Therefore, variability associated with $\epsilon$ also affects the accuracy of our predictions. This is known as the *irreducible error*, because no matter how well we estimate $f$, we cannot reduce the error introduced by $\epsilon$.</span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>Why is the irreducible error larger than zero? The quantity $\epsilon$ may contain unmeasured variables that are useful in predicting $Y$: since we don’t measure them, $f$ cannot use them for its prediction. The quantity $\epsilon$ may also contain unmeasurable variation.</span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a>Consider a given estimate $\hat{f}$ and a set of predictors $X$, which yields the</span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a>prediction $\hat{Y} = \hat{f}(X)$. Assume for a moment that both $\hat{f}$ and $X$ are fixed, so that the only variability comes from $\epsilon$. Then, we can say:</span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-expected-value.png)</span>{width="50%"}</span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a>where $E(Y − \hat{Y})^2$ represents the average, or *expected value*, of the squared difference between the predicted and actual value of $Y$, and $Var(\epsilon)$ represents the variance associated with the error term $\epsilon$.</span>
<span id="cb29-55"><a href="#cb29-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-56"><a href="#cb29-56" aria-hidden="true" tabindex="-1"></a>The focus of this book is on techniques for estimating $f$ with the aim of minimizing the reducible error. It is important to keep in mind that the irreducible error will always provide an upper bound on the accuracy of our prediction for $Y$. This bound is almost always unknown in practice.</span>
<span id="cb29-57"><a href="#cb29-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-58"><a href="#cb29-58" aria-hidden="true" tabindex="-1"></a>**Inference**</span>
<span id="cb29-59"><a href="#cb29-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-60"><a href="#cb29-60" aria-hidden="true" tabindex="-1"></a>We are often interested in understanding the association between $Y$ and $X_1, \ldots , X_p$. In this situation we wish to estimate $f$, but our goal is not necessarily to make predictions for $Y$. Now $\hat{f}$ cannot be treated as a black box, because we need to know its exact form. In this setting, one may be interested in answering the following questions:</span>
<span id="cb29-61"><a href="#cb29-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-62"><a href="#cb29-62" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*Which predictors are associated with the response?* It is often the case that only a small fraction of the available predictors are substantially associated with $Y$. Identifying the few important predictors among a large set of possible variables can be extremely useful, depending on the application.</span>
<span id="cb29-63"><a href="#cb29-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-64"><a href="#cb29-64" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*What is the relationship between the response and each predictor?* Some predictors may have a positive relationship with $Y$, in the sense that larger values of the predictor are associated with larger values of $Y$. Other predictors may have the opposite relationship. Depending on the complexity of $f$, the relationship between the response and a given predictor may also depend on the values of the other predictors.</span>
<span id="cb29-65"><a href="#cb29-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-66"><a href="#cb29-66" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*Can the relationship between $Y$ and each predictor be adequately summarized using a linear equation, or is the relationship more complicated?* Historically, most methods for estimating $f$ have taken a linear form. In some situations, such an assumption is reasonable or even desirable. But often the true relationship is more complicated, in which case a linear model may not provide an accurate representation of the relationship between the input and output variables.</span>
<span id="cb29-67"><a href="#cb29-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-68"><a href="#cb29-68" aria-hidden="true" tabindex="-1"></a><span class="fu">#### How do we estimate $f$?</span></span>
<span id="cb29-69"><a href="#cb29-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-70"><a href="#cb29-70" aria-hidden="true" tabindex="-1"></a>Throughout this book, we explore many linear and non-linear approaches for estimating $f$. However, these methods generally share certain characteristics. Here is an overview.</span>
<span id="cb29-71"><a href="#cb29-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-72"><a href="#cb29-72" aria-hidden="true" tabindex="-1"></a>Note we will always assume that we have observed a set of $n$ different data points, called the *training data* because we will use these observations to train, or teach, our method how to estimate $f$.</span>
<span id="cb29-73"><a href="#cb29-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-74"><a href="#cb29-74" aria-hidden="true" tabindex="-1"></a>Our goal is to apply a statistical learning method to the training data in order to estimate the unknown function $f$. In other words, we want to find a function $\hat{f}$ such that $Y \approx \hat{f}(X)$ for any observation $(X,Y)$. Broadly speaking, most statistical learning methods for this task can be characterized as either *parametric* or *non-parametric*. We now briefly discuss these two types of approaches.</span>
<span id="cb29-75"><a href="#cb29-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-76"><a href="#cb29-76" aria-hidden="true" tabindex="-1"></a>**Parametric**</span>
<span id="cb29-77"><a href="#cb29-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-78"><a href="#cb29-78" aria-hidden="true" tabindex="-1"></a>Parametric methods involve a two-step model-based approach.</span>
<span id="cb29-79"><a href="#cb29-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-80"><a href="#cb29-80" aria-hidden="true" tabindex="-1"></a>Step 1</span>
<span id="cb29-81"><a href="#cb29-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-82"><a href="#cb29-82" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>First, we make an assumption about the functional form, or shape, of $f$. For example, one very simple assumption is that $f$ is linear in $X$:</span>
<span id="cb29-83"><a href="#cb29-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-84"><a href="#cb29-84" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-85"><a href="#cb29-85" aria-hidden="true" tabindex="-1"></a>f(X) = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p</span>
<span id="cb29-86"><a href="#cb29-86" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-87"><a href="#cb29-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-88"><a href="#cb29-88" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This is a *linear model*. Once we have assumed that $f$ is linear, the problem of estimating $f$ is greatly simplified. Instead of having to estimate an entirely arbitrary $p$-dimensional function $f(X)$, one only needs to estimate the p+1 coefficients $\beta_0, \beta_1, \ldots, \beta_p$.</span>
<span id="cb29-89"><a href="#cb29-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-90"><a href="#cb29-90" aria-hidden="true" tabindex="-1"></a>Step 2</span>
<span id="cb29-91"><a href="#cb29-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-92"><a href="#cb29-92" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>After a model has been selected, we need a procedure that uses the training data to fit or train the model. In the case of the linear model, we need to estimate the parameters $\beta_0, \beta_1, \ldots, \beta_p$.That is,we want to find values of these parameters such that</span>
<span id="cb29-93"><a href="#cb29-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-94"><a href="#cb29-94" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-95"><a href="#cb29-95" aria-hidden="true" tabindex="-1"></a>Y \approx \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p</span>
<span id="cb29-96"><a href="#cb29-96" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-97"><a href="#cb29-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-98"><a href="#cb29-98" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The most common approach to fitting the model above is referred to as *(ordinary) least squares*. However, least squares is one of many possible ways to fit the linear model.</span>
<span id="cb29-99"><a href="#cb29-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-100"><a href="#cb29-100" aria-hidden="true" tabindex="-1"></a>The model-based approach just described is referred to as *parametric*; **it reduces the problem of estimating $f$ down to one of estimating a set of parameters**. Assuming a parametric form for $f$ simplifies the problem of estimating $f$ because it is generally much easier to estimate a set of parameters, such as $\beta_0, \beta_1, \ldots, \beta_p$ in the linear model, than it is to fit an entirely arbitrary function $f$. The potential disadvantage of a parametric approach is that the model we choose will usually not match the true unknown form of $f$. If the chosen model is too far from the true $f$, then our estimate will be poor. We can try to address this problem by choosing flexible models that can fit many different possible functional forms for $f$. But in general, fitting a more flexible model requires estimating a greater number of parameters. These more complex models can lead to a phenomenon known as *overfitting* the data, which essentially means they follow the errors, or noise, too closely.</span>
<span id="cb29-101"><a href="#cb29-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-102"><a href="#cb29-102" aria-hidden="true" tabindex="-1"></a>**Non-parametric**</span>
<span id="cb29-103"><a href="#cb29-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-104"><a href="#cb29-104" aria-hidden="true" tabindex="-1"></a>**Non-parametric methods do not make explicit assumptions about the functional form of $f$**. Instead they seek an estimate of $f$ that gets as close to the data points as possible without being too rough or wiggly. Such approaches can have a major advantage over parametric approaches: by avoiding the assumption of a particular functional form for $f$, they have the potential to accurately fit a wider range of possible shapes for $f$.</span>
<span id="cb29-105"><a href="#cb29-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-106"><a href="#cb29-106" aria-hidden="true" tabindex="-1"></a>Any parametric approach brings with it the possibility that the functional form used to estimate $f$ is very different from the true $f$, in which case the resulting model will not fit the data well. In contrast, non-parametric approaches completely avoid this danger, since essentially no assumption about the form of $f$ is made. But non-parametric approaches do suffer from a major disadvantage: since they do not reduce the problem of estimating $f$ to a small number of parameters, a very large number of observations (far more than is typically needed for a parametric approach) is required in order to obtain an accurate estimate for $f$.</span>
<span id="cb29-107"><a href="#cb29-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-108"><a href="#cb29-108" aria-hidden="true" tabindex="-1"></a>Be careful of overfitting though, non-parametric methods can fit the data perfectly if complex enough, which causes the fit obtained to not yield accurate estimates of the response on new observations that were not part of the original training data set.</span>
<span id="cb29-109"><a href="#cb29-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-110"><a href="#cb29-110" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-perfect-fit.png)</span>{width="50%"}</span>
<span id="cb29-111"><a href="#cb29-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-112"><a href="#cb29-112" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The trade-off between prediction accuracy and model interpretability</span></span>
<span id="cb29-113"><a href="#cb29-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-114"><a href="#cb29-114" aria-hidden="true" tabindex="-1"></a>Of the many methods that we examine in this book, some are less flexible, or more restrictive, in the sense that they can produce just a relatively small range of shapes to estimate $f$. For example, linear regression is a relatively inflexible approach, because it can only generate linear functions (smooth lines / planes). Other methods are considerably more flexible because they can generate a much wider range of possible shapes to estimate $f$.</span>
<span id="cb29-115"><a href="#cb29-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-116"><a href="#cb29-116" aria-hidden="true" tabindex="-1"></a>One might reasonably ask the following question: *why would we ever choose to use a more restrictive method instead of a very flexible approach?* There are several reasons that we might prefer a more restrictive model. If we are mainly interested in inference, then restrictive models are much more interpretable. For instance, when inference is the goal, the linear model may be a good choice since it will be quite easy to understand the relationship between $Y$ and $X_1, X_2, \ldots, X_p$. In contrast, very flexible approaches, such as the splines and boosting methods can lead to such complicated estimates of $f$ that it is difficult to understand how any individual predictor is associated with the response.</span>
<span id="cb29-117"><a href="#cb29-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-118"><a href="#cb29-118" aria-hidden="true" tabindex="-1"></a>Here is an illustration of the trade-off between flexibility and interpretability for some of the methods that we cover in this book.</span>
<span id="cb29-119"><a href="#cb29-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-120"><a href="#cb29-120" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-flex-vs-inter.png)</span>{width="50%"}</span>
<span id="cb29-121"><a href="#cb29-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-122"><a href="#cb29-122" aria-hidden="true" tabindex="-1"></a>Least squares linear regression is relatively inflexible but is quite interpretable. The *lasso* relies upon the linear model but uses an alternative fitting procedure for estimating the coefficients $\beta_0, \beta_1, \ldots, \beta_p$. The new procedure is more restrictive in estimating the coefficients, and sets a number of them to exactly zero. Hence in this sense the lasso is a less flexible approach than linear regression. It is also more interpretable than linear regression, because in the final model the response variable will only be related to a small subset of the predictors — namely, those with nonzero coefficient estimates.</span>
<span id="cb29-123"><a href="#cb29-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-124"><a href="#cb29-124" aria-hidden="true" tabindex="-1"></a>*Generalized additive models* (GAMs), discussed in Chapter 7, instead extend the linear model to allow for certain non-linear relationships. Consequently, GAMs are more flexible than linear regression. They are also somewhat less interpretable than linear regression, because the relationship between each predictor and the response is now modeled using a curve. Finally, fully non-linear methods such as *bagging*, *boosting*, *support vector machines* with non-linear kernels, and *neural networks* (deep learning) are highly flexible approaches that are harder to interpret.</span>
<span id="cb29-125"><a href="#cb29-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-126"><a href="#cb29-126" aria-hidden="true" tabindex="-1"></a>We have established that when inference is the goal, there are clear advantages to using simple and relatively inflexible statistical learning methods. In some settings, however, we are only interested in prediction, and the interpretability of the predictive model is simply not of interest; our sole requirement for the algorithm is that it predict accurately (i.e. interpretability is not a concern). In this setting, we might expect that it will be best to use the most flexible model available. Surprisingly, this is not always the case! We will often obtain more accurate predictions using a less flexible method. This phenomenon, which may seem counterintuitive at first glance, has to do with the potential for overfitting in highly flexible methods.</span>
<span id="cb29-127"><a href="#cb29-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-128"><a href="#cb29-128" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Supervised vs unsupervised learning</span></span>
<span id="cb29-129"><a href="#cb29-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-130"><a href="#cb29-130" aria-hidden="true" tabindex="-1"></a>Most statistical learning problems fall in to one of two categories: *supervised* or *unsupervised*. The examples that we have discussed so far in this chapter all fall into the supervised learning domain. For each observation of the predictor measurement(s) $x_i, i = 1, \ldots , n$ there is an associated response measurement $y_i$. We wish to fit a model that relates the response to the predictors, with the aim of accurately predicting the response for future observations (prediction) or better understanding the relationship between the response and the predictors (inference). Many classical statistical learning methods such as linear regression and logistic regression, as well as more modern approaches such as GAM, boosting, and support vector machines, operate in the supervised learning domain. The vast majority of this book is devoted to this setting.</span>
<span id="cb29-131"><a href="#cb29-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-132"><a href="#cb29-132" aria-hidden="true" tabindex="-1"></a>By contrast, unsupervised learning describes the somewhat more challenging situation in which for every observation $i = 1, \ldots, n$, we observe a vector of measurements $x_i$ but no associated response $y_i$. It is not possible to fit a linear regression model, since there is no response variable to predict.  In this setting, we are in some sense working blind; the situation is referred to as *unsupervised* because we lack a response variable that can supervise our analysis. What sort of statistical analysis is possible? We can seek to understand the relationships between the variables or between the observations. One statistical learning tool that we may use in this setting is *cluster analysis*, or clustering. The goal of cluster analysis is to ascertain, on the basis of $x_1, \ldots, x_n$, whether the observations fall into relatively distinct groups.</span>
<span id="cb29-133"><a href="#cb29-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-134"><a href="#cb29-134" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-clustering.png)</span>{width="50%"}</span>
<span id="cb29-135"><a href="#cb29-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-136"><a href="#cb29-136" aria-hidden="true" tabindex="-1"></a>However, in practice the group memberships are unknown, and the goal is to determine the group to which each observation belongs. A clustering method could not be expected to assign all of the overlapping points to their correct group.</span>
<span id="cb29-137"><a href="#cb29-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-138"><a href="#cb29-138" aria-hidden="true" tabindex="-1"></a>In the example above, there are only two variables, and so one can simply visually inspect the scatterplots of the observations in order to identify clusters. However, in practice, we often encounter data sets that contain many more than two variables. In this case, we cannot easily plot the observations. For instance, if there are $p$ variables in our data set, then $p(p − 1)/2$ distinct scatterplots can be made, and visual inspection is simply not a viable way to identify clusters. For this reason, automated clustering methods are important.</span>
<span id="cb29-139"><a href="#cb29-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-140"><a href="#cb29-140" aria-hidden="true" tabindex="-1"></a>Many problems fall naturally into the supervised or unsupervised learning paradigms. However, sometimes the question of whether an analysis should be considered supervised or unsupervised is less clear-cut. For instance, suppose that we have a set of $n$ observations. For $m$ of the observations, where $m &lt; n$, we have both predictor measurements and a response measurement. For the remaining $n − m$ observations, we have predictor measurements but no response measurement. Such a scenario can arise if the predictors can be measured relatively cheaply but the corresponding responses are much more expensive to collect. We refer to this setting as a *semi-supervised learning* problem. In this setting, we wish to use a statistical learning method that can incorporate the $m$ observations for which response measurements are available as well as the $n − m$ observations for which they are not. Although this is an interesting topic, it is beyond the scope of this book.</span>
<span id="cb29-141"><a href="#cb29-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-142"><a href="#cb29-142" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Regression vs classification problems</span></span>
<span id="cb29-143"><a href="#cb29-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-144"><a href="#cb29-144" aria-hidden="true" tabindex="-1"></a>We tend to refer to problems with a quantitative response as *regression* problems, while those involving a qualitative response are often referred to as *classification* problems. However, the distinction is not always that crisp. Least squares linear regression is used with a quantitative response, whereas logistic regression is typically used with a qualitative (two-class, or *binary*) response. Thus, despite its name, logistic regression is a classification method. But since it estimates class probabilities, it can be thought of as a regression method as well. Some statistical methods, such as $K$-nearest neighbors and boosting, can be used in the case of either quantitative or qualitative responses.</span>
<span id="cb29-145"><a href="#cb29-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-146"><a href="#cb29-146" aria-hidden="true" tabindex="-1"></a>We tend to select statistical learning methods on the basis of whether the response is quantitative or qualitative; i.e. we might use linear regression when quantitative and logistic regression when qualitative. However, whether the *predictors* are qualitative or quantitative is generally considered less important. Most of the statistical learning methods discussed in this book can be applied regardless of the predictor variable type, provided that any qualitative predictors are properly *coded* before the analysis is performed.</span>
<span id="cb29-147"><a href="#cb29-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-148"><a href="#cb29-148" aria-hidden="true" tabindex="-1"></a><span class="fu">### Assessing model accuracy</span></span>
<span id="cb29-149"><a href="#cb29-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-150"><a href="#cb29-150" aria-hidden="true" tabindex="-1"></a>One of the key aims of this book is to introduce the reader to a wide range of statistical learning methods that extend far beyond the standard linear regression approach. Why is it necessary to introduce so many different statistical learning approaches, rather than just a single *best* method? *There is no free lunch in statistics:* **no one method dominates all others over all possible data sets**. On a particular data set, one specific method may work best, but some other method may work better on a similar but different data set. Hence it is an important task to decide for any given set of data which method produces the best results. Selecting the best approach can be one of the most challenging parts of performing statistical learning in practice.</span>
<span id="cb29-151"><a href="#cb29-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-152"><a href="#cb29-152" aria-hidden="true" tabindex="-1"></a>Now, we discuss some of the most important concepts that arise in selecting a statistical learning procedure for a specific data set.</span>
<span id="cb29-153"><a href="#cb29-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-154"><a href="#cb29-154" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Measuring the quality of fit</span></span>
<span id="cb29-155"><a href="#cb29-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-156"><a href="#cb29-156" aria-hidden="true" tabindex="-1"></a>In order to evaluate the performance of a statistical learning method on a given data set, we need some way to measure how well its predictions actually match the observed data. That is, we need to quantify the extent to which the predicted response value for a given observation is close to the true response value for that observation. In the regression setting, the most commonly-used measure is the *mean squared error* (MSE), given by</span>
<span id="cb29-157"><a href="#cb29-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-158"><a href="#cb29-158" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-159"><a href="#cb29-159" aria-hidden="true" tabindex="-1"></a>MSE = \frac{1}{n} \sum_{i = 1}^n (y_i - \hat{f}(x_i))^2</span>
<span id="cb29-160"><a href="#cb29-160" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-161"><a href="#cb29-161" aria-hidden="true" tabindex="-1"></a>The MSE will be small if the predicted responses are very close to the true responses, and will be large if for some of the observations, the predicted and true responses differ substantially.</span>
<span id="cb29-162"><a href="#cb29-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-163"><a href="#cb29-163" aria-hidden="true" tabindex="-1"></a>This MSE is computed using the training data that was used to fit the model, and so should more accurately be referred to as the *training MSE*. But in general, we do not really care how well the method works training on the training data. Rather, *we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data*. To state it more mathematically, suppose that we fit our statistical learn- ing method on our training observations $<span class="sc">\{</span>(x_1,y_1), (x_2,y_2), \ldots, (x_n,y_n)<span class="sc">\}</span>$, and we obtain the estimate $\hat{f}$. We can then compute $\hat{f}(x_1), \hat{f}(x_2), \ldots , \hat{f}(x_n)$. If these are approximately equal to $y_1, y_2, \ldots, y_n$, then the training MSE is small. However, we are really not interested in whether $\hat{f}(x_i) \approx y_i$; instead, we want to know whether $\hat{f}(x_0)$ is approximately equal</span>
<span id="cb29-164"><a href="#cb29-164" aria-hidden="true" tabindex="-1"></a>to $y_0$, where $(x_0,y_0)$ is a *previously unseen test observation not used to train the statistical learning method*. We want to choose the method that gives the lowest *test MSE*, as opposed to the lowest training MSE. In other words, if we had a large number of test observations, we could compute </span>
<span id="cb29-165"><a href="#cb29-165" aria-hidden="true" tabindex="-1"></a>the average squared prediction error for these test observations $(x_0,y_0)$:</span>
<span id="cb29-166"><a href="#cb29-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-167"><a href="#cb29-167" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-168"><a href="#cb29-168" aria-hidden="true" tabindex="-1"></a>\text{Ave}(y_0 - \hat{f}(x_0))^2</span>
<span id="cb29-169"><a href="#cb29-169" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-170"><a href="#cb29-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-171"><a href="#cb29-171" aria-hidden="true" tabindex="-1"></a>We’d like to select the model for which this quantity is as small as possible.</span>
<span id="cb29-172"><a href="#cb29-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-173"><a href="#cb29-173" aria-hidden="true" tabindex="-1"></a>How can we go about trying to select a method that minimizes the test MSE? In some settings, we may have a test data set available. We can then simply evaluate the above formula on the test observations, and select the learning method for which the test MSE is smallest. But what if no test observations are available? In that case, one might imagine simply selecting a statistical learning method that minimizes the training MSE. This seems like it might be a sensible approach, since the training MSE and the test MSE appear to be closely related. Unfortunately, there is a fundamental problem with this strategy: there is no guarantee that the method with the lowest training MSE will also have the lowest test MSE. Roughly speaking, the problem is that many statistical methods specifically estimate coefficients so as to minimize the training set MSE. For these methods, the training set MSE can be quite small, but the test MSE is often much larger.</span>
<span id="cb29-174"><a href="#cb29-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-175"><a href="#cb29-175" aria-hidden="true" tabindex="-1"></a>The figure below illustrates this concept. In the left-hand panel, we have generated observations from some $f(X)$ with the true $f$ given by the black curve. The orange, blue and green curves illustrate three possible estimates for $f$ obtained using methods with increasing levels of flexibility. </span>
<span id="cb29-176"><a href="#cb29-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-177"><a href="#cb29-177" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-training-vs-test-mse.png)</span>{width="50%"}</span>
<span id="cb29-178"><a href="#cb29-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-179"><a href="#cb29-179" aria-hidden="true" tabindex="-1"></a>The blue and green curves were produced using *smoothing splines*. It is clear that as the level of flexibility increases, the curves fit the observed data more closely. The green curve is the most flexible and matches the data very well; however, we observe that it fits the true $f$ (shown in black) poorly because it is too wiggly.</span>
<span id="cb29-180"><a href="#cb29-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-181"><a href="#cb29-181" aria-hidden="true" tabindex="-1"></a>On the right panel, the grey curve displays the average training MSE as a function of flexibility, or more formally the *degrees of freedom*, for a number of smoothing splines. The degrees of freedom is a quantity that summarizes the flexibility of a curve. A more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve (i.e. more error dfs; a more restricted and hence smoother curve has fewer degrees of freedom than a wiggly curve). The training MSE declines monotonically as flexibility increases. In this example the true $f$ is non-linear, and so the orange linear fit is not flexible enough to estimate f well. The green curve has the lowest training MSE of all three methods, since it corresponds to the most flexible of the three curves fit in the left-hand panel.</span>
<span id="cb29-182"><a href="#cb29-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-183"><a href="#cb29-183" aria-hidden="true" tabindex="-1"></a>In this example, we know the true function $f$, and so we can also compute the test MSE over a very large test set, as a function of flexibility. (Of course, in general $f$ is unknown, so this will not be possible. The horizontal dashed line indicates $\text{Var}(\epsilon)$, the irreducible error, which corresponds to the lowest achievable test MSE among all possible methods. **As the flexibility of the statistical learning method increases, we observe a monotone decrease in the training MSE and a *U-shape* in the test MSE (red line)**. This is a fundamental property of statistical learning that holds regardless of the particular data set at hand and regardless of the statistical method being used. As model flexibility increases, training MSE will decrease, but the test MSE may not. When a given method yields a small training MSE but a large test MSE, we are said to be *overfitting* the data. This happens because our statistical learning procedure is working too hard to find patterns in the training data, and may be picking up some patterns that are just caused by random chance rather than by true properties of the unknown function $f$. When we overfit the training data, the test MSE will be very large because the supposed patterns that the method found in the training data simply don’t exist in the test data. **Note that regardless of whether or not overfitting has occurred, we almost always expect the training MSE to be smaller than the test MSE because most statistical learning methods either directly or indirectly seek to minimize the training MSE. Overfitting refers specifically to the case in which a less flexible model would have yielded a smaller test MSE.**</span>
<span id="cb29-184"><a href="#cb29-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-185"><a href="#cb29-185" aria-hidden="true" tabindex="-1"></a>Here is another example that displays the same patterns. However, because the truth is close to linear, the test MSE only decreases slightly before increasing again, so that the orange least squares fit is substantially better than the highly flexible green curve.</span>
<span id="cb29-186"><a href="#cb29-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-187"><a href="#cb29-187" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-training-vs-test-mse-2.png)</span>{width="50%"}</span>
<span id="cb29-188"><a href="#cb29-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-189"><a href="#cb29-189" aria-hidden="true" tabindex="-1"></a>Finally, here is another example in which $f$ is highly non-linear. Now there is a rapid decrease in both curves before the test MSE starts to increase slowly.</span>
<span id="cb29-190"><a href="#cb29-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-191"><a href="#cb29-191" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-training-vs-test-mse-3.png)</span>{width="50%"}</span>
<span id="cb29-192"><a href="#cb29-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-193"><a href="#cb29-193" aria-hidden="true" tabindex="-1"></a>In practice, one can usually compute the training MSE with relative ease, but estimating test MSE is considerably more difficult because usually no test data are available. As the previous three examples illustrate, the flexibility level corresponding to the model with the minimal test MSE can vary considerably among data sets. Throughout this book, we discuss a variety of approaches that can be used in practice to estimate this minimum point. One important method is *cross-validation*, which is a method for estimating test MSE using the training data.</span>
<span id="cb29-194"><a href="#cb29-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-195"><a href="#cb29-195" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The bias-variance trade-off</span></span>
<span id="cb29-196"><a href="#cb29-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-197"><a href="#cb29-197" aria-hidden="true" tabindex="-1"></a>The U-shape observed in the test MSE curves turns out to be the result of two competing properties of statistical learning methods. It is</span>
<span id="cb29-198"><a href="#cb29-198" aria-hidden="true" tabindex="-1"></a>possible to show that the expected test MSE, for a given value $x_0$, can</span>
<span id="cb29-199"><a href="#cb29-199" aria-hidden="true" tabindex="-1"></a>always be decomposed into the sum of three fundamental quantities: the *variance* of $\hat{f}(x_0)$, the squared *bias* of $f(x_0)$ and the variance of the error terms $\epsilon$. That is,</span>
<span id="cb29-200"><a href="#cb29-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-201"><a href="#cb29-201" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-202"><a href="#cb29-202" aria-hidden="true" tabindex="-1"></a>E\big(y_o - \hat{f}(x_0)\big)^2 = \text{Var}(\hat{f}(x_0)) + <span class="co">[</span><span class="ot">\text{Bias}(\hat{f}(x_0))</span><span class="co">]</span>^2 + \text{Var}(\epsilon)</span>
<span id="cb29-203"><a href="#cb29-203" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-204"><a href="#cb29-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-205"><a href="#cb29-205" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ??? proof of this --&gt;</span></span>
<span id="cb29-206"><a href="#cb29-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-207"><a href="#cb29-207" aria-hidden="true" tabindex="-1"></a>Here, the notation $E\big(y_o - \hat{f}(x_0)\big)^2$ defines the *expected test MSE* at $x_0$, and refers to the average test MSE that we would obtain if we repeatedly estimated $f$ using a large number of training sets, and tested each at $x_0$. The overall expected test MSE can be computed by averaging $E\big(y_o - \hat{f}(x_0)\big)^2$ over all possible values of $x_0$ in the test set.</span>
<span id="cb29-208"><a href="#cb29-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-209"><a href="#cb29-209" aria-hidden="true" tabindex="-1"></a>This equation tells us that in order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves *low variance* and *low bias*. Note that variance is inherently a nonnegative quantity, and squared bias is also nonnegative. Hence, we see that the expected test MSE can never lie below $\text{Var}(\epsilon)$, the irreducible error.</span>
<span id="cb29-210"><a href="#cb29-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-211"><a href="#cb29-211" aria-hidden="true" tabindex="-1"></a>What do we mean by the *variance* and *bias* of a statistical learning method? *Variance* refers to the amount by which $\hat{f}$ would change if we estimated it using a different training data set. Since the training data are used to fit the statistical learning method, different training data sets will result in a different $\hat{f}$. But ideally the estimate for f should not vary</span>
<span id="cb29-212"><a href="#cb29-212" aria-hidden="true" tabindex="-1"></a>too much between training sets. However, if a method has high variance then small changes in the training data can result in large changes in $\hat{f}$. **In general, more flexible statistical methods have higher variance**.</span>
<span id="cb29-213"><a href="#cb29-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-214"><a href="#cb29-214" aria-hidden="true" tabindex="-1"></a>On the other hand, *bias* refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. **Generally, more flexible methods result in less bias.**</span>
<span id="cb29-215"><a href="#cb29-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-216"><a href="#cb29-216" aria-hidden="true" tabindex="-1"></a>**As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease.** The relative rate of change of these two quantities determines whether the test MSE increases or decreases. As we increase the flexibility of a class of methods, the bias tends to initially decrease faster than the variance increases. Consequently, the expected test MSE declines. However, at some point increasing flexibility has little impact on the bias but starts to significantly increase the variance. When this happens the test MSE increases. For the previous examples, here are the variance, bias curves:</span>
<span id="cb29-217"><a href="#cb29-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-218"><a href="#cb29-218" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-variance-bias.png)</span>{width="50%"}</span>
<span id="cb29-219"><a href="#cb29-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-220"><a href="#cb29-220" aria-hidden="true" tabindex="-1"></a>This shows that variance is an increasing function of flexibility, and bias is a decreasing function. The goal is to find the balance point. This trade-off is one of the most important recurring themes in this book.</span>
<span id="cb29-221"><a href="#cb29-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-222"><a href="#cb29-222" aria-hidden="true" tabindex="-1"></a>**In a real-life situation in which $f$ is unobserved, it is generally not possible to explicitly compute the test MSE, bias, or variance for a statistical learning method. Nevertheless, one should always keep the bias-variance trade-off in mind.** In this book we explore methods that are extremely flexible and hence can essentially eliminate bias. However, this does not guarantee that they will outperform a much simpler method such as linear regression. To take an extreme example, suppose that the true $f$ is linear. In this situation linear regression will have no bias, making it very hard for a more flexible method to compete. In contrast, if the true $f$ is highly non-linear and we have an ample number of training observations, then we may do better using a highly flexible approach. Later, we discuss cross-validation, which is a way to estimate the test MSE using the training data.</span>
<span id="cb29-223"><a href="#cb29-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-224"><a href="#cb29-224" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The classification setting</span></span>
<span id="cb29-225"><a href="#cb29-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-226"><a href="#cb29-226" aria-hidden="true" tabindex="-1"></a>Thus far, our discussion of model accuracy has been focused on the regression setting. But many of the concepts that we have encountered, such as the bias-variance trade-off, transfer over to the classification setting with only some modifications due to the fact that $y_i$ is no longer quantitative. Suppose that we seek to estimate $f$ on the basis of training observations $<span class="sc">\{</span>(x_1,y_1), \ldots, (x_n,y_n)<span class="sc">\}</span>$, where now $y_1, \ldots , y_n$ are qualitative. The most common approach for quantifying the accuracy of our estimate $\hat{f}$ is the *training error rate*, the proportion of mistakes that are made if we apply our estimate $\hat{f}$ to the training observations:</span>
<span id="cb29-227"><a href="#cb29-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-228"><a href="#cb29-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-229"><a href="#cb29-229" aria-hidden="true" tabindex="-1"></a>\frac{1}{n} \sum_{i = 1}^n I(y_i \ne \hat{y}_i)</span>
<span id="cb29-230"><a href="#cb29-230" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-231"><a href="#cb29-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-232"><a href="#cb29-232" aria-hidden="true" tabindex="-1"></a>Here $\hat{y}_i$ is the predicted class label for the $i$th observation using $\hat{f}$. And</span>
<span id="cb29-233"><a href="#cb29-233" aria-hidden="true" tabindex="-1"></a>$I(y_i \ne \hat{y}_i))$ is an indicator variable that equals 1 if $y_i \ne \hat{y}_i$ and zero if $y_i = \hat{y}_i$ (i.e. indicator if wrong). Thus, the above equation computes the fraction of incorrect classifications.</span>
<span id="cb29-234"><a href="#cb29-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-235"><a href="#cb29-235" aria-hidden="true" tabindex="-1"></a>The *test error rate* associated with a set of test observations of the form $(x_0, y_0)$ is given by</span>
<span id="cb29-236"><a href="#cb29-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-237"><a href="#cb29-237" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-238"><a href="#cb29-238" aria-hidden="true" tabindex="-1"></a>\text{Ave}(I(y_0 \ne \hat{y}_0))</span>
<span id="cb29-239"><a href="#cb29-239" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-240"><a href="#cb29-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-241"><a href="#cb29-241" aria-hidden="true" tabindex="-1"></a>A good classifier is one for which the test error is smallest.</span>
<span id="cb29-242"><a href="#cb29-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-243"><a href="#cb29-243" aria-hidden="true" tabindex="-1"></a>**The Bayes classifier**</span>
<span id="cb29-244"><a href="#cb29-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-245"><a href="#cb29-245" aria-hidden="true" tabindex="-1"></a>It is possible to show (though the proof is outside of the scope of this book) that the test error rate given in is minimized, on average, by a very simple classifier that *assigns each observation to the most likely class, given its predictor values*. In other words, we should simply assign a test observation with predictor vector $x_0$ to the class $j$ for which</span>
<span id="cb29-246"><a href="#cb29-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-247"><a href="#cb29-247" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-248"><a href="#cb29-248" aria-hidden="true" tabindex="-1"></a>\text{Pr}(Y = j \mid X = x_0)</span>
<span id="cb29-249"><a href="#cb29-249" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-250"><a href="#cb29-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-251"><a href="#cb29-251" aria-hidden="true" tabindex="-1"></a>is the largest. Note that the above formula is a *conditional probability*: it is the probability that $Y = j$, given the observed predictor vector $x_0$. This very simple classifier is called the *Bayes classifier*. In a two-class problem where there are only two possible response values, say class 1 or class 2, the Bayes classifier corresponds to predicting class one if $\text{Pr}(Y = j \mid X = x_0) &gt; 0.5$, and class two otherwise.</span>
<span id="cb29-252"><a href="#cb29-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-253"><a href="#cb29-253" aria-hidden="true" tabindex="-1"></a>Here is an example:</span>
<span id="cb29-254"><a href="#cb29-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-255"><a href="#cb29-255" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-bayes-classifier.png)</span>{width="50%"}</span>
<span id="cb29-256"><a href="#cb29-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-257"><a href="#cb29-257" aria-hidden="true" tabindex="-1"></a>For each value of $X_1$ and $X_2$, there is a different probability of the response being orange or blue. Since this is simulated data, we know how the data were generated and we can calculate the conditional probabilities for each value of $X_1$ and $X_2$. The purple dashed line represents the points where the probability is exactly 50%. This is called the *Bayes decision boundary*.</span>
<span id="cb29-258"><a href="#cb29-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-259"><a href="#cb29-259" aria-hidden="true" tabindex="-1"></a>The Bayes classifier produces the lowest possible test error rate, called the *Bayes error rate*. Since the Bayes classifier will always choose the class for which the above formula is largest, the error rate will be $1 − \text{max}_j \, \text{Pr}(Y = j \mid X = x_0)$ at $X = x_0$. In general, the overall Bayes error rate is given by</span>
<span id="cb29-260"><a href="#cb29-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-261"><a href="#cb29-261" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-262"><a href="#cb29-262" aria-hidden="true" tabindex="-1"></a>1 - E\big(\underset{j}{\text{max}} \, \text{Pr}(Y = j \mid X)\big)</span>
<span id="cb29-263"><a href="#cb29-263" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-264"><a href="#cb29-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-265"><a href="#cb29-265" aria-hidden="true" tabindex="-1"></a>where the expectation averages the probability over all possible values of $X$. **The Bayes error rate is analogous to the irreducible error, discussed earlier.**</span>
<span id="cb29-266"><a href="#cb29-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-267"><a href="#cb29-267" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ??? how does this work --&gt;</span></span>
<span id="cb29-268"><a href="#cb29-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-269"><a href="#cb29-269" aria-hidden="true" tabindex="-1"></a>**$K$-Nearest neighbors**</span>
<span id="cb29-270"><a href="#cb29-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-271"><a href="#cb29-271" aria-hidden="true" tabindex="-1"></a>In theory we would always like to predict qualitative responses using the Bayes classifier. But for real data, we do not know the conditional distribution of $Y$ given $X$, and so computing the Bayes classifier is impossible. Therefore, the Bayes classifier serves as an unattainable gold standard against which to compare other methods.</span>
<span id="cb29-272"><a href="#cb29-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-273"><a href="#cb29-273" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ??? why impossible --&gt;</span></span>
<span id="cb29-274"><a href="#cb29-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-275"><a href="#cb29-275" aria-hidden="true" tabindex="-1"></a>Many approaches attempt to estimate the conditional distribution of $Y$ given $X$, and then classify a given observation to the class with highest *estimated* probability. One such method is the *K-nearest neighbors* (KNN) classifier. Given a positive integer $K$ and a test observation $x_0$, the KNN classifier first identifies the $K$ points in the training data that are closest to $x_0$, represented by $\cal{N}_0$. It then estimates the conditional probability for class $j$ as the fraction of points in $\cal{N}_0$ whose response values equal $j$:</span>
<span id="cb29-276"><a href="#cb29-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-277"><a href="#cb29-277" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-278"><a href="#cb29-278" aria-hidden="true" tabindex="-1"></a>\text{Pr}(Y = j \mid X = x_0) = \frac{1}{K}\sum_{i \in \cal{N}_0} I(y_i = j)</span>
<span id="cb29-279"><a href="#cb29-279" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb29-280"><a href="#cb29-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-281"><a href="#cb29-281" aria-hidden="true" tabindex="-1"></a>Finally, KNN classifies the test observation $x_0$ to the class with the largest probability from the equation above.</span>
<span id="cb29-282"><a href="#cb29-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-283"><a href="#cb29-283" aria-hidden="true" tabindex="-1"></a>Here is an example of the KNN approach:</span>
<span id="cb29-284"><a href="#cb29-284" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-285"><a href="#cb29-285" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-knn.png)</span>{width="50%"}</span>
<span id="cb29-286"><a href="#cb29-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-287"><a href="#cb29-287" aria-hidden="true" tabindex="-1"></a>Despite the fact that it is a very simple approach, KNN can often produce classifiers that are surprisingly close to the optimal Bayes classifier. Note that even though the true distribution is not known by the KNN classifier, the KNN decision boundary is very close to that of the Bayes classifier.</span>
<span id="cb29-288"><a href="#cb29-288" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-289"><a href="#cb29-289" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-knn-2.png)</span>{width="50%"}</span>
<span id="cb29-290"><a href="#cb29-290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-291"><a href="#cb29-291" aria-hidden="true" tabindex="-1"></a>However, the choice of $K$ has a drastic effect on the KNN classifier obtained.</span>
<span id="cb29-292"><a href="#cb29-292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-293"><a href="#cb29-293" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-knn-3.png)</span>{width="50%"}</span>
<span id="cb29-294"><a href="#cb29-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-295"><a href="#cb29-295" aria-hidden="true" tabindex="-1"></a>When $K = 1$, the decision boundary is overly flexible and finds patterns in the data that don’t correspond to the Bayes decision boundary. This corresponds to a classifier that has low bias but very high variance. As $K$ grows, the method becomes less flexible and produces a decision boundary that is close to linear. This corresponds to a low-variance but high-bias classifier.</span>
<span id="cb29-296"><a href="#cb29-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-297"><a href="#cb29-297" aria-hidden="true" tabindex="-1"></a>Just as in the regression setting, there is not a strong relationship between the training error rate and the test error rate. With $K = 1$, the KNN training error rate is 0, but the test error rate may be quite high. **In general, as we use more flexible classification methods, the training error rate will decline but the test error rate may not.**</span>
<span id="cb29-298"><a href="#cb29-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-299"><a href="#cb29-299" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-knn-results.png)</span>{width="50%"}</span>
<span id="cb29-300"><a href="#cb29-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-301"><a href="#cb29-301" aria-hidden="true" tabindex="-1"></a>As $1/K$ increases, the method becomes more flexible. As in the regression setting, the training error rate consistently declines as the flexibility increases. However, the test error exhibits a characteristic U-shape, declining at first (with a minimum at approximately $K = 10$) before increasing again when the method becomes excessively flexible and overfits.</span>
<span id="cb29-302"><a href="#cb29-302" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-303"><a href="#cb29-303" aria-hidden="true" tabindex="-1"></a>**In both the regression and classification settings, choosing the correct level of flexibility is critical to the success of any statistical learning method. The bias-variance trade-off, and the resulting U-shape in the test error, can make this a difficult task.**</span>
<span id="cb29-304"><a href="#cb29-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-305"><a href="#cb29-305" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lab</span></span>
<span id="cb29-306"><a href="#cb29-306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-307"><a href="#cb29-307" aria-hidden="true" tabindex="-1"></a>&lt; just basic R commands &gt;</span>
<span id="cb29-308"><a href="#cb29-308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-309"><a href="#cb29-309" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb29-310"><a href="#cb29-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-311"><a href="#cb29-311" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conceptual</span></span>
<span id="cb29-312"><a href="#cb29-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-313"><a href="#cb29-313" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 1</span></span>
<span id="cb29-314"><a href="#cb29-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-315"><a href="#cb29-315" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.</span></span>
<span id="cb29-316"><a href="#cb29-316" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb29-317"><a href="#cb29-317" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a. The sample size $n$ is extremely large, and the number of predictors $p$ is small.</span></span>
<span id="cb29-318"><a href="#cb29-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-319"><a href="#cb29-319" aria-hidden="true" tabindex="-1"></a>A flexible model will perform better than an inflexible model because there are ample degrees of freedom to estimate many parameters and still have many leftover for error df.</span>
<span id="cb29-320"><a href="#cb29-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-321"><a href="#cb29-321" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; b. The number of predictors $p$ is extremely large, and the number of observations $n$ is small.</span></span>
<span id="cb29-322"><a href="#cb29-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-323"><a href="#cb29-323" aria-hidden="true" tabindex="-1"></a>An inflexible will be better because there is a high chance of some predictors being randomly associated.</span>
<span id="cb29-324"><a href="#cb29-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-325"><a href="#cb29-325" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; c. The relationship between the predictors and response is highly non-linear.</span></span>
<span id="cb29-326"><a href="#cb29-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-327"><a href="#cb29-327" aria-hidden="true" tabindex="-1"></a>A flexible model will perform better because it can pick up on the non-linear trends better.</span>
<span id="cb29-328"><a href="#cb29-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-329"><a href="#cb29-329" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; d. The variance of the error terms, i.e. $\sigma^2 = \text{Var}(\epsilon)$, is extremely high.</span></span>
<span id="cb29-330"><a href="#cb29-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-331"><a href="#cb29-331" aria-hidden="true" tabindex="-1"></a>Inflexible model will be better because the flexible model will pick up on the noise a lot better even though it is not related to the relationship to the $X$s.</span>
<span id="cb29-332"><a href="#cb29-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-333"><a href="#cb29-333" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 2</span></span>
<span id="cb29-334"><a href="#cb29-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-335"><a href="#cb29-335" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide $n$ and $p$.</span></span>
<span id="cb29-336"><a href="#cb29-336" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb29-337"><a href="#cb29-337" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a. We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.</span></span>
<span id="cb29-338"><a href="#cb29-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-339"><a href="#cb29-339" aria-hidden="true" tabindex="-1"></a>Regression; inference; $n = 500, p = 3$</span>
<span id="cb29-340"><a href="#cb29-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-341"><a href="#cb29-341" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; b. We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.</span></span>
<span id="cb29-342"><a href="#cb29-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-343"><a href="#cb29-343" aria-hidden="true" tabindex="-1"></a>Classification; prediction; $n = 20, p = 13$</span>
<span id="cb29-344"><a href="#cb29-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-345"><a href="#cb29-345" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; c. We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.</span></span>
<span id="cb29-346"><a href="#cb29-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-347"><a href="#cb29-347" aria-hidden="true" tabindex="-1"></a>Regression; prediction; $n = 52, p = 3$.</span>
<span id="cb29-348"><a href="#cb29-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-349"><a href="#cb29-349" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 3</span></span>
<span id="cb29-350"><a href="#cb29-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-351"><a href="#cb29-351" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We now revisit the bias-variance decomposition.</span></span>
<span id="cb29-352"><a href="#cb29-352" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb29-353"><a href="#cb29-353" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a. Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.</span></span>
<span id="cb29-354"><a href="#cb29-354" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-355"><a href="#cb29-355" aria-hidden="true" tabindex="-1"></a>&lt; !!! get from ipad &gt;</span>
<span id="cb29-356"><a href="#cb29-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-357"><a href="#cb29-357" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/2-ex-3-sol.png)</span>{width="50%"}</span>
<span id="cb29-358"><a href="#cb29-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-359"><a href="#cb29-359" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; b. Explain why each of the five curves has the shape displayed in part (a).</span></span>
<span id="cb29-360"><a href="#cb29-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-361"><a href="#cb29-361" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bias: Decreases until leveling off as flexibility increases (nears zero)</span>
<span id="cb29-362"><a href="#cb29-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-363"><a href="#cb29-363" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Variance: Increases faster as flexibility increases</span>
<span id="cb29-364"><a href="#cb29-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-365"><a href="#cb29-365" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bayes (irreducible error): Constant, unrelated to flexibility ($X$)</span>
<span id="cb29-366"><a href="#cb29-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-367"><a href="#cb29-367" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Training error rate: Decrease monotonically as flexibility increases</span>
<span id="cb29-368"><a href="#cb29-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-369"><a href="#cb29-369" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Testing error rate: U-shaped with minimum where bias and variance are simultaneously minimized</span>
<span id="cb29-370"><a href="#cb29-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-371"><a href="#cb29-371" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 4</span></span>
<span id="cb29-372"><a href="#cb29-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-373"><a href="#cb29-373" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; You will now think of some real-life applications for statistical learning.</span></span>
<span id="cb29-374"><a href="#cb29-374" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;</span></span>
<span id="cb29-375"><a href="#cb29-375" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a. Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.</span></span>
<span id="cb29-376"><a href="#cb29-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-377"><a href="#cb29-377" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Insurance, predict whether a policyholder will have a claim or not. Response is binary, predictors are variables such as credit score, age of business, type of business, etc. Goal is prediction, want to see if a new customer is likely to have a claim.</span>
<span id="cb29-378"><a href="#cb29-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-379"><a href="#cb29-379" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>&lt; good enough &gt;</span>
<span id="cb29-380"><a href="#cb29-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-381"><a href="#cb29-381" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; b. Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.</span></span>
<span id="cb29-382"><a href="#cb29-382" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-383"><a href="#cb29-383" aria-hidden="true" tabindex="-1"></a>Insurance, predict the amount of a claim. Response is claim size in dollars, same predictors as in a.</span>
<span id="cb29-384"><a href="#cb29-384" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-385"><a href="#cb29-385" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; c. Describe three real-life applications in which cluster analysis might be useful.</span></span>
<span id="cb29-386"><a href="#cb29-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-387"><a href="#cb29-387" aria-hidden="true" tabindex="-1"></a>Insurance, determining if a business is low, medium or high risk. Same predictors as a.</span>
<span id="cb29-388"><a href="#cb29-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-389"><a href="#cb29-389" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 5</span></span>
<span id="cb29-390"><a href="#cb29-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-391"><a href="#cb29-391" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?</span></span>
<span id="cb29-392"><a href="#cb29-392" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-393"><a href="#cb29-393" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Flexible models can pick up on non-linear patterns in regression and are more useful in prediction when we are only concerned with accuracy, especially with large $n$; and they are less biased, however more variable. A less flexible approach is better for inference when we want to understand the relationship(s) between $Y$ and $X$; more bias, but less variable.</span>
<span id="cb29-394"><a href="#cb29-394" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-395"><a href="#cb29-395" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 6</span></span>
<span id="cb29-396"><a href="#cb29-396" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-397"><a href="#cb29-397" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a parametric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?</span></span>
<span id="cb29-398"><a href="#cb29-398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-399"><a href="#cb29-399" aria-hidden="true" tabindex="-1"></a>Parametric approach assumes a functional form of the response and one needs to only estimate parameters, which is a simpler task. And because of the assumption, the model is more interpretable. However a non-parametric method just fits the data as best as possible. Non-parametric methods can be less biased, but are more variable, especially as the complexity increases and more data is needed because of the more parameters being estimated.</span>
<span id="cb29-400"><a href="#cb29-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-401"><a href="#cb29-401" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 7</span></span>
<span id="cb29-402"><a href="#cb29-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-403"><a href="#cb29-403" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.</span></span>
<span id="cb29-404"><a href="#cb29-404" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-405"><a href="#cb29-405" aria-hidden="true" tabindex="-1"></a> <span class="al">![](files/images/2-ex-7-table.png)</span>{width="50%"}</span>
<span id="cb29-406"><a href="#cb29-406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-407"><a href="#cb29-407" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-408"><a href="#cb29-408" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Suppose we wish to use this data set to make a prediction for $Y$ when $X_1 = X_2 = X_3 = 0$ using $K$-nearest neighbors.</span></span>
<span id="cb29-409"><a href="#cb29-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-410"><a href="#cb29-410" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a. Compute the Euclidean distance between each observation and the test point, $X_1 = X_2 = X_3 = 0$.</span></span>
<span id="cb29-411"><a href="#cb29-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-414"><a href="#cb29-414" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb29-415"><a href="#cb29-415" aria-hidden="true" tabindex="-1"></a><span class="co"># read in data</span></span>
<span id="cb29-416"><a href="#cb29-416" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb29-417"><a href="#cb29-417" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb29-418"><a href="#cb29-418" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb29-419"><a href="#cb29-419" aria-hidden="true" tabindex="-1"></a>  <span class="at">x3 =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>),</span>
<span id="cb29-420"><a href="#cb29-420" aria-hidden="true" tabindex="-1"></a>  <span class="at">y =</span> <span class="fu">c</span>(<span class="st">"Red"</span>, <span class="st">"Red"</span>, <span class="st">"Red"</span>, <span class="st">"Green"</span>, <span class="st">"Green"</span>, <span class="st">"Red"</span>)</span>
<span id="cb29-421"><a href="#cb29-421" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-422"><a href="#cb29-422" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-423"><a href="#cb29-423" aria-hidden="true" tabindex="-1"></a><span class="co"># define function to calculate euclidean distance </span></span>
<span id="cb29-424"><a href="#cb29-424" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; sum function calculates the sum of the squares of absolute difference between  corresponding elements of vec_1 and vec_2 </span></span>
<span id="cb29-425"><a href="#cb29-425" aria-hidden="true" tabindex="-1"></a>calc_euc_dist <span class="ot">&lt;-</span> <span class="cf">function</span>(vec_1, vec_2) {</span>
<span id="cb29-426"><a href="#cb29-426" aria-hidden="true" tabindex="-1"></a>  <span class="fu">subtract</span>(vec_1, vec_2) <span class="sc">%&gt;%</span> <span class="fu">raise_to_power</span>(<span class="dv">2</span>) <span class="sc">%&gt;%</span> sum <span class="sc">%&gt;%</span> sqrt</span>
<span id="cb29-427"><a href="#cb29-427" aria-hidden="true" tabindex="-1"></a>} </span>
<span id="cb29-428"><a href="#cb29-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-429"><a href="#cb29-429" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate euclidean distance for each observation</span></span>
<span id="cb29-430"><a href="#cb29-430" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span></span>
<span id="cb29-431"><a href="#cb29-431" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rowwise</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb29-432"><a href="#cb29-432" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">euc_dist =</span> <span class="fu">calc_euc_dist</span>(<span class="fu">c</span>(x1, x2, x3), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>)),</span>
<span id="cb29-433"><a href="#cb29-433" aria-hidden="true" tabindex="-1"></a>         <span class="at">.keep =</span> <span class="st">"none"</span>)</span>
<span id="cb29-434"><a href="#cb29-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-435"><a href="#cb29-435" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-436"><a href="#cb29-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-437"><a href="#cb29-437" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; b. What is our prediction with K = 1? Why?</span></span>
<span id="cb29-438"><a href="#cb29-438" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-441"><a href="#cb29-441" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb29-442"><a href="#cb29-442" aria-hidden="true" tabindex="-1"></a>df <span class="sc">%&gt;%</span></span>
<span id="cb29-443"><a href="#cb29-443" aria-hidden="true" tabindex="-1"></a>  <span class="fu">rowwise</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb29-444"><a href="#cb29-444" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">euc_dist =</span> <span class="fu">calc_euc_dist</span>(<span class="fu">c</span>(x1, x2, x3), <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>)),</span>
<span id="cb29-445"><a href="#cb29-445" aria-hidden="true" tabindex="-1"></a>         <span class="at">.keep =</span> <span class="st">"unused"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb29-446"><a href="#cb29-446" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ungroup</span>() <span class="sc">%&gt;%</span> </span>
<span id="cb29-447"><a href="#cb29-447" aria-hidden="true" tabindex="-1"></a>  <span class="fu">slice_min</span>(<span class="at">order_by =</span> euc_dist, <span class="at">n =</span> <span class="dv">1</span>)</span>
<span id="cb29-448"><a href="#cb29-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-449"><a href="#cb29-449" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-450"><a href="#cb29-450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-451"><a href="#cb29-451" aria-hidden="true" tabindex="-1"></a>Green is our prediction</span>
<span id="cb29-452"><a href="#cb29-452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-453"><a href="#cb29-453" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; c. What is our prediction with K = 3? Why?</span></span>
<span id="cb29-454"><a href="#cb29-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-455"><a href="#cb29-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-458"><a href="#cb29-458" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb29-459"><a href="#cb29-459" aria-hidden="true" tabindex="-1"></a><span class="co"># turn above into a function</span></span>
<span id="cb29-460"><a href="#cb29-460" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-461"><a href="#cb29-461" aria-hidden="true" tabindex="-1"></a><span class="co"># define function to do KNN prediction for a particular test point</span></span>
<span id="cb29-462"><a href="#cb29-462" aria-hidden="true" tabindex="-1"></a>algorithm_knn <span class="ot">&lt;-</span> <span class="cf">function</span>(df, y, x0, <span class="at">k =</span> <span class="dv">1</span>, <span class="at">final_result =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb29-463"><a href="#cb29-463" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb29-464"><a href="#cb29-464" aria-hidden="true" tabindex="-1"></a>  <span class="co"># get column names</span></span>
<span id="cb29-465"><a href="#cb29-465" aria-hidden="true" tabindex="-1"></a>  nms <span class="ot">=</span> df <span class="sc">%&gt;%</span> </span>
<span id="cb29-466"><a href="#cb29-466" aria-hidden="true" tabindex="-1"></a>    colnames</span>
<span id="cb29-467"><a href="#cb29-467" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-468"><a href="#cb29-468" aria-hidden="true" tabindex="-1"></a>  <span class="co"># standardized column names</span></span>
<span id="cb29-469"><a href="#cb29-469" aria-hidden="true" tabindex="-1"></a>  <span class="co"># -&gt; place response variable first and renamed</span></span>
<span id="cb29-470"><a href="#cb29-470" aria-hidden="true" tabindex="-1"></a>  <span class="co"># -&gt; followed by explanatory variables renamed x1, x2, ... (unnecessary)</span></span>
<span id="cb29-471"><a href="#cb29-471" aria-hidden="true" tabindex="-1"></a>  df_z <span class="ot">&lt;-</span> df <span class="sc">%&gt;%</span> </span>
<span id="cb29-472"><a href="#cb29-472" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(<span class="at">y =</span> <span class="fu">any_of</span>(y), <span class="fu">any_of</span>(nms[<span class="fu">which</span>(nms <span class="sc">!=</span> y)])) <span class="co">#%&gt;% </span></span>
<span id="cb29-473"><a href="#cb29-473" aria-hidden="true" tabindex="-1"></a>    <span class="co">#rename_with(~ paste0("x", 1:(ncol(df)-1)), .cols = any_of(nms[which(nms != y)]))</span></span>
<span id="cb29-474"><a href="#cb29-474" aria-hidden="true" tabindex="-1"></a>   </span>
<span id="cb29-475"><a href="#cb29-475" aria-hidden="true" tabindex="-1"></a>  <span class="co"># calculate euclidean distances</span></span>
<span id="cb29-476"><a href="#cb29-476" aria-hidden="true" tabindex="-1"></a>  euc_dist <span class="ot">=</span> <span class="fu">c</span>()</span>
<span id="cb29-477"><a href="#cb29-477" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df_z)) {</span>
<span id="cb29-478"><a href="#cb29-478" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-479"><a href="#cb29-479" aria-hidden="true" tabindex="-1"></a>    euc_dist[i] <span class="ot">=</span> <span class="fu">calc_euc_dist</span>(<span class="fu">as.vector</span>(df_z[i, <span class="dv">2</span><span class="sc">:</span><span class="fu">ncol</span>(df_z)], <span class="at">mode =</span> <span class="st">"double"</span>), x0) </span>
<span id="cb29-480"><a href="#cb29-480" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-481"><a href="#cb29-481" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb29-482"><a href="#cb29-482" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-483"><a href="#cb29-483" aria-hidden="true" tabindex="-1"></a>  <span class="co"># create dataframe of results</span></span>
<span id="cb29-484"><a href="#cb29-484" aria-hidden="true" tabindex="-1"></a>  df_dist <span class="ot">=</span> df_z <span class="sc">%&gt;%</span> </span>
<span id="cb29-485"><a href="#cb29-485" aria-hidden="true" tabindex="-1"></a>    <span class="fu">select</span>(y) <span class="sc">%&gt;%</span> </span>
<span id="cb29-486"><a href="#cb29-486" aria-hidden="true" tabindex="-1"></a>    <span class="fu">bind_cols</span>(<span class="at">euc_dist =</span> euc_dist)</span>
<span id="cb29-487"><a href="#cb29-487" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-488"><a href="#cb29-488" aria-hidden="true" tabindex="-1"></a>  <span class="co"># display results</span></span>
<span id="cb29-489"><a href="#cb29-489" aria-hidden="true" tabindex="-1"></a>  results <span class="ot">=</span> df_dist <span class="sc">%&gt;%</span> </span>
<span id="cb29-490"><a href="#cb29-490" aria-hidden="true" tabindex="-1"></a>    <span class="fu">slice_min</span>(<span class="at">order_by =</span> euc_dist, <span class="at">n =</span> k) <span class="sc">%&gt;%</span> </span>
<span id="cb29-491"><a href="#cb29-491" aria-hidden="true" tabindex="-1"></a>    <span class="fu">summarize</span>(<span class="at">.by =</span> y, </span>
<span id="cb29-492"><a href="#cb29-492" aria-hidden="true" tabindex="-1"></a>              <span class="at">est_prob =</span> <span class="fu">n</span>() <span class="sc">/</span> <span class="fu">nrow</span>(.))</span>
<span id="cb29-493"><a href="#cb29-493" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-494"><a href="#cb29-494" aria-hidden="true" tabindex="-1"></a>  <span class="co"># conditionally simplify results</span></span>
<span id="cb29-495"><a href="#cb29-495" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(final_result) {</span>
<span id="cb29-496"><a href="#cb29-496" aria-hidden="true" tabindex="-1"></a>    results <span class="sc">%&lt;&gt;%</span> </span>
<span id="cb29-497"><a href="#cb29-497" aria-hidden="true" tabindex="-1"></a>      <span class="fu">slice_min</span>(<span class="at">order_by =</span> est_prob) <span class="sc">%&gt;%</span> </span>
<span id="cb29-498"><a href="#cb29-498" aria-hidden="true" tabindex="-1"></a>      <span class="fu">select</span>(y) <span class="sc">%&gt;%</span> </span>
<span id="cb29-499"><a href="#cb29-499" aria-hidden="true" tabindex="-1"></a>      as.character</span>
<span id="cb29-500"><a href="#cb29-500" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb29-501"><a href="#cb29-501" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-502"><a href="#cb29-502" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(results)</span>
<span id="cb29-503"><a href="#cb29-503" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb29-504"><a href="#cb29-504" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb29-505"><a href="#cb29-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-506"><a href="#cb29-506" aria-hidden="true" tabindex="-1"></a><span class="co"># test function on K = 1</span></span>
<span id="cb29-507"><a href="#cb29-507" aria-hidden="true" tabindex="-1"></a><span class="fu">algorithm_knn</span>(df, <span class="at">y =</span> <span class="st">"y"</span>, <span class="at">x0 =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), <span class="at">k =</span> <span class="dv">1</span>, <span class="at">final_result =</span> <span class="cn">FALSE</span>)</span>
<span id="cb29-508"><a href="#cb29-508" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-509"><a href="#cb29-509" aria-hidden="true" tabindex="-1"></a><span class="co"># now for K = 3</span></span>
<span id="cb29-510"><a href="#cb29-510" aria-hidden="true" tabindex="-1"></a><span class="fu">algorithm_knn</span>(df, <span class="at">y =</span> <span class="st">"y"</span>, <span class="at">x0 =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), <span class="at">k =</span> <span class="dv">3</span>, <span class="at">final_result =</span> <span class="cn">FALSE</span>)</span>
<span id="cb29-511"><a href="#cb29-511" aria-hidden="true" tabindex="-1"></a><span class="fu">algorithm_knn</span>(df, <span class="at">y =</span> <span class="st">"y"</span>, <span class="at">x0 =</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">3</span>), <span class="at">k =</span> <span class="dv">3</span>, <span class="at">final_result =</span> <span class="cn">TRUE</span>)</span>
<span id="cb29-512"><a href="#cb29-512" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-513"><a href="#cb29-513" aria-hidden="true" tabindex="-1"></a><span class="co"># test a set of points</span></span>
<span id="cb29-514"><a href="#cb29-514" aria-hidden="true" tabindex="-1"></a>x0 <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb29-515"><a href="#cb29-515" aria-hidden="true" tabindex="-1"></a>  <span class="at">x1 =</span> extraDistr<span class="sc">::</span><span class="fu">rdunif</span>(<span class="at">n =</span> <span class="dv">5</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">3</span>),</span>
<span id="cb29-516"><a href="#cb29-516" aria-hidden="true" tabindex="-1"></a>  <span class="at">x2 =</span> extraDistr<span class="sc">::</span><span class="fu">rdunif</span>(<span class="at">n =</span> <span class="dv">5</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">3</span>),</span>
<span id="cb29-517"><a href="#cb29-517" aria-hidden="true" tabindex="-1"></a>  <span class="at">x3 =</span> extraDistr<span class="sc">::</span><span class="fu">rdunif</span>(<span class="at">n =</span> <span class="dv">5</span>, <span class="at">min =</span> <span class="dv">0</span>, <span class="at">max =</span> <span class="dv">3</span>)</span>
<span id="cb29-518"><a href="#cb29-518" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-519"><a href="#cb29-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-520"><a href="#cb29-520" aria-hidden="true" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">c</span>()</span>
<span id="cb29-521"><a href="#cb29-521" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(x0)) {</span>
<span id="cb29-522"><a href="#cb29-522" aria-hidden="true" tabindex="-1"></a>  results[i] <span class="ot">=</span> <span class="fu">algorithm_knn</span>(df, <span class="at">y =</span> <span class="st">"y"</span>, <span class="at">x0 =</span> x0[i,], <span class="at">k =</span> <span class="dv">3</span>, <span class="at">final_result =</span> <span class="cn">TRUE</span>)</span>
<span id="cb29-523"><a href="#cb29-523" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb29-524"><a href="#cb29-524" aria-hidden="true" tabindex="-1"></a>results</span>
<span id="cb29-525"><a href="#cb29-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-526"><a href="#cb29-526" aria-hidden="true" tabindex="-1"></a><span class="co"># note if results include a vector of two classes, then there was a tie. Not sure how to break ties, but could research and code something...</span></span>
<span id="cb29-527"><a href="#cb29-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-528"><a href="#cb29-528" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-529"><a href="#cb29-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-530"><a href="#cb29-530" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; d. If the Bayes decision boundary in this problem is highly non- linear, then would we expect the best value for K to be large or small? Why?</span></span>
<span id="cb29-531"><a href="#cb29-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-532"><a href="#cb29-532" aria-hidden="true" tabindex="-1"></a>Small so that the model is more flexible (high $k$ leads to linear boundaries due to averaging).</span>
<span id="cb29-533"><a href="#cb29-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-534"><a href="#cb29-534" aria-hidden="true" tabindex="-1"></a><span class="fu">### Applied</span></span>
<span id="cb29-535"><a href="#cb29-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-536"><a href="#cb29-536" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 8</span></span>
<span id="cb29-537"><a href="#cb29-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-540"><a href="#cb29-540" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb29-541"><a href="#cb29-541" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb29-542"><a href="#cb29-542" aria-hidden="true" tabindex="-1"></a>data_college <span class="ot">&lt;-</span> ISLR2<span class="sc">::</span>College</span>
<span id="cb29-543"><a href="#cb29-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-544"><a href="#cb29-544" aria-hidden="true" tabindex="-1"></a><span class="co"># summarize numeric variables</span></span>
<span id="cb29-545"><a href="#cb29-545" aria-hidden="true" tabindex="-1"></a>data_college <span class="sc">%&gt;%</span> </span>
<span id="cb29-546"><a href="#cb29-546" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span> </span>
<span id="cb29-547"><a href="#cb29-547" aria-hidden="true" tabindex="-1"></a>  summary</span>
<span id="cb29-548"><a href="#cb29-548" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-549"><a href="#cb29-549" aria-hidden="true" tabindex="-1"></a><span class="co"># scatterplot matrix</span></span>
<span id="cb29-550"><a href="#cb29-550" aria-hidden="true" tabindex="-1"></a>data_college[, <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>] <span class="sc">%&gt;%</span> pairs</span>
<span id="cb29-551"><a href="#cb29-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-552"><a href="#cb29-552" aria-hidden="true" tabindex="-1"></a><span class="co"># side-by-side boxplots</span></span>
<span id="cb29-553"><a href="#cb29-553" aria-hidden="true" tabindex="-1"></a><span class="fu">boxplot</span>(Outstate <span class="sc">~</span> Private, <span class="at">data =</span> data_college)</span>
<span id="cb29-554"><a href="#cb29-554" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-555"><a href="#cb29-555" aria-hidden="true" tabindex="-1"></a><span class="co"># create new variable and summarize</span></span>
<span id="cb29-556"><a href="#cb29-556" aria-hidden="true" tabindex="-1"></a>data_college <span class="sc">%&lt;&gt;%</span> </span>
<span id="cb29-557"><a href="#cb29-557" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Elite =</span> <span class="fu">case_when</span>(</span>
<span id="cb29-558"><a href="#cb29-558" aria-hidden="true" tabindex="-1"></a>    Top10perc <span class="sc">&gt;</span> <span class="dv">50</span> <span class="sc">~</span> <span class="st">"Yes"</span>,</span>
<span id="cb29-559"><a href="#cb29-559" aria-hidden="true" tabindex="-1"></a>    <span class="at">.default =</span> <span class="st">"No"</span></span>
<span id="cb29-560"><a href="#cb29-560" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb29-561"><a href="#cb29-561" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(data_college<span class="sc">$</span>Elite)</span>
<span id="cb29-562"><a href="#cb29-562" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-563"><a href="#cb29-563" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-564"><a href="#cb29-564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-565"><a href="#cb29-565" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 9</span></span>
<span id="cb29-566"><a href="#cb29-566" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-569"><a href="#cb29-569" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb29-570"><a href="#cb29-570" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb29-571"><a href="#cb29-571" aria-hidden="true" tabindex="-1"></a>data_auto <span class="ot">&lt;-</span> ISLR2<span class="sc">::</span>Auto </span>
<span id="cb29-572"><a href="#cb29-572" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-573"><a href="#cb29-573" aria-hidden="true" tabindex="-1"></a><span class="co"># determine variable types</span></span>
<span id="cb29-574"><a href="#cb29-574" aria-hidden="true" tabindex="-1"></a>data_auto <span class="sc">%&gt;%</span> <span class="fu">map_chr</span>(class)</span>
<span id="cb29-575"><a href="#cb29-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-576"><a href="#cb29-576" aria-hidden="true" tabindex="-1"></a><span class="co"># find range of each quantitative predictor</span></span>
<span id="cb29-577"><a href="#cb29-577" aria-hidden="true" tabindex="-1"></a>data_auto <span class="sc">%&gt;%</span> </span>
<span id="cb29-578"><a href="#cb29-578" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span> </span>
<span id="cb29-579"><a href="#cb29-579" aria-hidden="true" tabindex="-1"></a>  <span class="fu">reframe</span>(<span class="fu">across</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(.), range))</span>
<span id="cb29-580"><a href="#cb29-580" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-581"><a href="#cb29-581" aria-hidden="true" tabindex="-1"></a><span class="co"># find mean and st dev of each quantitative predictor</span></span>
<span id="cb29-582"><a href="#cb29-582" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; then format nicely</span></span>
<span id="cb29-583"><a href="#cb29-583" aria-hidden="true" tabindex="-1"></a>data_auto <span class="sc">%&gt;%</span> </span>
<span id="cb29-584"><a href="#cb29-584" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span> </span>
<span id="cb29-585"><a href="#cb29-585" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="fu">across</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">ncol</span>(.), <span class="fu">list</span>(<span class="at">mean =</span> mean, <span class="at">sd =</span> sd))) <span class="sc">%&gt;%</span> </span>
<span id="cb29-586"><a href="#cb29-586" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">everything</span>(),</span>
<span id="cb29-587"><a href="#cb29-587" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">"var"</span>,</span>
<span id="cb29-588"><a href="#cb29-588" aria-hidden="true" tabindex="-1"></a>               <span class="at">values_to =</span> <span class="st">"val"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb29-589"><a href="#cb29-589" aria-hidden="true" tabindex="-1"></a>  <span class="fu">separate_wider_delim</span>(<span class="at">cols =</span> var,</span>
<span id="cb29-590"><a href="#cb29-590" aria-hidden="true" tabindex="-1"></a>                       <span class="at">delim =</span> <span class="st">"_"</span>,</span>
<span id="cb29-591"><a href="#cb29-591" aria-hidden="true" tabindex="-1"></a>                       <span class="at">names =</span> <span class="fu">c</span>(<span class="st">"var"</span>, <span class="st">"fun"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb29-592"><a href="#cb29-592" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_wider</span>(<span class="at">names_from =</span> <span class="st">"fun"</span>,</span>
<span id="cb29-593"><a href="#cb29-593" aria-hidden="true" tabindex="-1"></a>              <span class="at">values_from =</span> <span class="st">"val"</span>)</span>
<span id="cb29-594"><a href="#cb29-594" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-595"><a href="#cb29-595" aria-hidden="true" tabindex="-1"></a><span class="co"># EDA</span></span>
<span id="cb29-596"><a href="#cb29-596" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; trying to predict mpg</span></span>
<span id="cb29-597"><a href="#cb29-597" aria-hidden="true" tabindex="-1"></a>data_auto <span class="sc">%&gt;%</span> </span>
<span id="cb29-598"><a href="#cb29-598" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span> </span>
<span id="cb29-599"><a href="#cb29-599" aria-hidden="true" tabindex="-1"></a>  pairs</span>
<span id="cb29-600"><a href="#cb29-600" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-601"><a href="#cb29-601" aria-hidden="true" tabindex="-1"></a><span class="co"># useful variables</span></span>
<span id="cb29-602"><a href="#cb29-602" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; all predictors appear to be correlated with the response (some have non-linear relationship), however there are several that are highly correlated among themselves</span></span>
<span id="cb29-603"><a href="#cb29-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-604"><a href="#cb29-604" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb29-605"><a href="#cb29-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-606"><a href="#cb29-606" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 10</span></span>
<span id="cb29-607"><a href="#cb29-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-610"><a href="#cb29-610" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb29-611"><a href="#cb29-611" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb29-612"><a href="#cb29-612" aria-hidden="true" tabindex="-1"></a>data_boston <span class="ot">&lt;-</span> ISLR2<span class="sc">::</span>Boston</span>
<span id="cb29-613"><a href="#cb29-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-614"><a href="#cb29-614" aria-hidden="true" tabindex="-1"></a><span class="co"># &lt; ... easy stuff ... &gt; </span></span>
<span id="cb29-615"><a href="#cb29-615" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-616"><a href="#cb29-616" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>