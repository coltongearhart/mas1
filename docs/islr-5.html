<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>15&nbsp; ISLR – Resampling methods – MAS I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./glm-1.html" rel="next">
<link href="./islr-4.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./partC-glm.html">Extended Linear Models</a></li><li class="breadcrumb-item"><a href="./islr-5.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">ISLR – Resampling methods</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><header id="title-block-header" class="quarto-title-block default page-columns page-full"><div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./partC-glm.html">Extended Linear Models</a></li><li class="breadcrumb-item"><a href="./islr-5.html"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">ISLR – Resampling methods</span></a></li></ol></nav>
      <div class="quarto-title-block"><div><h1 class="title">
<span class="chapter-number">15</span>&nbsp; <span class="chapter-title">ISLR – Resampling methods</span>
</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MAS I</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./partA-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-0.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">CA – Basics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">CA – Claim severity distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">CA – Insurance applications</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">CA – Tail properties of distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">CA – Poisson processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">CA – Reliability theory</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">CA – Discrete Markov chains</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">CA – Life contingencies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">CA – Simulation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-quizzes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">CA – Probability quizzes</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./partB-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">CA – All stats</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./partC-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Extended Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./islr-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">ISLR – Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./islr-3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">ISLR – Linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./islr-4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">ISLR – Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./islr-5.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">ISLR – Resampling methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">GLM – Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">GLM – Model fitting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">GLM – Exponential family and generalized linear models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">GLM – Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">GLM – Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-glm-4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">CA – ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-glm-7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">CA – Other linear regression approaches</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-glm-11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">CA – Generalized additive models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-glm-misc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">CA – Misc GLM</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-glm-quizzes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">CA – GLM quizzes</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part-review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Review</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./review-study-guides.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Review – Study guides</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#notes" id="toc-notes" class="nav-link active" data-scroll-target="#notes"><span class="header-section-number">15.1</span> Notes</a>
  <ul>
<li>
<a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation"><span class="header-section-number">15.1.1</span> Cross-validation</a>
  <ul class="collapse">
<li><a href="#the-validation-set-approach" id="toc-the-validation-set-approach" class="nav-link" data-scroll-target="#the-validation-set-approach">The validation set approach</a></li>
  <li><a href="#leave-one-out-cross-validation" id="toc-leave-one-out-cross-validation" class="nav-link" data-scroll-target="#leave-one-out-cross-validation">Leave-one-out-cross-validation</a></li>
  <li><a href="#k-fold-cross-validation" id="toc-k-fold-cross-validation" class="nav-link" data-scroll-target="#k-fold-cross-validation"><span class="math inline">\(k\)</span>-fold cross-validation</a></li>
  <li><a href="#bias-variance-trade-off-for-k-fold-cross-validation" id="toc-bias-variance-trade-off-for-k-fold-cross-validation" class="nav-link" data-scroll-target="#bias-variance-trade-off-for-k-fold-cross-validation">Bias-variance trade-off for <span class="math inline">\(k\)</span>-fold cross-validation</a></li>
  <li><a href="#cross-validation-on-classification-problems" id="toc-cross-validation-on-classification-problems" class="nav-link" data-scroll-target="#cross-validation-on-classification-problems">Cross-validation on classification problems</a></li>
  </ul>
</li>
  <li><a href="#the-bootstrap" id="toc-the-bootstrap" class="nav-link" data-scroll-target="#the-bootstrap"><span class="header-section-number">15.1.2</span> The bootstrap</a></li>
  </ul>
</li>
  <li>
<a href="#lab" id="toc-lab" class="nav-link" data-scroll-target="#lab"><span class="header-section-number">15.2</span> Lab</a>
  <ul>
<li><a href="#the-validation-set-approach-1" id="toc-the-validation-set-approach-1" class="nav-link" data-scroll-target="#the-validation-set-approach-1"><span class="header-section-number">15.2.1</span> The validation set approach</a></li>
  <li><a href="#leave-one-out-cross-validation-1" id="toc-leave-one-out-cross-validation-1" class="nav-link" data-scroll-target="#leave-one-out-cross-validation-1"><span class="header-section-number">15.2.2</span> Leave-one-out cross-validation</a></li>
  <li><a href="#k-fold-cross-validation-1" id="toc-k-fold-cross-validation-1" class="nav-link" data-scroll-target="#k-fold-cross-validation-1"><span class="header-section-number">15.2.3</span> <span class="math inline">\(k\)</span>-fold cross-validation</a></li>
  <li><a href="#the-bootstrap-1" id="toc-the-bootstrap-1" class="nav-link" data-scroll-target="#the-bootstrap-1"><span class="header-section-number">15.2.4</span> The bootstrap</a></li>
  </ul>
</li>
  <li>
<a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">15.3</span> Exercises</a>
  <ul>
<li>
<a href="#conceptual" id="toc-conceptual" class="nav-link" data-scroll-target="#conceptual"><span class="header-section-number">15.3.1</span> Conceptual</a>
  <ul class="collapse">
<li><a href="#question-1" id="toc-question-1" class="nav-link" data-scroll-target="#question-1">Question 1</a></li>
  <li><a href="#question-2" id="toc-question-2" class="nav-link" data-scroll-target="#question-2">Question 2</a></li>
  <li><a href="#question-3" id="toc-question-3" class="nav-link" data-scroll-target="#question-3">Question 3</a></li>
  <li><a href="#question-4" id="toc-question-4" class="nav-link" data-scroll-target="#question-4">Question 4</a></li>
  </ul>
</li>
  <li>
<a href="#applied" id="toc-applied" class="nav-link" data-scroll-target="#applied"><span class="header-section-number">15.3.2</span> Applied</a>
  <ul class="collapse">
<li><a href="#question-5" id="toc-question-5" class="nav-link" data-scroll-target="#question-5">Question 5</a></li>
  <li><a href="#question-6" id="toc-question-6" class="nav-link" data-scroll-target="#question-6">Question 6</a></li>
  <li><a href="#question-7" id="toc-question-7" class="nav-link" data-scroll-target="#question-7">Question 7</a></li>
  <li><a href="#question-8" id="toc-question-8" class="nav-link" data-scroll-target="#question-8">Question 8</a></li>
  <li><a href="#question-9" id="toc-question-9" class="nav-link" data-scroll-target="#question-9">Question 9</a></li>
  </ul>
</li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content"><section id="notes" class="level2" data-number="15.1"><h2 data-number="15.1" class="anchored" data-anchor-id="notes">
<span class="header-section-number">15.1</span> Notes</h2>
<p><em>Resampling methods</em> are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model. For example, in order to estimate the variability of a linear regression fit, we can repeatedly draw different samples from the training data, fit a linear regression to each new sample, and then examine the extent to which the resulting fits differ. Such an approach may allow us to obtain information that would not be available from fitting the model only once using the original training sample.</p>
<p>In this chapter, we discuss two of the most commonly used resampling methods, <em>cross-validation</em> and the <em>bootstrap.</em> Both methods are important tools in the practical application of many statistical learning procedures. For example, <strong>cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility</strong>. The process of evaluating a model’s performance is known as <em>model assessment</em>, whereas the process of selecting the proper level of flexibility for a model is known as <em>model selection</em>. <strong>The bootstrap is used in several contexts, most commonly to provide a measure of accuracy of a parameter estimate or of a given statistical learning method.</strong></p>
<section id="cross-validation" class="level3" data-number="15.1.1"><h3 data-number="15.1.1" class="anchored" data-anchor-id="cross-validation">
<span class="header-section-number">15.1.1</span> Cross-validation</h3>
<p>Given a data set, the use of a particular statistical learning method is warranted if it results in a low test error. The test error can be easily calculated if a designated test set is available. Unfortunately, this is usually not the case. The training error rate can easily be found, but it dramatically underestimates the test error rate.</p>
<p>In the absence of a very large designated test set that can be used to directly estimate the test error rate, a number of techniques can be used to estimate this quantity using the available training data. Some methods make a mathematical adjustment to the training error rate in order to estimate the test error rate. These are discussed in later chapters. In this section, we instead consider a class of methods that estimate the test error rate by <em>holding out</em> a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.</p>
<section id="the-validation-set-approach" class="level4"><h4 class="anchored" data-anchor-id="the-validation-set-approach">The validation set approach</h4>
<p><img src="files/images/5-validation-approach.png" class="img-fluid" style="width:50.0%"></p>
<p><img src="files/images/5-validation-approach2.png" class="img-fluid" style="width:50.0%"></p>
<p>The validation set approach is conceptually simple and is easy to imple- ment. But it has two potential drawbacks:</p>
<ol type="1">
<li><p>As is shown in the right-hand panel of Figure 5.2, the validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.</p></li>
<li><p>In the validation approach, only a subset of the observations – those that are included in the training set rather than in the validation set – are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that <strong>the validation set error rate may tend to <em>overestimate</em> the test error rate for the model fit on the entire data set</strong>.</p></li>
</ol>
<p>In the coming subsections, we will present <em>cross-validation</em>, a refinement of the validation set approach that addresses these two issues.</p>
</section><section id="leave-one-out-cross-validation" class="level4"><h4 class="anchored" data-anchor-id="leave-one-out-cross-validation">Leave-one-out-cross-validation</h4>
<p><img src="files/images/5-loocv.png" class="img-fluid" style="width:50.0%"></p>
<p>The LOOCV estimate for the test MSE is the average of these <span class="math inline">\(n\)</span> test error estimates:</p>
<p><span class="math display">\[
\text{CV}_{(n)} = \frac{1}{n} \sum_{i = 1}^n \text{MSE}_i
\]</span></p>
<p>LOOCV has a couple of major advantages over the validation set approach.</p>
<ul>
<li><p>First, it has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain <span class="math inline">\(n − 1\)</span> observations, almost as many as are in the entire data set. This is in contrast to the validation set approach, in which the training set is typically around half the size of the original data set. Consequently, the LOOCV approach tends not to overestimate the test error rate as much as the validation set approach does.</p></li>
<li><p>Second, in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.</p></li>
</ul>
<p><img src="files/images/5-loocv-kfold.png" class="img-fluid" style="width:50.0%"></p>
<p>LOOCV has the potential to be expensive to implement, since the model has to be fit <span class="math inline">\(n\)</span> times. This can be very time consuming if <span class="math inline">\(n\)</span> is large, and if each individual model is slow to fit. With least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:</p>
<p><span class="math display">\[
\text{CV}_{(n)} = \frac{1}{n} \sum_{i = 1}^n \big(\frac{y_i - \hat{y}_i}{1 - h_i}\big)^2
\]</span></p>
<p>where <span class="math inline">\(\hat{y}_i\)</span> is the <span class="math inline">\(i\)</span>th fitted value from the original least squares fit, and <span class="math inline">\(h_i\)</span> is the leverage defined <a href="islr-3.html#eq-leverage" class="quarto-xref">Equation&nbsp;<span>13.1</span></a>. This is like the ordinary MSE, except the <span class="math inline">\(i\)</span>th residual is divided by <span class="math inline">\(1 − h_i\)</span>. The leverage lies between <span class="math inline">\(1/n\)</span> and 1, and reflects the amount that an observation influences its own fit. Hence the residuals for high-leverage points are inflated in this formula by exactly the right amount for this equality to hold.</p>
<p>LOOCV is a very general method, and can be used with any kind of predictive modeling.</p>
</section><section id="k-fold-cross-validation" class="level4"><h4 class="anchored" data-anchor-id="k-fold-cross-validation">
<span class="math inline">\(k\)</span>-fold cross-validation</h4>
<p><img src="files/images/5-kfold.png" class="img-fluid" style="width:50.0%"></p>
<p>The k-fold CV estimate is computed by averaging these values,</p>
<p><span class="math display">\[
\text{CV}_{(k)} = \frac{1}{k} \sum_{i = 1}^k \text{MSE}_i
\]</span></p>
<p>It is not hard to see that LOOCV is a special case of <span class="math inline">\(k\)</span>-fold CV in which <span class="math inline">\(k\)</span> is set to equal <span class="math inline">\(n\)</span>. In practice, one typically performs <span class="math inline">\(k\)</span>-fold CV using <span class="math inline">\(k = 5\)</span> or <span class="math inline">\(k = 10\)</span>. The obvious advantage of using <span class="math inline">\(k = 5\)</span> or <span class="math inline">\(k = 10\)</span> rather than <span class="math inline">\(k = n\)</span> is the computational benefits? And cross-validation is a very general approach that can be applied to almost any statistical learning method. The next section describes the non-computational benefits that exist as well.</p>
<p>When we examine real data, we do not know the <em>true</em> test MSE, and so it is difficult to determine the accuracy of the cross-validation estimate. However, if we examine simulated data, then we can compute the true test MSE, and can thereby evaluate the accuracy of our cross-validation results.</p>
<p><img src="files/images/5-loocv-kfold2.png" class="img-fluid" style="width:50.0%"></p>
<p>In all three plots, the two cross-validation estimates are very similar.</p>
<p>When we perform cross-validation, our goal might be to determine how well a given statistical learning procedure can be expected to perform on independent data; in this case, the actual estimate of the test MSE is of interest. But at other times we are interested only in the location of the <em>minimum point in the estimated test MSE curve</em>. This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error. For this purpose, the location of the minimum point in the estimated test MSE curve is important, but the actual value of the estimated test MSE is not. We find in Figure 5.6 that despite the fact that they sometimes underestimate the true test MSE, all of the CV curves come close to identifying the correct level of flexibility – that is, the flexibility level corresponding to the smallest test MSE.</p>
</section><section id="bias-variance-trade-off-for-k-fold-cross-validation" class="level4"><h4 class="anchored" data-anchor-id="bias-variance-trade-off-for-k-fold-cross-validation">Bias-variance trade-off for <span class="math inline">\(k\)</span>-fold cross-validation</h4>
<p>There is potentially more important advantage of <span class="math inline">\(k\)</span>-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV. This has to do with a bias-variance trade-off.</p>
<p>It was mentioned earlier that the validation set approach can lead to overestimates of the test error rate, since in this approach the training set used to fit the statistical learning method contains only half the observations of the entire data set. Using this logic, it is not hard to see that LOOCV will give approximately unbiased estimates of the test error, since each training set contains <span class="math inline">\(n - 1\)</span> observations, which is almost as many as the number of observations in the full data set. And performing <span class="math inline">\(k\)</span>-fold CV for, say, <span class="math inline">\(k = 5\)</span> or <span class="math inline">\(k = 10\)</span> will lead to an intermediate level of bias, since each training set contains approximately <span class="math inline">\((k − 1)n/k\)</span> observations – fewer than in the LOOCV approach, but substantially more than in the validation set approach. Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to <span class="math inline">\(k\)</span>-fold CV.</p>
<p>However, we know that bias is not the only source for concern in an estimating procedure; we must also consider the procedure’s variance. It turns out that LOOCV has higher variance than does <span class="math inline">\(k\)</span>-fold CV with <span class="math inline">\(k &lt; n\)</span>. Why is this the case? When we perform LOOCV, we are in effect averaging the outputs of <span class="math inline">\(n\)</span> fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, when we perform <span class="math inline">\(k\)</span>-fold CV with <span class="math inline">\(k &lt; n\)</span>, we are averaging the outputs of <span class="math inline">\(k\)</span> fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from <span class="math inline">\(k\)</span>-fold CV.</p>
<p><strong>To summarize, there is a bias-variance trade-off associated with the choice of <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>-fold cross-validation. Typically, given these considerations, one performs <span class="math inline">\(k\)</span>-fold cross-validation using <span class="math inline">\(k = 5\)</span> or <span class="math inline">\(k = 10\)</span>, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.</strong></p>
</section><section id="cross-validation-on-classification-problems" class="level4"><h4 class="anchored" data-anchor-id="cross-validation-on-classification-problems">Cross-validation on classification problems</h4>
<p>Cross-validation can also be a very useful approach in the classification setting when <span class="math inline">\(Y\)</span> is qualitative. For instance, in the classification setting, the LOOCV error rate takes the form (the k-fold CV error rate and validation set error rates are defined analogously):</p>
<p><span class="math display">\[
\text{CV}_{(n)} = \frac{1}{n} \sum_{i = 1}^n I(y_i \ne \hat{y}_i)
\]</span></p>
<p>As an example, we fit various logistic regression models. Since this is simulated data, we can compute the <em>true</em> test error rate, which takes a value of 0.201 and so is substantially larger than the Bayes error rate of 0.133. Clearly logistic regression does not have enough flexibility to model the Bayes decision boundary in this setting. We can easily extend logistic regression to obtain a non-linear decision boundary by using polynomial functions of the predictors, as we did in the regression setting. For example, we can fit a <em>quadratic</em> logistic regression model, given by</p>
<p><span class="math display">\[
\log\big(\frac{p}{1 - p}\big) = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_2 + \beta_4 X_2^2
\]</span></p>
<p><img src="files/images/5-quadratic-logreg.png" class="img-fluid" style="width:50.0%"></p>
<p>In practice, for real data, the Bayes decision boundary and the test er- ror rates are unknown. So how might we decide between the four logistic regression models displayed in Figure 5.7? <strong>We can use cross-validation in order to make this decision</strong>.</p>
<p>As we have seen previously, the training error tends to decrease as the flexibility of the fit increases. (The figure indicates that though the training error rate doesn’t quite decrease monotonically, it tends to decrease on the whole as the model complexity increases.) In contrast, the test error displays a characteristic U-shape. The 10-fold CV error rate provides a pretty good approximation to the test error rate. While it somewhat underestimates the error rate, it reaches a minimum when fourth-order polynomials are used, which is very close to the minimum of the test curve, which occurs when third-order polynomials are used. On the right, Again the training error rate declines as the method becomes more flexible, and so we see that the training error rate cannot be used to select the optimal value for <span class="math inline">\(K\)</span>. Though the cross-validation error curve slightly underestimates the test error rate, it takes on a minimum very close to the best value for <span class="math inline">\(K\)</span>.</p>
<p><img src="files/images/5-cv-classification-results.png" class="img-fluid" style="width:50.0%"></p>
</section></section><section id="the-bootstrap" class="level3" data-number="15.1.2"><h3 data-number="15.1.2" class="anchored" data-anchor-id="the-bootstrap">
<span class="header-section-number">15.1.2</span> The bootstrap</h3>
<p><strong>The <em>bootstrap</em> is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method</strong>. As a simple example, the bootstrap can be used to estimate the standard errors of the coefficients from a linear regression fit. In the specific case of linear regression, this is not particularly useful, since we saw know that standard statistical software such as R outputs such standard errors automatically. However, the power of the bootstrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software.</p>
<p>With simulated data, can generate new samples and estimate the sampling distribution of the statistic of interest very easily and get the standard error. In practice, however, the procedure for estimating <span class="math inline">\(SE( &lt; \text{statistic} &gt;)\)</span> cannot be applied, because for real data we cannot generate new samples. However, the bootstrap approach allows us to use a computer to emulate the process of obtaining new sample sets, so that we can estimate the variability of our statistic without generating additional samples. Rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the <em>original data set</em>.</p>
<p><img src="files/images/5-bootstrap.png" class="img-fluid" style="width:50.0%"></p>
<p><img src="files/images/5-bootstrap-results.png" class="img-fluid" style="width:50.0%"></p>
<p>Again, the boxplots have similar spreads, indicating that the bootstrap approach can be used to effectively estimate the variability associated with our statistic.</p>
<p>Bootstrap estimates:</p>
<ul>
<li><p>Of location are biased</p></li>
<li><p>Of spread are fairly accurate</p></li>
<li><p><a href="https://coltongearhart.github.io/computational-methods/bootstrap.html">Better description</a></p></li>
<li><p>One of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required.</p></li>
</ul></section></section><section id="lab" class="level2" data-number="15.2"><h2 data-number="15.2" class="anchored" data-anchor-id="lab">
<span class="header-section-number">15.2</span> Lab</h2>
<p>NOTE: Will learn <code>tidymodels</code> after taking MAS I exam</p>
<section id="the-validation-set-approach-1" class="level3" data-number="15.2.1"><h3 data-number="15.2.1" class="anchored" data-anchor-id="the-validation-set-approach-1">
<span class="header-section-number">15.2.1</span> The validation set approach</h3>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load data</span></span>
<span><span class="va">data_auto</span> <span class="op">&lt;-</span> <span class="fu">ISLR2</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ISLR2/man/Auto.html">Auto</a></span></span>
<span></span>
<span><span class="co"># split into training and testing</span></span>
<span><span class="va">data_split</span> <span class="op">&lt;-</span> <span class="va">data_auto</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">initial_split</a></span><span class="op">(</span>prop <span class="op">=</span> <span class="fl">.7</span><span class="op">)</span></span>
<span><span class="va">data_train</span> <span class="op">&lt;-</span> <span class="va">data_split</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">training</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">data_test</span> <span class="op">&lt;-</span> <span class="va">data_split</span> <span class="op">%&gt;%</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">testing</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># fit various degrees of polynomial models</span></span>
<span><span class="va">mods_lm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">map</span><span class="op">(</span>\<span class="op">(</span><span class="va">power</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="va">power</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">data_train</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span><span class="co"># make predictions</span></span>
<span><span class="va">preds_lm</span> <span class="op">&lt;-</span> <span class="va">mods_lm</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">map</span><span class="op">(</span>\<span class="op">(</span><span class="va">mod</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod</span>, newdata <span class="op">=</span> <span class="va">data_test</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># calculate mse</span></span>
<span><span class="va">preds_lm</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">map</span><span class="op">(</span>\<span class="op">(</span><span class="va">preds</span><span class="op">)</span> <span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/rmse.html">rmse_vec</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">data_test</span><span class="op">$</span><span class="va">mpg</span>, estimate <span class="op">=</span> <span class="va">preds</span><span class="op">)</span> <span class="op">%&gt;%</span> <span class="fu">raise_to_power</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
[1] 27.19162

[[2]]
[1] 22.35439

[[3]]
[1] 22.48835

[[4]]
[1] 22.42548</code></pre>
</div>
</div>
</section><section id="leave-one-out-cross-validation-1" class="level3" data-number="15.2.2"><h3 data-number="15.2.2" class="anchored" data-anchor-id="leave-one-out-cross-validation-1">
<span class="header-section-number">15.2.2</span> Leave-one-out cross-validation</h3>
<p>The LOOCV estimate can be automatically computed for any generalized linear model using the <code><a href="https://rdrr.io/r/stats/glm.html">glm()</a></code> and <code><a href="https://rdrr.io/pkg/boot/man/cv.glm.html">boot::cv.glm()</a></code> functions; and by default <code>family = "gaussian"</code>, which means it does linear regression.</p>
<div class="cell">
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># fit model</span></span>
<span><span class="va">mod_glm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span>, data <span class="op">=</span> <span class="va">data_auto</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># obtain LOOCV estimate</span></span>
<span><span class="va">cv_err</span> <span class="op">&lt;-</span> <span class="fu">boot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/cv.glm.html">cv.glm</a></span><span class="op">(</span><span class="va">data_auto</span>, <span class="va">mod_glm</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><code>cv_err$delta</code> gives two numbers: &lt; first &gt; = LOOCV estimate, &lt; second &gt; = bias-adjusted CV error estimate (for when using <span class="math inline">\(k &lt; n\)</span>)</p>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># extract results</span></span>
<span><span class="co"># -&gt; two numbers are identical here</span></span>
<span><span class="va">cv_err</span><span class="op">$</span><span class="va">delta</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 24.23151 24.23114</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># perform LOOCV for different polynomial models and obtain the CV error estimates</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">map</span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">power</span>, <span class="va">df</span> <span class="op">=</span> <span class="va">data_auto</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="va">power</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>      <span class="fu">boot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/cv.glm.html">cv.glm</a></span><span class="op">(</span><span class="va">df</span>, <span class="va">.</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>      <span class="va">.</span><span class="op">$</span><span class="va">delta</span> <span class="op">%&gt;%</span> </span>
<span>      <span class="va">.</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="op">}</span><span class="op">)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
[1] 24.23151

[[2]]
[1] 19.24821

[[3]]
[1] 19.33498

[[4]]
[1] 19.42443</code></pre>
</div>
</div>
</section><section id="k-fold-cross-validation-1" class="level3" data-number="15.2.3"><h3 data-number="15.2.3" class="anchored" data-anchor-id="k-fold-cross-validation-1">
<span class="header-section-number">15.2.3</span> <span class="math inline">\(k\)</span>-fold cross-validation</h3>
<p>The <code>cv.glm()</code> function can also be used to implement <span class="math inline">\(k\)</span>-fold CV.</p>
<div class="cell">
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># perform k-fold CV for different polynomial models and obtain the CV error estimates</span></span>
<span><span class="co"># -&gt; just have to specify K = k</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">4</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">map</span><span class="op">(</span><span class="kw">function</span><span class="op">(</span><span class="va">power</span>, <span class="va">df</span> <span class="op">=</span> <span class="va">data_auto</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="va">power</span><span class="op">)</span>, data <span class="op">=</span> <span class="va">df</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>      <span class="fu">boot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/cv.glm.html">cv.glm</a></span><span class="op">(</span><span class="va">df</span>, <span class="va">.</span>, K <span class="op">=</span> <span class="fl">10</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>      <span class="va">.</span><span class="op">$</span><span class="va">delta</span> <span class="op">%&gt;%</span> </span>
<span>      <span class="va">.</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="op">}</span><span class="op">)</span> </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
[1] 24.15958

[[2]]
[1] 19.22642

[[3]]
[1] 19.32106

[[4]]
[1] 19.3941</code></pre>
</div>
</div>
</section><section id="the-bootstrap-1" class="level3" data-number="15.2.4"><h3 data-number="15.2.4" class="anchored" data-anchor-id="the-bootstrap-1">
<span class="header-section-number">15.2.4</span> The bootstrap</h3>
<ul>
<li>Already created some examples <a href="https://coltongearhart.github.io/computational-methods/bootstrap.html">here</a>
</li>
</ul></section></section><section id="exercises" class="level2" data-number="15.3"><h2 data-number="15.3" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">15.3</span> Exercises</h2>
<section id="conceptual" class="level3" data-number="15.3.1"><h3 data-number="15.3.1" class="anchored" data-anchor-id="conceptual">
<span class="header-section-number">15.3.1</span> Conceptual</h3>
<section id="question-1" class="level4"><h4 class="anchored" data-anchor-id="question-1">Question 1</h4>
<blockquote class="blockquote">
<p>Using basic statistical properties of the variance, as well as single-variable calculus, derive (5.6). In other words, prove that <span class="math inline">\(\alpha\)</span> given by (5.6) does indeed minimize <span class="math inline">\(Var(\alpha X + (1 - \alpha)Y)\)</span>.</p>
</blockquote>
<p><img src="files/images/5-q1.png" class="img-fluid"></p>
</section><section id="question-2" class="level4"><h4 class="anchored" data-anchor-id="question-2">Question 2</h4>
<blockquote class="blockquote">
<p>We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.</p>
</blockquote>
<blockquote class="blockquote">
<ol type="a">
<li>What is the probability that the first bootstrap observation is <em>not</em> the <span class="math inline">\(j\)</span>th observation from the original sample? Justify your answer.</li>
</ol>
</blockquote>
<p><span class="math display">\[
1 - 1/n
\]</span></p>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>What is the probability that the second bootstrap observation is <em>not</em> the <span class="math inline">\(j\)</span>th observation from the original sample?</li>
</ol>
</blockquote>
<p><span class="math display">\[
1 - 1/n
\]</span></p>
<blockquote class="blockquote">
<ol start="3" type="a">
<li>Argue that the probability that the <span class="math inline">\(j\)</span>th observation is <em>not</em> in the bootstrap sample is <span class="math inline">\((1 - 1/n)^n\)</span>.</li>
</ol>
</blockquote>
<p><span class="math inline">\(n\)</span> independent events -&gt; Multliply the probabilities</p>
<blockquote class="blockquote">
<ol start="4" type="a">
<li>When <span class="math inline">\(n = 5, 100, 10000\)</span>, what is the probability that the <span class="math inline">\(j\)</span>th observation is in the bootstrap sample?</li>
</ol>
</blockquote>
<p><span class="math display">\[
1 - (1 - 1/n)^n
\]</span></p>
<div class="cell">
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">boot_prob</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fl">1</span> <span class="op">-</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="fl">1</span> <span class="op">/</span> <span class="va">n</span><span class="op">)</span><span class="op">^</span><span class="va">n</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">100</span>, <span class="fl">10000</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">map</span><span class="op">(</span>\<span class="op">(</span><span class="va">n</span><span class="op">)</span> <span class="fu">boot_prob</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[1]]
[1] 0.67232

[[2]]
[1] 0.6339677

[[3]]
[1] 0.632139</code></pre>
</div>
</div>
<blockquote class="blockquote">
<ol start="7" type="a">
<li>Create a plot that displays, for each integer value of <span class="math inline">\(n\)</span> from 1 to 100, the probability that the <span class="math inline">\(j\)</span>th observation is in the bootstrap sample. Comment on what you observe.</li>
</ol>
</blockquote>
<div class="cell">
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>n <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">100</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>prob <span class="op">=</span> <span class="fu">boot_prob</span><span class="op">(</span><span class="va">n</span><span class="op">)</span><span class="op">)</span> <span class="op">%$%</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">n</span>, y <span class="op">=</span> <span class="va">prob</span>, type <span class="op">=</span> <span class="st">"o"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-5_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Approaches a limit.</p>
<blockquote class="blockquote">
<ol start="8" type="a">
<li>We will now investigate numerically the probability that a bootstrap sample of size <span class="math inline">\(n = 100\)</span> contains the <span class="math inline">\(j\)</span>th observation. Here <span class="math inline">\(j = 4\)</span>. We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.</li>
</ol>
</blockquote>
<div class="cell">
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">store</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span> <span class="op">(</span><span class="cn">NA</span>, <span class="fl">10000</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10000</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="va">store</span><span class="op">[</span><span class="va">i</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">100</span>, rep <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span> <span class="op">==</span> <span class="fl">4</span><span class="op">)</span> <span class="op">&gt;</span> <span class="fl">0</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">store</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.636</code></pre>
</div>
</div>
<blockquote class="blockquote">
<p>Comment on the results obtained.</p>
</blockquote>
<p>The probability of including <span class="math inline">\(4\)</span> when resampling numbers <span class="math inline">\(1,...,100\)</span> is close to <span class="math inline">\(1 - (1 - 1/100)^{100}\)</span>.</p>
</section><section id="question-3" class="level4"><h4 class="anchored" data-anchor-id="question-3">Question 3</h4>
<blockquote class="blockquote">
<ol start="3" type="1">
<li>We now review <span class="math inline">\(k\)</span>-fold cross-validation.</li>
</ol>
</blockquote>
<blockquote class="blockquote">
<ol type="a">
<li>Explain how <span class="math inline">\(k\)</span>-fold cross-validation is implemented.</li>
</ol>
</blockquote>
<p>Steps:</p>
<ol type="1">
<li><p>Data is randomply split into <span class="math inline">\(k\)</span> subsections, called folds.</p></li>
<li><p>Model is fit on all but 1 of the subsections, and is used to predict data in the remaining section.</p></li>
<li><p>Final error estimate is averaged across all models.</p></li>
</ol>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>What are the advantages and disadvantages of <span class="math inline">\(k\)</span>-fold cross-validation relative to:</li>
<li>The validation set approach?</li>
</ol>
<ol start="2" type="i">
<li>LOOCV?</li>
</ol>
</blockquote>
<ol type="i">
<li><p><span class="math inline">\(k\)</span>-fold is way less biased because more data is used to fit the model (overestimates test error rate relative to using the entire dataset to fit the model); does not depend on the split as heavily as the validation approach</p></li>
<li><p><span class="math inline">\(k\)</span>-fold is computationally easier and final results (error estimates) are less variable (won’t get as big of difference if fit on different set of data)</p></li>
</ol></section><section id="question-4" class="level4"><h4 class="anchored" data-anchor-id="question-4">Question 4</h4>
<blockquote class="blockquote">
<p>Suppose that we use some statistical learning method to make a prediction for the response <span class="math inline">\(Y\)</span> for a particular value of the predictor <span class="math inline">\(X\)</span>. Carefully describe how we might estimate the standard deviation of our prediction.</p>
</blockquote>
<ol type="1">
<li><p>Create 10,000 bootstrap samples</p></li>
<li><p>Fit the same model on each bootstrap sample</p></li>
<li><p>Predict the new observation using each model.</p></li>
<li><p>Calculate standard deviation of predictions</p></li>
</ol></section></section><section id="applied" class="level3" data-number="15.3.2"><h3 data-number="15.3.2" class="anchored" data-anchor-id="applied">
<span class="header-section-number">15.3.2</span> Applied</h3>
<p>!!! TO BE DONE ONCE LEARN TIDYMODELS</p>
<section id="question-5" class="level4"><h4 class="anchored" data-anchor-id="question-5">Question 5</h4>
</section><section id="question-6" class="level4"><h4 class="anchored" data-anchor-id="question-6">Question 6</h4>
</section><section id="question-7" class="level4"><h4 class="anchored" data-anchor-id="question-7">Question 7</h4>
</section><section id="question-8" class="level4"><h4 class="anchored" data-anchor-id="question-8">Question 8</h4>
</section><section id="question-9" class="level4"><h4 class="anchored" data-anchor-id="question-9">Question 9</h4>


<!-- -->

</section></section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./islr-4.html" class="pagination-link" aria-label="ISLR -- Classification">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">ISLR – Classification</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./glm-1.html" class="pagination-link" aria-label="GLM -- Introduction">
        <span class="nav-page-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">GLM – Introduction</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb15" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># ISLR -- Resampling methods</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: load-prereqs</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a><span class="co"># knitr options</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"_common.R"</span>)</span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a><span class="fu">## Notes</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>*Resampling methods* are an indispensable tool in modern statistics. They involve repeatedly drawing samples from a training set and refitting a model of interest on each sample in order to obtain additional information about the fitted model. For example, in order to estimate the variability of a linear regression fit, we can repeatedly draw different samples from the training data, fit a linear regression to each new sample, and then examine the extent to which the resulting fits differ. Such an approach may allow us to obtain information that would not be available from fitting the model only once using the original training sample.</span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>In this chapter, we discuss two of the most commonly used resampling methods, *cross-validation* and the *bootstrap.* Both methods are important tools in the practical application of many statistical learning procedures. For example, **cross-validation can be used to estimate the test error associated with a given statistical learning method in order to evaluate its performance, or to select the appropriate level of flexibility**. The process of evaluating a model's performance is known as *model assessment*, whereas the process of selecting the proper level of flexibility for a model is known as *model selection*. **The bootstrap is used in several contexts, most commonly to provide a measure of accuracy of a parameter estimate or of a given statistical learning method.**</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a><span class="fu">### Cross-validation</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>Given a data set, the use of a particular statistical learning method is warranted if it results in a low test error. The test error can be easily calculated if a designated test set is available. Unfortunately, this is usually not the case. The training error rate can easily be found, but it dramatically underestimates the test error rate.</span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a>In the absence of a very large designated test set that can be used to directly estimate the test error rate, a number of techniques can be used to estimate this quantity using the available training data. Some methods make a mathematical adjustment to the training error rate in order to estimate the test error rate. These are discussed in later chapters. In this section, we instead consider a class of methods that estimate the test error rate by *holding out* a subset of the training observations from the fitting process, and then applying the statistical learning method to those held out observations.</span>
<span id="cb15-26"><a href="#cb15-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-27"><a href="#cb15-27" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The validation set approach</span></span>
<span id="cb15-28"><a href="#cb15-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-29"><a href="#cb15-29" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/5-validation-approach.png)</span>{width="50%"}</span>
<span id="cb15-30"><a href="#cb15-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-31"><a href="#cb15-31" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/5-validation-approach2.png)</span>{width="50%"}</span>
<span id="cb15-32"><a href="#cb15-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-33"><a href="#cb15-33" aria-hidden="true" tabindex="-1"></a>The validation set approach is conceptually simple and is easy to imple- ment. But it has two potential drawbacks:</span>
<span id="cb15-34"><a href="#cb15-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-35"><a href="#cb15-35" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>As is shown in the right-hand panel of Figure 5.2, the validation estimate of the test error rate can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.</span>
<span id="cb15-36"><a href="#cb15-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-37"><a href="#cb15-37" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>In the validation approach, only a subset of the observations -- those that are included in the training set rather than in the validation set -- are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that **the validation set error rate may tend to *overestimate* the test error rate for the model fit on the entire data set**.</span>
<span id="cb15-38"><a href="#cb15-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-39"><a href="#cb15-39" aria-hidden="true" tabindex="-1"></a>In the coming subsections, we will present *cross-validation*, a refinement of the validation set approach that addresses these two issues.</span>
<span id="cb15-40"><a href="#cb15-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-41"><a href="#cb15-41" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Leave-one-out-cross-validation</span></span>
<span id="cb15-42"><a href="#cb15-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-43"><a href="#cb15-43" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/5-loocv.png)</span>{width="50%"}</span>
<span id="cb15-44"><a href="#cb15-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-45"><a href="#cb15-45" aria-hidden="true" tabindex="-1"></a>The LOOCV estimate for the test MSE is the average of these $n$ test error estimates:</span>
<span id="cb15-46"><a href="#cb15-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-47"><a href="#cb15-47" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-48"><a href="#cb15-48" aria-hidden="true" tabindex="-1"></a>\text{CV}_{(n)} = \frac{1}{n} \sum_{i = 1}^n \text{MSE}_i</span>
<span id="cb15-49"><a href="#cb15-49" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-50"><a href="#cb15-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-51"><a href="#cb15-51" aria-hidden="true" tabindex="-1"></a>LOOCV has a couple of major advantages over the validation set approach. </span>
<span id="cb15-52"><a href="#cb15-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-53"><a href="#cb15-53" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>First, it has far less bias. In LOOCV, we repeatedly fit the statistical learning method using training sets that contain $n − 1$ observations, almost as many as are in the entire data set. This is in contrast to the validation set approach, in which the training set is typically around half the size of the original data set. Consequently, the LOOCV approach tends not to overestimate the test error rate as much as the validation set approach does. </span>
<span id="cb15-54"><a href="#cb15-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-55"><a href="#cb15-55" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Second, in contrast to the validation approach which will yield different results when applied repeatedly due to randomness in the training/validation set splits, performing LOOCV multiple times will always yield the same results: there is no randomness in the training/validation set splits.</span>
<span id="cb15-56"><a href="#cb15-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-57"><a href="#cb15-57" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/5-loocv-kfold.png)</span>{width="50%"}</span>
<span id="cb15-58"><a href="#cb15-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-59"><a href="#cb15-59" aria-hidden="true" tabindex="-1"></a>LOOCV has the potential to be expensive to implement, since the model has to be fit $n$ times. This can be very time consuming if $n$ is large, and if each individual model is slow to fit. With least squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! The following formula holds:</span>
<span id="cb15-60"><a href="#cb15-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-61"><a href="#cb15-61" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-62"><a href="#cb15-62" aria-hidden="true" tabindex="-1"></a>\text{CV}_{(n)} = \frac{1}{n} \sum_{i = 1}^n \big(\frac{y_i - \hat{y}_i}{1 - h_i}\big)^2</span>
<span id="cb15-63"><a href="#cb15-63" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-64"><a href="#cb15-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-65"><a href="#cb15-65" aria-hidden="true" tabindex="-1"></a>where $\hat{y}_i$ is the $i$th fitted value from the original least squares fit, and $h_i$ is the leverage defined @eq-leverage. This is like the ordinary MSE, except the $i$th residual is divided by $1 − h_i$. The leverage lies between $1/n$ and 1, and reflects the amount that an observation influences its own fit. Hence the residuals for high-leverage points are inflated in this formula by exactly the right amount for this equality to hold.</span>
<span id="cb15-66"><a href="#cb15-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-67"><a href="#cb15-67" aria-hidden="true" tabindex="-1"></a>LOOCV is a very general method, and can be used with any kind of predictive modeling.</span>
<span id="cb15-68"><a href="#cb15-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-69"><a href="#cb15-69" aria-hidden="true" tabindex="-1"></a><span class="fu">#### $k$-fold cross-validation</span></span>
<span id="cb15-70"><a href="#cb15-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-71"><a href="#cb15-71" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/5-kfold.png)</span>{width="50%"}</span>
<span id="cb15-72"><a href="#cb15-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-73"><a href="#cb15-73" aria-hidden="true" tabindex="-1"></a>The k-fold CV estimate is computed by averaging these values,</span>
<span id="cb15-74"><a href="#cb15-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-75"><a href="#cb15-75" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-76"><a href="#cb15-76" aria-hidden="true" tabindex="-1"></a>\text{CV}_{(k)} = \frac{1}{k} \sum_{i = 1}^k \text{MSE}_i</span>
<span id="cb15-77"><a href="#cb15-77" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-78"><a href="#cb15-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-79"><a href="#cb15-79" aria-hidden="true" tabindex="-1"></a>It is not hard to see that LOOCV is a special case of $k$-fold CV in which $k$ is set to equal $n$.  In practice, one typically performs $k$-fold CV using $k = 5$ or $k = 10$. The obvious advantage of using $k = 5$ or $k = 10$ rather than $k = n$ is the computational benefits? And cross-validation is a very general approach that can be applied to almost any statistical learning method. The next section describes the non-computational benefits that exist as well.</span>
<span id="cb15-80"><a href="#cb15-80" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-81"><a href="#cb15-81" aria-hidden="true" tabindex="-1"></a>When we examine real data, we do not know the *true* test MSE, and so it is difficult to determine the accuracy of the cross-validation estimate. However, if we examine simulated data, then we can compute the true test MSE, and can thereby evaluate the accuracy of our cross-validation results.</span>
<span id="cb15-82"><a href="#cb15-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-83"><a href="#cb15-83" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/5-loocv-kfold2.png)</span>{width="50%"}</span>
<span id="cb15-84"><a href="#cb15-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-85"><a href="#cb15-85" aria-hidden="true" tabindex="-1"></a>In all three plots, the two cross-validation estimates are very similar.</span>
<span id="cb15-86"><a href="#cb15-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-87"><a href="#cb15-87" aria-hidden="true" tabindex="-1"></a>When we perform cross-validation, our goal might be to determine how well a given statistical learning procedure can be expected to perform on independent data; in this case, the actual estimate of the test MSE is of interest. But at other times we are interested only in the location of the *minimum point in the estimated test MSE curve*. This is because we might be performing cross-validation on a number of statistical learning methods, or on a single method using different levels of flexibility, in order to identify the method that results in the lowest test error. For this purpose, the location of the minimum point in the estimated test MSE curve is important, but the actual value of the estimated test MSE is not. We find in Figure 5.6 that despite the fact that they sometimes underestimate the true test MSE, all of the CV curves come close to identifying the correct level of flexibility -- that is, the flexibility level corresponding to the smallest test MSE.</span>
<span id="cb15-88"><a href="#cb15-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-89"><a href="#cb15-89" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Bias-variance trade-off for $k$-fold cross-validation</span></span>
<span id="cb15-90"><a href="#cb15-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-91"><a href="#cb15-91" aria-hidden="true" tabindex="-1"></a>There is potentially more important advantage of $k$-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV. This has to do with a bias-variance trade-off.</span>
<span id="cb15-92"><a href="#cb15-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-93"><a href="#cb15-93" aria-hidden="true" tabindex="-1"></a>It was mentioned earlier that the validation set approach can lead to overestimates of the test error rate, since in this approach the training set used to fit the statistical learning method contains only half the observations of the entire data set. Using this logic, it is not hard to see that LOOCV will give approximately unbiased estimates of the test error, since each training set contains $n - 1$ observations, which is almost as many as the number of observations in the full data set. And performing $k$-fold CV for, say, $k = 5$ or $k = 10$ will lead to an intermediate level of bias, since each training set contains approximately $(k − 1)n/k$ observations -- fewer than in the LOOCV approach, but substantially more than in the validation set approach. Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to $k$-fold CV.</span>
<span id="cb15-94"><a href="#cb15-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-95"><a href="#cb15-95" aria-hidden="true" tabindex="-1"></a>However, we know that bias is not the only source for concern in an estimating procedure; we must also consider the procedure’s variance. It turns out that LOOCV has higher variance than does $k$-fold CV with $k &lt; n$. Why is this the case? When we perform LOOCV, we are in effect averaging the outputs of $n$ fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, when we perform $k$-fold CV with $k &lt; n$, we are averaging the outputs of $k$ fitted models that are somewhat less correlated with each other, since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from $k$-fold CV.</span>
<span id="cb15-96"><a href="#cb15-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-97"><a href="#cb15-97" aria-hidden="true" tabindex="-1"></a>**To summarize, there is a bias-variance trade-off associated with the choice of $k$ in $k$-fold cross-validation. Typically, given these considerations, one performs $k$-fold cross-validation using $k = 5$ or $k = 10$, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.**</span>
<span id="cb15-98"><a href="#cb15-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-99"><a href="#cb15-99" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Cross-validation on classification problems</span></span>
<span id="cb15-100"><a href="#cb15-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-101"><a href="#cb15-101" aria-hidden="true" tabindex="-1"></a>Cross-validation can also be a very useful approach in the classification setting when $Y$ is qualitative. For instance, in the classification setting, the LOOCV error rate takes the form (the k-fold CV error rate and validation set error rates are defined analogously):</span>
<span id="cb15-102"><a href="#cb15-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-103"><a href="#cb15-103" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-104"><a href="#cb15-104" aria-hidden="true" tabindex="-1"></a>\text{CV}_{(n)} = \frac{1}{n} \sum_{i = 1}^n I(y_i \ne \hat{y}_i)</span>
<span id="cb15-105"><a href="#cb15-105" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-106"><a href="#cb15-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-107"><a href="#cb15-107" aria-hidden="true" tabindex="-1"></a>As an example, we fit various logistic regression models. Since this is simulated data, we can compute the *true* test error rate, which takes a value of 0.201 and so is substantially larger than the Bayes error rate of 0.133. Clearly logistic regression does not have enough flexibility to model the Bayes decision boundary in this setting. We can easily extend logistic regression to obtain a non-linear decision boundary by using polynomial functions of the predictors, as we did in the regression setting. For example, we can fit a *quadratic* logistic regression model, given by</span>
<span id="cb15-108"><a href="#cb15-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-109"><a href="#cb15-109" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-110"><a href="#cb15-110" aria-hidden="true" tabindex="-1"></a>\log\big(\frac{p}{1 - p}\big) = \beta_0 + \beta_1 X_1 + \beta_2 X_1^2 + \beta_3 X_2 + \beta_4 X_2^2 </span>
<span id="cb15-111"><a href="#cb15-111" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-112"><a href="#cb15-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-113"><a href="#cb15-113" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/5-quadratic-logreg.png)</span>{width="50%"}</span>
<span id="cb15-114"><a href="#cb15-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-115"><a href="#cb15-115" aria-hidden="true" tabindex="-1"></a>In practice, for real data, the Bayes decision boundary and the test er- ror rates are unknown. So how might we decide between the four logistic regression models displayed in Figure 5.7? **We can use cross-validation in order to make this decision**.</span>
<span id="cb15-116"><a href="#cb15-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-117"><a href="#cb15-117" aria-hidden="true" tabindex="-1"></a>As we have seen previously, the training error tends to decrease as the flexibility of the fit increases. (The figure indicates that though the training error rate doesn’t quite decrease monotonically, it tends to decrease on the whole as the model complexity increases.) In contrast, the test error displays a characteristic U-shape. The 10-fold CV error rate provides a pretty good approximation to the test error rate. While it somewhat underestimates the error rate, it reaches a minimum when fourth-order polynomials are used, which is very close to the minimum of the test curve, which occurs when third-order polynomials are used. On the right, Again the training error rate declines as the method becomes more flexible, and so we see that the training error rate cannot be used to select the optimal value for $K$. Though the cross-validation error curve slightly underestimates the test error rate, it takes on a minimum very close to the best value for $K$.</span>
<span id="cb15-118"><a href="#cb15-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-119"><a href="#cb15-119" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/5-cv-classification-results.png)</span>{width="50%"}</span>
<span id="cb15-120"><a href="#cb15-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-121"><a href="#cb15-121" aria-hidden="true" tabindex="-1"></a><span class="fu">### The bootstrap</span></span>
<span id="cb15-122"><a href="#cb15-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-123"><a href="#cb15-123" aria-hidden="true" tabindex="-1"></a>**The *bootstrap* is a widely applicable and extremely powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method**. As a simple example, the bootstrap can be used to estimate the standard errors of the coefficients from a linear regression fit. In the specific case of linear regression, this is not particularly useful, since we saw know that standard statistical software such as R outputs such standard errors automatically. However, the power of the bootstrap lies in the fact that it can be easily applied to a wide range of statistical learning methods, including some for which a measure of variability is otherwise difficult to obtain and is not automatically output by statistical software.</span>
<span id="cb15-124"><a href="#cb15-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-125"><a href="#cb15-125" aria-hidden="true" tabindex="-1"></a>With simulated data, can generate new samples and estimate the sampling distribution of the statistic of interest very easily and get the standard error. In practice, however, the procedure for estimating $SE( &lt; \text{statistic} &gt;)$ cannot be applied, because for real data we cannot generate new samples. However, the bootstrap approach allows us to use a computer to emulate the process of obtaining new sample sets, so that we can estimate the variability of our statistic without generating additional samples. Rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the *original data set*.</span>
<span id="cb15-126"><a href="#cb15-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-127"><a href="#cb15-127" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/5-bootstrap.png)</span>{width="50%"}</span>
<span id="cb15-128"><a href="#cb15-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-129"><a href="#cb15-129" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/5-bootstrap-results.png)</span>{width="50%"}</span>
<span id="cb15-130"><a href="#cb15-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-131"><a href="#cb15-131" aria-hidden="true" tabindex="-1"></a>Again, the boxplots have similar spreads, indicating that the bootstrap approach can be used to effectively estimate the variability associated with our statistic.</span>
<span id="cb15-132"><a href="#cb15-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-133"><a href="#cb15-133" aria-hidden="true" tabindex="-1"></a>Bootstrap estimates:</span>
<span id="cb15-134"><a href="#cb15-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-135"><a href="#cb15-135" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Of location are biased</span>
<span id="cb15-136"><a href="#cb15-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-137"><a href="#cb15-137" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Of spread are fairly accurate</span>
<span id="cb15-138"><a href="#cb15-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-139"><a href="#cb15-139" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Better description</span><span class="co">](https://coltongearhart.github.io/computational-methods/bootstrap.html)</span></span>
<span id="cb15-140"><a href="#cb15-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-141"><a href="#cb15-141" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>One of the great advantages of the bootstrap approach is that it can be applied in almost all situations. No complicated mathematical calculations are required.</span>
<span id="cb15-142"><a href="#cb15-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-143"><a href="#cb15-143" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lab</span></span>
<span id="cb15-144"><a href="#cb15-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-145"><a href="#cb15-145" aria-hidden="true" tabindex="-1"></a>NOTE: Will learn <span class="in">`tidymodels`</span> after taking MAS I exam</span>
<span id="cb15-146"><a href="#cb15-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-147"><a href="#cb15-147" aria-hidden="true" tabindex="-1"></a><span class="fu">### The validation set approach</span></span>
<span id="cb15-148"><a href="#cb15-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-151"><a href="#cb15-151" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-152"><a href="#cb15-152" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb15-153"><a href="#cb15-153" aria-hidden="true" tabindex="-1"></a>data_auto <span class="ot">&lt;-</span> ISLR2<span class="sc">::</span>Auto</span>
<span id="cb15-154"><a href="#cb15-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-155"><a href="#cb15-155" aria-hidden="true" tabindex="-1"></a><span class="co"># split into training and testing</span></span>
<span id="cb15-156"><a href="#cb15-156" aria-hidden="true" tabindex="-1"></a>data_split <span class="ot">&lt;-</span> data_auto <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(<span class="at">prop =</span> .<span class="dv">7</span>)</span>
<span id="cb15-157"><a href="#cb15-157" aria-hidden="true" tabindex="-1"></a>data_train <span class="ot">&lt;-</span> data_split <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">training</span>()</span>
<span id="cb15-158"><a href="#cb15-158" aria-hidden="true" tabindex="-1"></a>data_test <span class="ot">&lt;-</span> data_split <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">testing</span>()</span>
<span id="cb15-159"><a href="#cb15-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-160"><a href="#cb15-160" aria-hidden="true" tabindex="-1"></a><span class="co"># fit various degrees of polynomial models</span></span>
<span id="cb15-161"><a href="#cb15-161" aria-hidden="true" tabindex="-1"></a>mods_lm <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb15-162"><a href="#cb15-162" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(\(power) <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, power), <span class="at">data =</span> data_train))</span>
<span id="cb15-163"><a href="#cb15-163" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb15-164"><a href="#cb15-164" aria-hidden="true" tabindex="-1"></a><span class="co"># make predictions</span></span>
<span id="cb15-165"><a href="#cb15-165" aria-hidden="true" tabindex="-1"></a>preds_lm <span class="ot">&lt;-</span> mods_lm <span class="sc">%&gt;%</span> </span>
<span id="cb15-166"><a href="#cb15-166" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(\(mod) <span class="fu">predict</span>(mod, <span class="at">newdata =</span> data_test))</span>
<span id="cb15-167"><a href="#cb15-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-168"><a href="#cb15-168" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate mse</span></span>
<span id="cb15-169"><a href="#cb15-169" aria-hidden="true" tabindex="-1"></a>preds_lm <span class="sc">%&gt;%</span> </span>
<span id="cb15-170"><a href="#cb15-170" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(\(preds) yardstick<span class="sc">::</span><span class="fu">rmse_vec</span>(<span class="at">truth =</span> data_test<span class="sc">$</span>mpg, <span class="at">estimate =</span> preds) <span class="sc">%&gt;%</span> <span class="fu">raise_to_power</span>(<span class="dv">2</span>))</span>
<span id="cb15-171"><a href="#cb15-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-172"><a href="#cb15-172" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-173"><a href="#cb15-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-174"><a href="#cb15-174" aria-hidden="true" tabindex="-1"></a><span class="fu">### Leave-one-out cross-validation</span></span>
<span id="cb15-175"><a href="#cb15-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-176"><a href="#cb15-176" aria-hidden="true" tabindex="-1"></a>The LOOCV estimate can be automatically computed for any generalized linear model using the <span class="in">`glm()`</span> and <span class="in">`boot::cv.glm()`</span> functions; and by default <span class="in">`family = "gaussian"`</span>, which means it does linear regression.</span>
<span id="cb15-177"><a href="#cb15-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-180"><a href="#cb15-180" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-181"><a href="#cb15-181" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb15-182"><a href="#cb15-182" aria-hidden="true" tabindex="-1"></a>mod_glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> data_auto)</span>
<span id="cb15-183"><a href="#cb15-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-184"><a href="#cb15-184" aria-hidden="true" tabindex="-1"></a><span class="co"># obtain LOOCV estimate</span></span>
<span id="cb15-185"><a href="#cb15-185" aria-hidden="true" tabindex="-1"></a>cv_err <span class="ot">&lt;-</span> boot<span class="sc">::</span><span class="fu">cv.glm</span>(data_auto, mod_glm)</span>
<span id="cb15-186"><a href="#cb15-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-187"><a href="#cb15-187" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-188"><a href="#cb15-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-189"><a href="#cb15-189" aria-hidden="true" tabindex="-1"></a><span class="in">`cv_err$delta`</span> gives two numbers: &lt; first &gt; = LOOCV estimate, &lt; second &gt; = bias-adjusted CV error estimate (for when using $k &lt; n$)</span>
<span id="cb15-190"><a href="#cb15-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-193"><a href="#cb15-193" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-194"><a href="#cb15-194" aria-hidden="true" tabindex="-1"></a><span class="co"># extract results</span></span>
<span id="cb15-195"><a href="#cb15-195" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; two numbers are identical here</span></span>
<span id="cb15-196"><a href="#cb15-196" aria-hidden="true" tabindex="-1"></a>cv_err<span class="sc">$</span>delta</span>
<span id="cb15-197"><a href="#cb15-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-198"><a href="#cb15-198" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-199"><a href="#cb15-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-202"><a href="#cb15-202" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-203"><a href="#cb15-203" aria-hidden="true" tabindex="-1"></a><span class="co"># perform LOOCV for different polynomial models and obtain the CV error estimates</span></span>
<span id="cb15-204"><a href="#cb15-204" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb15-205"><a href="#cb15-205" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(<span class="cf">function</span>(power, <span class="at">df =</span> data_auto) {</span>
<span id="cb15-206"><a href="#cb15-206" aria-hidden="true" tabindex="-1"></a>    <span class="fu">glm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, power), <span class="at">data =</span> df) <span class="sc">%&gt;%</span> </span>
<span id="cb15-207"><a href="#cb15-207" aria-hidden="true" tabindex="-1"></a>      boot<span class="sc">::</span><span class="fu">cv.glm</span>(df, .) <span class="sc">%&gt;%</span> </span>
<span id="cb15-208"><a href="#cb15-208" aria-hidden="true" tabindex="-1"></a>      .<span class="sc">$</span>delta <span class="sc">%&gt;%</span> </span>
<span id="cb15-209"><a href="#cb15-209" aria-hidden="true" tabindex="-1"></a>      .[<span class="dv">1</span>]</span>
<span id="cb15-210"><a href="#cb15-210" aria-hidden="true" tabindex="-1"></a>  }) </span>
<span id="cb15-211"><a href="#cb15-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-212"><a href="#cb15-212" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-213"><a href="#cb15-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-214"><a href="#cb15-214" aria-hidden="true" tabindex="-1"></a><span class="fu">### $k$-fold cross-validation</span></span>
<span id="cb15-215"><a href="#cb15-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-216"><a href="#cb15-216" aria-hidden="true" tabindex="-1"></a>The <span class="in">`cv.glm()`</span> function can also be used to implement $k$-fold CV.</span>
<span id="cb15-217"><a href="#cb15-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-220"><a href="#cb15-220" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-221"><a href="#cb15-221" aria-hidden="true" tabindex="-1"></a><span class="co"># perform k-fold CV for different polynomial models and obtain the CV error estimates</span></span>
<span id="cb15-222"><a href="#cb15-222" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; just have to specify K = k</span></span>
<span id="cb15-223"><a href="#cb15-223" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb15-224"><a href="#cb15-224" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(<span class="cf">function</span>(power, <span class="at">df =</span> data_auto) {</span>
<span id="cb15-225"><a href="#cb15-225" aria-hidden="true" tabindex="-1"></a>    <span class="fu">glm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, power), <span class="at">data =</span> df) <span class="sc">%&gt;%</span> </span>
<span id="cb15-226"><a href="#cb15-226" aria-hidden="true" tabindex="-1"></a>      boot<span class="sc">::</span><span class="fu">cv.glm</span>(df, ., <span class="at">K =</span> <span class="dv">10</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb15-227"><a href="#cb15-227" aria-hidden="true" tabindex="-1"></a>      .<span class="sc">$</span>delta <span class="sc">%&gt;%</span> </span>
<span id="cb15-228"><a href="#cb15-228" aria-hidden="true" tabindex="-1"></a>      .[<span class="dv">1</span>]</span>
<span id="cb15-229"><a href="#cb15-229" aria-hidden="true" tabindex="-1"></a>  }) </span>
<span id="cb15-230"><a href="#cb15-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-231"><a href="#cb15-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-232"><a href="#cb15-232" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-233"><a href="#cb15-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-234"><a href="#cb15-234" aria-hidden="true" tabindex="-1"></a><span class="fu">### The bootstrap</span></span>
<span id="cb15-235"><a href="#cb15-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-236"><a href="#cb15-236" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Already created some examples <span class="co">[</span><span class="ot">here</span><span class="co">](https://coltongearhart.github.io/computational-methods/bootstrap.html)</span></span>
<span id="cb15-237"><a href="#cb15-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-238"><a href="#cb15-238" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb15-239"><a href="#cb15-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-240"><a href="#cb15-240" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conceptual</span></span>
<span id="cb15-241"><a href="#cb15-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-242"><a href="#cb15-242" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 1</span></span>
<span id="cb15-243"><a href="#cb15-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-244"><a href="#cb15-244" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Using basic statistical properties of the variance, as well as single-variable calculus, derive (5.6). In other words, prove that $\alpha$ given by (5.6) does indeed minimize $Var(\alpha X + (1 - \alpha)Y)$.</span></span>
<span id="cb15-245"><a href="#cb15-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-246"><a href="#cb15-246" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/5-q1.png)</span></span>
<span id="cb15-247"><a href="#cb15-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-248"><a href="#cb15-248" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 2</span></span>
<span id="cb15-249"><a href="#cb15-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-250"><a href="#cb15-250" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of n observations.</span></span>
<span id="cb15-251"><a href="#cb15-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-252"><a href="#cb15-252" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a. What is the probability that the first bootstrap observation is *not* the $j$th observation from the original sample? Justify your answer.</span></span>
<span id="cb15-253"><a href="#cb15-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-254"><a href="#cb15-254" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-255"><a href="#cb15-255" aria-hidden="true" tabindex="-1"></a>1 - 1/n</span>
<span id="cb15-256"><a href="#cb15-256" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-257"><a href="#cb15-257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-258"><a href="#cb15-258" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; b. What is the probability that the second bootstrap observation is *not* the $j$th observation from the original sample?</span></span>
<span id="cb15-259"><a href="#cb15-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-260"><a href="#cb15-260" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-261"><a href="#cb15-261" aria-hidden="true" tabindex="-1"></a>1 - 1/n</span>
<span id="cb15-262"><a href="#cb15-262" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-263"><a href="#cb15-263" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-264"><a href="#cb15-264" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; c. Argue that the probability that the $j$th observation is *not* in the bootstrap sample is $(1 - 1/n)^n$.</span></span>
<span id="cb15-265"><a href="#cb15-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-266"><a href="#cb15-266" aria-hidden="true" tabindex="-1"></a>$n$ independent events -&gt; Multliply the probabilities</span>
<span id="cb15-267"><a href="#cb15-267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-268"><a href="#cb15-268" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; d. When $n = 5, 100, 10000$, what is the probability that the $j$th observation is in the bootstrap sample?</span></span>
<span id="cb15-269"><a href="#cb15-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-270"><a href="#cb15-270" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-271"><a href="#cb15-271" aria-hidden="true" tabindex="-1"></a>1 - (1 - 1/n)^n</span>
<span id="cb15-272"><a href="#cb15-272" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb15-273"><a href="#cb15-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-276"><a href="#cb15-276" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-277"><a href="#cb15-277" aria-hidden="true" tabindex="-1"></a>boot_prob <span class="ot">&lt;-</span> <span class="cf">function</span>(n) {</span>
<span id="cb15-278"><a href="#cb15-278" aria-hidden="true" tabindex="-1"></a>  <span class="dv">1</span> <span class="sc">-</span> (<span class="dv">1</span> <span class="sc">-</span> <span class="dv">1</span> <span class="sc">/</span> n)<span class="sc">^</span>n</span>
<span id="cb15-279"><a href="#cb15-279" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-280"><a href="#cb15-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-281"><a href="#cb15-281" aria-hidden="true" tabindex="-1"></a><span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">100</span>, <span class="dv">10000</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb15-282"><a href="#cb15-282" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(\(n) <span class="fu">boot_prob</span>(n))</span>
<span id="cb15-283"><a href="#cb15-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-284"><a href="#cb15-284" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-285"><a href="#cb15-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-286"><a href="#cb15-286" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; g. Create a plot that displays, for each integer value of $n$ from 1 to 100, the probability that the $j$th observation is in the bootstrap sample. Comment on what you observe.</span></span>
<span id="cb15-287"><a href="#cb15-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-290"><a href="#cb15-290" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-291"><a href="#cb15-291" aria-hidden="true" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">n =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb15-292"><a href="#cb15-292" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">prob =</span> <span class="fu">boot_prob</span>(n)) <span class="sc">%$%</span></span>
<span id="cb15-293"><a href="#cb15-293" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="at">x =</span> n, <span class="at">y =</span> prob, <span class="at">type =</span> <span class="st">"o"</span>)</span>
<span id="cb15-294"><a href="#cb15-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-295"><a href="#cb15-295" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-296"><a href="#cb15-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-297"><a href="#cb15-297" aria-hidden="true" tabindex="-1"></a>Approaches a limit.</span>
<span id="cb15-298"><a href="#cb15-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-299"><a href="#cb15-299" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; h. We will now investigate numerically the probability that a bootstrap sample of size $n = 100$ contains the $j$th observation. Here $j = 4$. We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.</span></span>
<span id="cb15-300"><a href="#cb15-300" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb15-303"><a href="#cb15-303" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-304"><a href="#cb15-304" aria-hidden="true" tabindex="-1"></a>store <span class="ot">&lt;-</span> <span class="fu">rep</span> (<span class="cn">NA</span>, <span class="dv">10000</span>)</span>
<span id="cb15-305"><a href="#cb15-305" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10000</span>) {</span>
<span id="cb15-306"><a href="#cb15-306" aria-hidden="true" tabindex="-1"></a>  store[i] <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>, <span class="at">rep =</span> <span class="cn">TRUE</span>) <span class="sc">==</span> <span class="dv">4</span>) <span class="sc">&gt;</span> <span class="dv">0</span></span>
<span id="cb15-307"><a href="#cb15-307" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb15-308"><a href="#cb15-308" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(store)</span>
<span id="cb15-309"><a href="#cb15-309" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-310"><a href="#cb15-310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-311"><a href="#cb15-311" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;    Comment on the results obtained.</span></span>
<span id="cb15-312"><a href="#cb15-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-313"><a href="#cb15-313" aria-hidden="true" tabindex="-1"></a>The probability of including $4$ when resampling numbers $1,...,100$ is close to</span>
<span id="cb15-314"><a href="#cb15-314" aria-hidden="true" tabindex="-1"></a>$1 - (1 - 1/100)^{100}$.</span>
<span id="cb15-315"><a href="#cb15-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-316"><a href="#cb15-316" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 3</span></span>
<span id="cb15-317"><a href="#cb15-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-318"><a href="#cb15-318" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; 3. We now review $k$-fold cross-validation.</span></span>
<span id="cb15-319"><a href="#cb15-319" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-320"><a href="#cb15-320" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a. Explain how $k$-fold cross-validation is implemented.</span></span>
<span id="cb15-321"><a href="#cb15-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-322"><a href="#cb15-322" aria-hidden="true" tabindex="-1"></a>Steps:</span>
<span id="cb15-323"><a href="#cb15-323" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-324"><a href="#cb15-324" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Data is randomply split into $k$ subsections, called folds.</span>
<span id="cb15-325"><a href="#cb15-325" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-326"><a href="#cb15-326" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Model is fit on all but 1 of the subsections, and is used to predict data in the remaining section.</span>
<span id="cb15-327"><a href="#cb15-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-328"><a href="#cb15-328" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Final error estimate is averaged across all models.</span>
<span id="cb15-329"><a href="#cb15-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-330"><a href="#cb15-330" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; b. What are the advantages and disadvantages of $k$-fold cross-validation relative to:</span></span>
<span id="cb15-331"><a href="#cb15-331" aria-hidden="true" tabindex="-1"></a><span class="at">    i. The validation set approach?</span></span>
<span id="cb15-332"><a href="#cb15-332" aria-hidden="true" tabindex="-1"></a><span class="at">    ii. LOOCV?</span></span>
<span id="cb15-333"><a href="#cb15-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-334"><a href="#cb15-334" aria-hidden="true" tabindex="-1"></a>i. $k$-fold is way less biased because more data is used to fit the model (overestimates test error rate relative to using the entire dataset to fit the model); does not depend on the split as heavily as the validation approach</span>
<span id="cb15-335"><a href="#cb15-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-336"><a href="#cb15-336" aria-hidden="true" tabindex="-1"></a>ii. $k$-fold is computationally easier and final results (error estimates) are less variable (won't get as big of difference if fit on different set of data)</span>
<span id="cb15-337"><a href="#cb15-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-338"><a href="#cb15-338" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 4</span></span>
<span id="cb15-339"><a href="#cb15-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-340"><a href="#cb15-340" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Suppose that we use some statistical learning method to make a prediction for the response $Y$ for a particular value of the predictor $X$. Carefully describe how we might estimate the standard deviation of our prediction.</span></span>
<span id="cb15-341"><a href="#cb15-341" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-342"><a href="#cb15-342" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>Create 10,000 bootstrap samples</span>
<span id="cb15-343"><a href="#cb15-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-344"><a href="#cb15-344" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Fit the same model on each bootstrap sample</span>
<span id="cb15-345"><a href="#cb15-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-346"><a href="#cb15-346" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Predict the new observation using each model.</span>
<span id="cb15-347"><a href="#cb15-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-348"><a href="#cb15-348" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Calculate standard deviation of predictions</span>
<span id="cb15-349"><a href="#cb15-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-350"><a href="#cb15-350" aria-hidden="true" tabindex="-1"></a><span class="fu">### Applied</span></span>
<span id="cb15-351"><a href="#cb15-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-352"><a href="#cb15-352" aria-hidden="true" tabindex="-1"></a>!!! TO BE DONE ONCE LEARN TIDYMODELS</span>
<span id="cb15-353"><a href="#cb15-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-354"><a href="#cb15-354" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 5</span></span>
<span id="cb15-355"><a href="#cb15-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-358"><a href="#cb15-358" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-359"><a href="#cb15-359" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-360"><a href="#cb15-360" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-361"><a href="#cb15-361" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 6</span></span>
<span id="cb15-362"><a href="#cb15-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-365"><a href="#cb15-365" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-366"><a href="#cb15-366" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-367"><a href="#cb15-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-368"><a href="#cb15-368" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 7</span></span>
<span id="cb15-369"><a href="#cb15-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-372"><a href="#cb15-372" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-373"><a href="#cb15-373" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-374"><a href="#cb15-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-375"><a href="#cb15-375" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 8</span></span>
<span id="cb15-376"><a href="#cb15-376" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-379"><a href="#cb15-379" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-380"><a href="#cb15-380" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb15-381"><a href="#cb15-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-382"><a href="#cb15-382" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 9</span></span>
<span id="cb15-383"><a href="#cb15-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-386"><a href="#cb15-386" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb15-387"><a href="#cb15-387" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>