<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.56">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>14&nbsp; ISLR – Classification – MAS I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./islr-5.html" rel="next">
<link href="./islr-3.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./partC-glm.html">Extended Linear Models</a></li><li class="breadcrumb-item"><a href="./islr-4.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">ISLR – Classification</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><header id="title-block-header" class="quarto-title-block default page-columns page-full"><div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./partC-glm.html">Extended Linear Models</a></li><li class="breadcrumb-item"><a href="./islr-4.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">ISLR – Classification</span></a></li></ol></nav>
      <div class="quarto-title-block"><div><h1 class="title">
<span class="chapter-number">14</span>&nbsp; <span class="chapter-title">ISLR – Classification</span>
</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">MAS I</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Overview</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./partA-prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-0.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">CA – Basics</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">CA – Claim severity distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">CA – Insurance applications</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">CA – Tail properties of distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">CA – Poisson processes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">CA – Reliability theory</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">CA – Discrete Markov chains</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">CA – Life contingencies</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">CA – Simulation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-prob-quizzes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">CA – Probability quizzes</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./partB-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">CA – All stats</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./partC-glm.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Extended Linear Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./islr-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">ISLR – Statistical learning</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./islr-3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">ISLR – Linear regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./islr-4.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">ISLR – Classification</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./islr-5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">ISLR – Resampling methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">GLM – Introduction</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">GLM – Model fitting</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">GLM – Exponential family and generalized linear models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">GLM – Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./glm-5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">GLM – Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-glm-4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">CA – ANOVA</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-glm-7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">CA – Other linear regression approaches</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-glm-11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">CA – Generalized additive models</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-glm-misc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">CA – Misc GLM</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ca-glm-quizzes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">CA – GLM quizzes</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part-review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Review</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./review-study-guides.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Study guides</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./review-practice-exams.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Practice exams</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li>
<a href="#notes" id="toc-notes" class="nav-link active" data-scroll-target="#notes"><span class="header-section-number">14.1</span> Notes</a>
  <ul>
<li><a href="#an-overview-of-classification" id="toc-an-overview-of-classification" class="nav-link" data-scroll-target="#an-overview-of-classification"><span class="header-section-number">14.1.1</span> An overview of classification</a></li>
  <li><a href="#why-not-linear-regression" id="toc-why-not-linear-regression" class="nav-link" data-scroll-target="#why-not-linear-regression"><span class="header-section-number">14.1.2</span> Why not linear regression?</a></li>
  <li>
<a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">14.1.3</span> Logistic regression</a>
  <ul class="collapse">
<li><a href="#sec-logistic-model" id="toc-sec-logistic-model" class="nav-link" data-scroll-target="#sec-logistic-model">The logisitic model</a></li>
  <li><a href="#estimating-the-regression-coefficients" id="toc-estimating-the-regression-coefficients" class="nav-link" data-scroll-target="#estimating-the-regression-coefficients">Estimating the regression coefficients</a></li>
  <li><a href="#making-predictions" id="toc-making-predictions" class="nav-link" data-scroll-target="#making-predictions">Making predictions</a></li>
  <li><a href="#multiple-logistic-regression" id="toc-multiple-logistic-regression" class="nav-link" data-scroll-target="#multiple-logistic-regression">Multiple logistic regression</a></li>
  <li><a href="#multinomial-logisitic-regression" id="toc-multinomial-logisitic-regression" class="nav-link" data-scroll-target="#multinomial-logisitic-regression">Multinomial logisitic regression</a></li>
  </ul>
</li>
  <li>
<a href="#generative-models-for-classification" id="toc-generative-models-for-classification" class="nav-link" data-scroll-target="#generative-models-for-classification"><span class="header-section-number">14.1.4</span> Generative models for classification</a>
  <ul class="collapse">
<li><a href="#linear-discrimant-analysis-for-p-1" id="toc-linear-discrimant-analysis-for-p-1" class="nav-link" data-scroll-target="#linear-discrimant-analysis-for-p-1">Linear discrimant analysis for <span class="math inline">\(p = 1\)</span></a></li>
  <li><a href="#linear-discriminant-analysis-for-p-1" id="toc-linear-discriminant-analysis-for-p-1" class="nav-link" data-scroll-target="#linear-discriminant-analysis-for-p-1">Linear discriminant analysis for <span class="math inline">\(p &gt; 1\)</span></a></li>
  <li><a href="#quadratic-discriminant-analysis" id="toc-quadratic-discriminant-analysis" class="nav-link" data-scroll-target="#quadratic-discriminant-analysis">Quadratic discriminant analysis</a></li>
  <li><a href="#naive-bayes" id="toc-naive-bayes" class="nav-link" data-scroll-target="#naive-bayes">Naive Bayes</a></li>
  </ul>
</li>
  <li>
<a href="#a-comparison-of-classification-methods" id="toc-a-comparison-of-classification-methods" class="nav-link" data-scroll-target="#a-comparison-of-classification-methods"><span class="header-section-number">14.1.5</span> A comparison of classification methods</a>
  <ul class="collapse">
<li><a href="#an-analytical-comparison" id="toc-an-analytical-comparison" class="nav-link" data-scroll-target="#an-analytical-comparison">An analytical comparison</a></li>
  <li><a href="#an-empirical-comparison" id="toc-an-empirical-comparison" class="nav-link" data-scroll-target="#an-empirical-comparison">An empirical comparison</a></li>
  </ul>
</li>
  <li>
<a href="#generalized-linear-models" id="toc-generalized-linear-models" class="nav-link" data-scroll-target="#generalized-linear-models"><span class="header-section-number">14.1.6</span> Generalized linear models</a>
  <ul class="collapse">
<li><a href="#poisson-regression" id="toc-poisson-regression" class="nav-link" data-scroll-target="#poisson-regression">Poisson regression</a></li>
  <li><a href="#generalized-linear-models-in-greater-generality" id="toc-generalized-linear-models-in-greater-generality" class="nav-link" data-scroll-target="#generalized-linear-models-in-greater-generality">Generalized linear models in greater generality</a></li>
  </ul>
</li>
  </ul>
</li>
  <li>
<a href="#lab" id="toc-lab" class="nav-link" data-scroll-target="#lab"><span class="header-section-number">14.2</span> Lab</a>
  <ul>
<li><a href="#load-data" id="toc-load-data" class="nav-link" data-scroll-target="#load-data"><span class="header-section-number">14.2.1</span> Load data</a></li>
  <li><a href="#logisitic-regression" id="toc-logisitic-regression" class="nav-link" data-scroll-target="#logisitic-regression"><span class="header-section-number">14.2.2</span> Logisitic regression</a></li>
  <li><a href="#linear-discriminant-analysis" id="toc-linear-discriminant-analysis" class="nav-link" data-scroll-target="#linear-discriminant-analysis"><span class="header-section-number">14.2.3</span> Linear discriminant analysis</a></li>
  <li><a href="#quadratic-discriminant-analysis-1" id="toc-quadratic-discriminant-analysis-1" class="nav-link" data-scroll-target="#quadratic-discriminant-analysis-1"><span class="header-section-number">14.2.4</span> Quadratic discriminant analysis</a></li>
  <li><a href="#naive-bayes-1" id="toc-naive-bayes-1" class="nav-link" data-scroll-target="#naive-bayes-1"><span class="header-section-number">14.2.5</span> Naive Bayes</a></li>
  <li><a href="#knn" id="toc-knn" class="nav-link" data-scroll-target="#knn"><span class="header-section-number">14.2.6</span> KNN</a></li>
  <li><a href="#poisson-regression-1" id="toc-poisson-regression-1" class="nav-link" data-scroll-target="#poisson-regression-1"><span class="header-section-number">14.2.7</span> Poisson regression</a></li>
  </ul>
</li>
  <li>
<a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="header-section-number">14.3</span> Exercises</a>
  <ul>
<li>
<a href="#conceptual" id="toc-conceptual" class="nav-link" data-scroll-target="#conceptual"><span class="header-section-number">14.3.1</span> Conceptual</a>
  <ul class="collapse">
<li><a href="#question-1" id="toc-question-1" class="nav-link" data-scroll-target="#question-1">Question 1</a></li>
  <li><a href="#question-2" id="toc-question-2" class="nav-link" data-scroll-target="#question-2">Question 2</a></li>
  <li><a href="#question-3" id="toc-question-3" class="nav-link" data-scroll-target="#question-3">Question 3</a></li>
  <li><a href="#question-4" id="toc-question-4" class="nav-link" data-scroll-target="#question-4">Question 4</a></li>
  <li><a href="#question-5" id="toc-question-5" class="nav-link" data-scroll-target="#question-5">Question 5</a></li>
  <li><a href="#question-6" id="toc-question-6" class="nav-link" data-scroll-target="#question-6">Question 6</a></li>
  <li><a href="#question-7" id="toc-question-7" class="nav-link" data-scroll-target="#question-7">Question 7</a></li>
  <li><a href="#question-8" id="toc-question-8" class="nav-link" data-scroll-target="#question-8">Question 8</a></li>
  <li><a href="#question-9" id="toc-question-9" class="nav-link" data-scroll-target="#question-9">Question 9</a></li>
  <li><a href="#question-10" id="toc-question-10" class="nav-link" data-scroll-target="#question-10">Question 10</a></li>
  <li><a href="#question-11" id="toc-question-11" class="nav-link" data-scroll-target="#question-11">Question 11</a></li>
  <li><a href="#question-12" id="toc-question-12" class="nav-link" data-scroll-target="#question-12">Question 12</a></li>
  </ul>
</li>
  <li>
<a href="#applied" id="toc-applied" class="nav-link" data-scroll-target="#applied"><span class="header-section-number">14.3.2</span> Applied</a>
  <ul class="collapse">
<li><a href="#question-13" id="toc-question-13" class="nav-link" data-scroll-target="#question-13">Question 13</a></li>
  <li><a href="#question-14" id="toc-question-14" class="nav-link" data-scroll-target="#question-14">Question 14</a></li>
  <li><a href="#question-15" id="toc-question-15" class="nav-link" data-scroll-target="#question-15">Question 15</a></li>
  <li><a href="#question-16" id="toc-question-16" class="nav-link" data-scroll-target="#question-16">Question 16</a></li>
  </ul>
</li>
  </ul>
</li>
  </ul></nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content"><!-- % (followsp is short for 'follow special') shortcut that can be used for iid or ind ~ 'Named dist ' in normal font with space before parameters would go --><!-- % shortcut for matrix notation --><section id="notes" class="level2" data-number="14.1"><h2 data-number="14.1" class="anchored" data-anchor-id="notes">
<span class="header-section-number">14.1</span> Notes</h2>
<section id="an-overview-of-classification" class="level3" data-number="14.1.1"><h3 data-number="14.1.1" class="anchored" data-anchor-id="an-overview-of-classification">
<span class="header-section-number">14.1.1</span> An overview of classification</h3>
<p>The linear regression model discussed in earlier assumes that the response variable <span class="math inline">\(Y\)</span> is <em>quantitative</em>. But in many situations, the response variable is instead <em>qualitative</em>. In this chapter, we study approaches for predicting qualitative responses, a process that is known as <em>classification</em>.</p>
<p>Just as in the regression setting, in the classification setting we have a set of training observations <span class="math inline">\((x_1,y_1), \ldots, (x_n,y_n)\)</span> that we can use to build a classifier. We want our classifier to perform well not only on the training data, but also on test observations that were not used to train the classifier.</p>
</section><section id="why-not-linear-regression" class="level3" data-number="14.1.2"><h3 data-number="14.1.2" class="anchored" data-anchor-id="why-not-linear-regression">
<span class="header-section-number">14.1.2</span> Why not linear regression?</h3>
<p>Could try to code categories to numbers such as</p>
<p><span class="math display">\[
Y = \begin{cases}
   1  &amp; \text{if a} \\
   2  &amp; \text{if b} \\
   3  &amp; \text{if c} \\
\end{cases}
\]</span> However, this implies an ordering of the outcomes, which means ‘b’ is above ‘a’, ‘c’ is above ‘b’ and the difference between ‘a’ and ‘b’ is the same as the difference between ‘b’ and ‘c’. Typically with categorical variables, order is arbitrary, so it could easily be switched around and lead to a drastically different model. If there is a natural ordering, such as mild, moderate, and severe AND we felt the gap mild and moderate was similar to the gap between moderate and severe, then a 1, 2, 3 coding would be reasonable. Unfortunately, in general there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.</p>
<p>For a <em>binary</em> (two level) qualitative response, the situation is better. We can use a dummy variable approach to code the response:</p>
<p><span class="math display">\[
Y = \begin{cases}
   0   &amp; \text{if a} \\
   1   &amp; \text{if b} \\
\end{cases}
\]</span></p>
<p>We could then fit a linear regression to this binary response, and predict ‘b’ if <span class="math inline">\(\hat{Y} &gt; 0.5\)</span> and ‘a’ otherwise. In this case, even if we flip the coding, the linear regression will produce the same final predictions (so coding doesn’t matter).</p>
<p>For a binary response with a 0/1 coding as above, regression by least squares is not completely unreasonable: it can be shown that the <span class="math inline">\(X\hat{\beta}\)</span> obtained using linear regression is in fact an estimate of <span class="math inline">\(P(Y = 1 \mid X)\)</span> in this special case. However, if we use linear regression, some of our estimates might be outside the [0, 1] interval, making them hard to interpret as probabilities!</p>
<p><img src="files/images/4-classification-linreg.png" class="img-fluid" style="width:50.0%"></p>
<p>Nevertheless, the predictions provide an ordering and can be interpreted as crude probability estimates. <strong>Curiously, it turns out that the classifications that we get if we use linear regression to predict a binary response will be the same as for the linear discriminant analysis (LDA) procedure shown later.</strong></p>
<p>To summarize, there are at least two reasons not to perform classification using a regression method: (a) a regression method cannot accommodate a qualitative response with more than two classes; (b) a regression method will not provide meaningful estimates of <span class="math inline">\(P(Y \mid X)\)</span>, even with just two classes. Thus, it is preferable to use a classification method that is truly suited for qualitative response values.</p>
</section><section id="logistic-regression" class="level3" data-number="14.1.3"><h3 data-number="14.1.3" class="anchored" data-anchor-id="logistic-regression">
<span class="header-section-number">14.1.3</span> Logistic regression</h3>
<p>Consider the Default data set, where the response default falls into one of two categories, Yes or No.&nbsp;<strong>Rather than modeling this response Y directly, logistic regression models the <em>probability</em> that Y belongs to a particular category.</strong></p>
<p>For the Default data, logistic regression models the probability of default. For example, the probability of default given balance can be written as</p>
<p><span class="math display">\[
P(\text{default} = \text{Yes} \mid \text{balance})
\]</span></p>
<p>The values of <span class="math inline">\(P(\text{default} = \text{Yes} \mid \text{balance})\)</span>, which we abbreviate <span class="math inline">\(p(\text{balance})\)</span>, will range between 0 and 1. Then for any given value of balance, a prediction can be made for default. For example, one might predict default = Yes for any individual for whom <span class="math inline">\(p(\text{balance}) &gt; 0.5\)</span>. Alternatively, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as <span class="math inline">\(p(\text{balance}) &gt; 0.1\)</span>.</p>
<section id="sec-logistic-model" class="level4"><h4 class="anchored" data-anchor-id="sec-logistic-model">The logisitic model</h4>
<p>How should we model the relationship between <span class="math inline">\(p(X) = P(Y = 1 \mid X)\)</span> and <span class="math inline">\(X\)</span>? Above, we considered using a linear regression model to represent these probabilities:</p>
<p><span class="math display">\[
p(X) = \beta_0 + \beta_1 X
\]</span></p>
<p>If we use this approach to predict default=Yes using balance, then we obtain the model shown in the left-hand panel of Figure 4.2, which results in the obvious problem of predictions out of bounds. Any time a straight line is fit to a binary response that is coded as 0 or 1, in principle we can always predict <span class="math inline">\(p(X) &lt; 0\)</span> for some values of <span class="math inline">\(X\)</span> and <span class="math inline">\(p(X) &gt; 1\)</span> for others (unless the range of <span class="math inline">\(X\)</span> is limited).</p>
<p>To avoid this problem, we must model <span class="math inline">\(p(X)\)</span> using a function that gives outputs between 0 and 1 for all values of X. Many functions meet this description. In logistic regression, we use the <em>logistic function</em>,</p>
<p><span id="eq-logistic-function"><span class="math display">\[
p(X) = \frac{\mathrm{e}^{\beta_0 + \beta_1 X}}{1 + \mathrm{e}^{\beta_0 + \beta_1 X}}  = \frac{1}{1 + \mathrm{e}^{-(\beta_0 + \beta_1 X)}}
\tag{14.1}\]</span></span></p>
<p>To fit this model, we use <em>maximum likelihood</em>. The right-hand panel of Figure 4.2 illustrates the fit of the logistic regression model to the Default data. Notice that for low balances we now predict the probability of default as close to, but never below, zero. Likewise, for high balances we predict a default probability close to, but never above, one. The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of <span class="math inline">\(X\)</span>, we will obtain a sensible prediction. We also see that the logistic model is better able to capture the range of probabilities than is the linear regression. The average fitted probability in both cases is 0.0333 (averaged over the training data), which is the same as the overall proportion of defaulters in the data set. We can show:</p>
<p><span class="math display">\[
\begin{align*}
p(X) = p &amp;= \frac{\mathrm{e}^{\beta_0 + \beta_1 X}}{1 + \mathrm{e}^{\beta_0 + \beta_1 X}} \quad\text{for simplicity}\\
p(1 + \mathrm{e}^{\beta_0 + \beta_1 X}) &amp;= \mathrm{e}^{\beta_0 + \beta_1 X}\\
p + p\mathrm{e}^{\beta_0 + \beta_1 X} &amp;= \mathrm{e}^{\beta_0 + \beta_1 X}\\
p + p\mathrm{e}^{\beta_0 + \beta_1 X} - \mathrm{e}^{\beta_0 + \beta_1 X} &amp;= 0\\
p + \mathrm{e}^{\beta_0 + \beta_1 X}(p - 1) &amp;= 0\\
\mathrm{e}^{\beta_0 + \beta_1 X}(p - 1) &amp;= -p\\
\mathrm{e}^{\beta_0 + \beta_1 X} &amp;= \frac{-p}{p - 1}\\
\mathrm{e}^{\beta_0 + \beta_1 X} &amp;= \frac{p(X)}{1 - p(X)}\\
\end{align*}
\]</span></p>
<p>The quantity <span class="math inline">\(p(X) / [1 - p(X)]\)</span> is called the <em>odds</em>, and can take on any value between 0 and <span class="math inline">\(\infty\)</span>. Values of the odds close to 0 and <span class="math inline">\(\infty\)</span> indicate very low and very high probabilities of default, respectively.For example, on average 1 in 5 people with an odds of 1/4 will default, since <span class="math inline">\(p(X) = 0.2\)</span> implies an odds of <span class="math inline">\(\frac{0.2}{1 - 0.2} = 1/4\)</span>.</p>
<p>By taking the logarithm of both sides, we arrive at the <em>log odds</em> or <em>logit</em>.</p>
<p><span id="eq-logit"><span class="math display">\[
\log\Big(\frac{p(X)}{1 - p(X)}\Big) = \beta_0 + \beta_1 X
\tag{14.2}\]</span></span></p>
<p>Thus, we see that the logistic regression model <span class="math inline">\(p(X) = \frac{\mathrm{e}^{\beta_0 + \beta_1 X}}{1 + \mathrm{e}^{\beta_0 + \beta_1 X}}\)</span> has a logit that is linear in <span class="math inline">\(X\)</span>.</p>
<p>Now in a logistic regression model, increasing <span class="math inline">\(X\)</span> by one unit changes the log odds by <span class="math inline">\(\beta_1\)</span>. Equivalently, it multiplies the odds by <span class="math inline">\(\mathrm{e}^{\beta_1}\)</span>. However, because the relationship between <span class="math inline">\(p(X)\)</span> and <span class="math inline">\(X\)</span> is not a straight line anymore, <span class="math inline">\(\beta_1\)</span> does <em>not</em> correspond to the change in <span class="math inline">\(p(X)\)</span> associated with a one-unit increase in X. The amount that <span class="math inline">\(p(X)\)</span> changes due to a one-unit change in <span class="math inline">\(X\)</span> depends on the current value of <span class="math inline">\(X\)</span>. But regardless of the value of <span class="math inline">\(X\)</span>, if <span class="math inline">\(\beta_1\)</span> is positive then increasing <span class="math inline">\(X\)</span> will be associated with increasing <span class="math inline">\(p(X)\)</span>, and if <span class="math inline">\(\beta_1\)</span> is negative then increasing <span class="math inline">\(X\)</span> will be associated with decreasing <span class="math inline">\(p(X)\)</span>.</p>
</section><section id="estimating-the-regression-coefficients" class="level4"><h4 class="anchored" data-anchor-id="estimating-the-regression-coefficients">Estimating the regression coefficients</h4>
<p>The coefficients in <a href="#eq-logistic-function" class="quarto-xref">Equation&nbsp;<span>14.1</span></a> are unknown, and must be estimated based on the available training data. Although we could use (non-linear) least squares to fit the model <a href="#eq-logit" class="quarto-xref">Equation&nbsp;<span>14.2</span></a>, the more general method of maximum likelihood is preferred, since it has better statistical properties. The basic intuition behind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> such that the predicted probability <span class="math inline">\(\hat{p}(x_i)\)</span> of default for each individual, using <a href="#eq-logistic-function" class="quarto-xref">Equation&nbsp;<span>14.1</span></a>, corresponds as closely as possible to the individual’s observed default status. In other words, we try to find <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> such that plugging these estimates into the model for <span class="math inline">\(p(X)\)</span>, given in <a href="#eq-logistic-function" class="quarto-xref">Equation&nbsp;<span>14.1</span></a>, yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not. This intuition can be formalized using a mathematical equation called a <em>likelihood function</em>:</p>
<p><span id="eq-logistic-likelihood"><span class="math display">\[
\ell(\beta_0, \beta_1) = \prod_{i:y_i = 1} p(x_i) \prod_{i':y_{i'}=0} (1 - p(x_{i'}))
\tag{14.3}\]</span></span></p>
<p>(note that this notation is the same as the following, just without the powers)</p>
<p><span class="math display">\[
\ell(\beta_0, \beta_1) = \prod_{i = 1}^n p(x_i)^{y_i} (1 - p(x_i))^{1 - y_i}
\]</span></p>
<p>The estimates <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> are chosen to <em>maximize</em> this likelihood function. Said another way: to minimize the mis-classification rate, we should predict <span class="math inline">\(Y = 1\)</span> when <span class="math inline">\(p \ge 0.5\)</span> and <span class="math inline">\(Y = 0\)</span> when <span class="math inline">\(p &lt; 0.5\)</span>. This means guessing 1 whenever <span class="math inline">\(\beta_0 + X\beta\)</span> is non-negative (as seen in the last version of <a href="#eq-logistic-function" class="quarto-xref">Equation&nbsp;<span>14.1</span></a>, where <span class="math inline">\(\frac{1}{1 + \mathrm{e}^{-0}} = 1/2\)</span> is the cutoff point), and 0 otherwise. So logistic regression gives us a linear classifier. The decision boundary separating the two predicted classes is the solution of <span class="math inline">\(\beta_0 + X \beta_1 = 0\)</span>, which is a point if <span class="math inline">\(X\)</span> is one dimensional, a line if it is two dimensional, etc.</p>
<p>Maximum likelihood is a very general approach that is used to fit many of the non-linear models that we examine throughout this book. In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood.</p>
<p><img src="files/images/4-logreg-output.png" class="img-fluid" style="width:50.0%"></p>
<p>Many aspects of the logistic regression output shown in Table 4.1 are similar to the linear regression output in the previous chapter. For example, we can measure the accuracy of the coefficient estimates by computing their standard errors. The z-statistic in Table 4.1 plays the same role as the t-statistic in the linear regression output. For instance, the z-statistic associated with <span class="math inline">\(\beta_1\)</span> is equal to <span class="math inline">\(\beta_1/SE(\beta_1)\)</span>, and so a large (absolute) value of the z-statistic indicates evidence against the null hypothesis <span class="math inline">\(H_0: \beta_1 = 0\)</span>. This null hypothesis implies that <span class="math inline">\(p(X) = e^{\beta_0}\)</span>: in other words, that the probability of default does not depend on balance. The estimated intercept in Table 4.1 is typically not of interest; its main purpose is to adjust the average fitted probabilities to the proportion of ones in the data (in this case, the overall default rate).</p>
</section><section id="making-predictions" class="level4"><h4 class="anchored" data-anchor-id="making-predictions">Making predictions</h4>
<p>Once the coefficients have been estimated, we can compute the probability of default for any given credit card balance.</p>
<p><img src="files/images/4-logreg-prediction.png" class="img-fluid" style="width:50.0%"></p>
</section><section id="multiple-logistic-regression" class="level4"><h4 class="anchored" data-anchor-id="multiple-logistic-regression">Multiple logistic regression</h4>
<p>We now consider the problem of predicting a binary response using multiple predictors. By analogy with the extension from simple to multiple linear regression in the previous chapter, we can generalize <a href="#eq-logit" class="quarto-xref">Equation&nbsp;<span>14.2</span></a> as follows:</p>
<p><span id="eq-logit-multiple"><span class="math display">\[
\log\Big(\frac{p(X)}{1 - p(X)}\Big) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
\tag{14.4}\]</span></span></p>
<p>This can be rewritten as:</p>
<p><span id="eq-logistic-function-multiple"><span class="math display">\[
p(X) = \frac{\mathrm{e}^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}{1 + \mathrm{e}^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}
\tag{14.5}\]</span></span></p>
<p>Then we use maximum likelihood method as before. Beware of interpreting the effects (mainly the sign) of coefficients when some important predictors may be missing from your model. <em>Confounding</em> is an issue, just like in MLR.</p>
</section><section id="multinomial-logisitic-regression" class="level4"><h4 class="anchored" data-anchor-id="multinomial-logisitic-regression">Multinomial logisitic regression</h4>
<p>We sometimes wish to classify a response variable that has more than two classes. It turns out that it is possible to extend the two-class logistic regression approach to the setting of <span class="math inline">\(K &gt; 2\)</span> classes. This extension is sometimes known as <em>multinomial logistic regression</em>. To do this, we first select a single class to serve as the <em>baseline</em>; without loss of generality, we select the <span class="math inline">\(K\)</span>th class for this role. Then we replace the model <a href="#eq-logistic-function-multiple" class="quarto-xref">Equation&nbsp;<span>14.5</span></a> with the model</p>
<p><span class="math display">\[
p(Y = k \mid X = x) = \frac{\mathrm{e}^{\beta_{k0} + \beta_{k1} X_1 + \cdots + \beta_{kp} X_p}}{1 + \sum_{l = 1}^{K - 1}\mathrm{e}^{\beta_{l0} + \beta_{l1} X_1 + \cdots + \beta_{lp} X_p}}
\]</span></p>
<p>for <span class="math inline">\(k = 1, \ldots, K-1\)</span> and</p>
<p><span class="math display">\[
p(Y = K \mid X = x) = \frac{1}{1 + \sum_{l = 1}^{K - 1}\mathrm{e}^{\beta_{l0} + \beta_{l1} X_1 + \cdots + \beta_{lp} X_p}}
\]</span></p>
<p>It is not hard to show that for <span class="math inline">\(k = 1, \ldots, K-1\)</span>,</p>
<p><span id="eq-multinomial-logreg"><span class="math display">\[
\log\Big(\frac{P(Y = k \mid X = x)}{P(Y = K \mid X = x)}\Big) = \beta_{k0} + \beta_{k1} X_1 + \cdots + \beta_{kp} X_p
\tag{14.6}\]</span></span></p>
<p>Notice that this is quite similar to <a href="#eq-logit" class="quarto-xref">Equation&nbsp;<span>14.2</span></a> and indicates that once again, the log odds between any pair of classes is linear in the features. The decision of which class to have as the baseline is unimportant. If we were to change the baseline, the coefficient estimates would differ between the two fitted models, but the fitted values (predictions), the log odds between any pair of classes, and the other key model outputs will remain the same. Nonetheless, interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline.</p>
<p><img src="files/images/4-multi-logreg-coefs.png" class="img-fluid" style="width:50.0%"></p>
<p>Textbook presents an alternative coding for multinomial logistic regression, known as the <em>softmax</em> coding. This is used a lot in machine learning; skipping for now.</p>
</section></section><section id="generative-models-for-classification" class="level3" data-number="14.1.4"><h3 data-number="14.1.4" class="anchored" data-anchor-id="generative-models-for-classification">
<span class="header-section-number">14.1.4</span> Generative models for classification</h3>
<p><strong>Logistic regression involves directly modeling <span class="math inline">\(P(Y = k \mid X = x)\)</span> using the logistic function, given by eq-logistic-function-multiple for the case of two response classes.</strong> In statistical jargon, we model the conditional distribution of the response <span class="math inline">\(Y\)</span>, given the predictor(s) <span class="math inline">\(X\)</span>. We now consider an alternative and less direct approach to estimating these probabilities. <strong>In this new approach, we model the distribution of the predictors <span class="math inline">\(X\)</span> separately in each of the response classes (i.e.&nbsp;for each value of <span class="math inline">\(Y\)</span>). We then use Bayes’ theorem to flip these around into estimates for <span class="math inline">\(P(Y = k \mid X = x)\)</span>.</strong> When the distribution of <span class="math inline">\(X\)</span> within each class is assumed to be normal, it turns out that the model is very similar in form to logistic regression.</p>
<p>Why do we need another method, when we have logistic regression? There are several reasons:</p>
<ul>
<li><p>When there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable. The methods that we consider in this section do not suffer from this problem.</p></li>
<li><p>If the distribution of the predictors <span class="math inline">\(X\)</span> is approximately normal in each of the classes and the sample size is small, then the approaches in this section may be more accurate than logistic regression.</p></li>
<li><p>The methods in this section can be naturally extended to the case of more than two response classes.</p></li>
</ul>
<p>Suppose that we wish to classify an observation into one of <span class="math inline">\(K\)</span> classes, where <span class="math inline">\(K \ge 2\)</span>. In other words, the qualitative response variable <span class="math inline">\(Y\)</span> can take on $ K$ possible distinct and unordered values. Let <span class="math inline">\(\pi_k\)</span> represent the overall or prior probability that a randomly chosen observation comes from the <span class="math inline">\(k\)</span>th class. Let <span class="math inline">\(f_k(X) \equiv P(X \mid Y = k)\)</span> denote the <em>density function</em> of <span class="math inline">\(X\)</span> for an observation that comes from the <span class="math inline">\(k\)</span>th class. In other words, <span class="math inline">\(f_k(x)\)</span> is relatively large if there is a high probability that an observation in the <span class="math inline">\(k\)</span>th class has <span class="math inline">\(X \approx x\)</span>, and <span class="math inline">\(f_k(x)\)</span> is small if it is very unlikely that an observation in the <span class="math inline">\(k\)</span>th class has <span class="math inline">\(X \approx x\)</span>. Then Bayes’ theorem states that</p>
<p><span id="eq-bayes-theorem"><span class="math display">\[
P(Y = k \mid X = x) = \frac{\pi_k f_k(x)}{\sum_{l = 1}^K \pi_l f_l(x)}
\tag{14.7}\]</span></span></p>
<p>!!! draw tree picture</p>
<p>In accordance with our earlier notation, we will use the abbreviation <span class="math inline">\(p_k(x) = P(Y = k \mid X = x)\)</span>; this is the posterior probability that an observation <span class="math inline">\(X = x\)</span> belongs to the <span class="math inline">\(k\)</span>th class. That is, it is the probability that the observation belongs to the <span class="math inline">\(k\)</span>th class, <em>given</em> the predictor value for that observation.</p>
<p>The above equation suggests that instead of directly computing the posterior probability <span class="math inline">\(p_k(x)\)</span> as in <a href="#sec-logistic-model" class="quarto-xref"><span>Section 14.1.3.1</span></a>, we can simply plug in estimates of <span class="math inline">\(\pi_k\)</span> and <span class="math inline">\(f_k(x)\)</span> into <a href="#eq-bayes-theorem" class="quarto-xref">Equation&nbsp;<span>14.7</span></a>. In general, estimating <span class="math inline">\(\pi_k\)</span> is easy if we have a random sample from the population: we simply compute the fraction of the training observations that belong to the <span class="math inline">\(k\)</span>th class. However, estimating the density function <span class="math inline">\(f_k(x)\)</span> is much more challenging. As we will see, to estimate <span class="math inline">\(f_k(x)\)</span>, we will typically have to make some simplifying assumptions.</p>
<p>We know from earlier that the Bayes classifier, which classifies an observation <span class="math inline">\(x\)</span> to the class for which <span class="math inline">\(p_k(x)\)</span> is largest, has the lowest possible error rate out of all classifiers. (Of course, this is only true if all of the terms in <a href="#eq-bayes-theorem" class="quarto-xref">Equation&nbsp;<span>14.7</span></a> are correctly specified.) Therefore, if we can find a way to estimate <span class="math inline">\(f_k(x)\)</span>, then we can plug it into <a href="#eq-bayes-theorem" class="quarto-xref">Equation&nbsp;<span>14.7</span></a> in order to approximate the Bayes classifier.</p>
<p>In the following sections, we discuss three classifiers that use different estimates of <span class="math inline">\(f_k(x)\)</span> in <a href="#eq-bayes-theorem" class="quarto-xref">Equation&nbsp;<span>14.7</span></a> to approximate the Bayes classifier: <em>linear discriminant analysis</em>, <em>quadratic discriminant analysis</em>, and <em>naive Bayes</em>.</p>
<section id="linear-discrimant-analysis-for-p-1" class="level4"><h4 class="anchored" data-anchor-id="linear-discrimant-analysis-for-p-1">Linear discrimant analysis for <span class="math inline">\(p = 1\)</span>
</h4>
<p>For now, assume that <span class="math inline">\(p = 1\)</span> —- that is, we have only one predictor. We would like to obtain an estimate for <span class="math inline">\(f_k(x)\)</span> that we can plug into <a href="#eq-bayes-theorem" class="quarto-xref">Equation&nbsp;<span>14.7</span></a> in order to estimate <span class="math inline">\(p_k(x)\)</span>. We will then classify an observation to the class for which <span class="math inline">\(p_k(x)\)</span> is greatest. To estimate <span class="math inline">\(f_k(x)\)</span>, we will first make some assumptions about its form.</p>
<p>In particular, we assume that <span class="math inline">\(f_k(x)\)</span> is <em>normal</em> or <em>Gaussian.</em> In the one- normal dimensional setting, the normal density takes the form</p>
<p><span id="eq-normal-x"><span class="math display">\[
f_k(x) = \frac{1}{\sqrt{2\pi} \sigma_k} \exp\Big(-\frac{1}{2\sigma_k^2}(x - \mu_k)^2\Big)
\tag{14.8}\]</span></span></p>
<p>where <span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\sigma_k^2\)</span> are the mean and variance parameters for the <span class="math inline">\(k\)</span>th class. For now, let us further assume that <span class="math inline">\(\sigma_1^2 = \cdots = \sigma_K^2\)</span>, that is, there is a shared variance term across all <span class="math inline">\(K\)</span> classes, which for simplicity we can denote by <span class="math inline">\(\sigma^2\)</span>. Plugging this into <a href="#eq-bayes-theorem" class="quarto-xref">Equation&nbsp;<span>14.7</span></a>, we find that</p>
<p><span class="math display">\[
p_k(x) = \frac{\pi_k \frac{1}{\sqrt{2\pi} \sigma} \exp\big(-\frac{1}{2\sigma^2}(x - \mu_k)^2\big)}{\sum_{l = 1}^K \pi_l \frac{1}{\sqrt{2\pi} \sigma} \exp\big(-\frac{1}{2\sigma^2}(x - \mu_l)^2\big)}
\]</span></p>
<p>The Bayes classifier involves assigning an observation <span class="math inline">\(X = x\)</span> to the class for which the above equation is the largest (recall this Bayes classifier is different than Bayes theorem <a href="#eq-bayes-theorem" class="quarto-xref">Equation&nbsp;<span>14.7</span></a>, which allows us to manipulate conditional distributions). Taking the log of this and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which</p>
<p><span id="eq-bayes-decision-boundary"><span class="math display">\[
\delta_k(x) = x \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)
\tag{14.9}\]</span></span></p>
<p>is largest. For instance, if <span class="math inline">\(K = 2\)</span> and <span class="math inline">\(\pi_1 = \pi_2\)</span>, then the Bayes classifier assignes an observation to class 1 if <span class="math inline">\(2x(\mu_1 - \mu_2) &gt; \mu_1^2 - \mu_2^2\)</span> and to class 2 otherwise. The Bayes decision boundary is the point for which <span class="math inline">\(\delta_1(x) = \delta_2(x)\)</span>; one can show that this amounts to</p>
<p><span id="eq-bayes-decision-boundary2"><span class="math display">\[
x = \frac{\mu_1^2 - \mu_2^2}{2(\mu_1 - \mu_2)} = \frac{\mu_1 + \mu_2}{2}
\tag{14.10}\]</span></span></p>
<!-- !!! do this math -->
<p>An example is shown in the left-hand panel of Figure 4.4. The two normal density functions that are displayed, <span class="math inline">\(f_1(x)\)</span> and <span class="math inline">\(f_2(x)\)</span>, represent two distinct classes. The mean and variance parameters for the two density functions are <span class="math inline">\(\mu_1 = −1.25, \mu_2 = 1.25\)</span> and <span class="math inline">\(\sigma_1^2 = \sigma_2^2 = 1\)</span>. The two densities overlap, and so given that <span class="math inline">\(X = x\)</span>, there is some uncertainty about the class to which the observation belongs. If we assume that an observation is equally likely to come from either class – that is, <span class="math inline">\(\pi_1 = \pi_2 = 0.5\)</span> –then by inspection of <span class="citation" data-cites="bayes-decision-boundary2">@bayes-decision-boundary2</span>, we see that the Bayes classifier assigns the observation to class 1 if <span class="math inline">\(x &lt; 0\)</span> and class 2 otherwise. Note that in this case, we can compute the Bayes classifier because we know that <span class="math inline">\(X\)</span> is drawn from a Gaussian distribution within each class, and we know all of the parameters involved. In a real-life situation, we are not able to calculate the Bayes classifier.</p>
<p>In practice, even if we are quite certain of our assumption that <span class="math inline">\(X\)</span> is drawn from a Gaussian distribution within each class, to apply the Bayes classifier we still have to estimate the parameters <span class="math inline">\(\mu_1, \ldots, \mu_K, \pi_1, \ldots, \pi_K\)</span>, and <span class="math inline">\(\sigma^2\)</span>. The <em>linear discriminant analysis</em> (LDA) method approximates the Bayes classifier by plugging estimates for <span class="math inline">\(\pi_k\)</span>, <span class="math inline">\(\mu_k\)</span>, and <span class="math inline">\(\sigma^2\)</span> into <span class="citation" data-cites="bayes-decision-boundary">@bayes-decision-boundary</span>. In particular, the following estimates are used:</p>
<p><span class="math display">\[
\begin{align*}
\hat{\mu}_k &amp;= \frac{1}{n_k} \sum_{i:y_i = k} x_i\\
\hat{\sigma}^2 &amp;= \frac{1}{n - K} \sum_{k = 1}^K \sum_{i:y_i = k} (x_i - \hat{\mu}_k)^2\\
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(n\)</span> is the total number of training observations, and <span class="math inline">\(n_k\)</span> is the number of training observations in the <span class="math inline">\(k\)</span>th class. The estimate for <span class="math inline">\(\mu_k\)</span> is simply the average of all the training observations from the <span class="math inline">\(k\)</span>th class, while <span class="math inline">\(\sigma^2\)</span> can be seen as a weighted average of the sample variances for each of the <span class="math inline">\(K\)</span> classes. Sometimes we have knowledge of the class membership probabilities <span class="math inline">\(\pi_1,  \ldots, \pi_K\)</span>, which can be used directly. In the absence of any additional information, LDA estimates <span class="math inline">\(\pi_k\)</span> using the proportion of the training observations that belong to the <span class="math inline">\(k\)</span>th class. In other words,</p>
<p><span class="math display">\[
\hat{\pi}_k = n_k / n
\]</span></p>
<p>The LDA classifier plugs the estimates above into <a href="#eq-bayes-decision-boundary" class="quarto-xref">Equation&nbsp;<span>14.9</span></a> and assigns an observation <span class="math inline">\(X = x\)</span> to the class for which</p>
<p><span class="math display">\[
\hat{\delta}_k(x) = x \frac{\hat{\mu}_k}{\hat{\sigma}^2} - \frac{\hat{\mu}_k^2}{2\hat{\sigma}^2} + \log(\hat{\pi}_k)
\]</span></p>
<p>is largest. The word <em>linear</em> in the classifier’s name stems from the fact that the discriminant functions <span class="math inline">\(\hat{\delta}_k(x)\)</span> above are linear functions of <span class="math inline">\(x\)</span> (as discriminant opposed to a more complex function of <span class="math inline">\(x\)</span>).</p>
<p>The right-hand panel of Figure 4.4 displays a histogram of a random sample of 20 observations from each class. In this case, since <span class="math inline">\(n_1 = n_2 = 20\)</span>, we have <span class="math inline">\(\hat{\pi}_1 = \hat{\pi}_2\)</span>. As a result, the decision boundary corresponds to the midpoint between the sample means for the two classes, <span class="math inline">\((\hat{\mu}_1 + \hat{\mu}_2) / 2\)</span>.</p>
<p>To reiterate, the LDA classifier results from assuming that the observations within each class come from a normal distribution with a class-specific mean and a common variance <span class="math inline">\(\sigma^2\)</span>, and plugging estimates for these parameters into the Bayes classifier. In a later section, we will consider a less stringent set of assumptions, by allowing the observations in the <span class="math inline">\(k\)</span>th class to have a class-specific variance, <span class="math inline">\(\sigma_k^2\)</span>.</p>
</section><section id="linear-discriminant-analysis-for-p-1" class="level4"><h4 class="anchored" data-anchor-id="linear-discriminant-analysis-for-p-1">Linear discriminant analysis for <span class="math inline">\(p &gt; 1\)</span>
</h4>
<p>We now extend the LDA classifier to the case of multiple predictors. To do this, we will assume that <span class="math inline">\(X = (X_1, X_2, \ldots, X_p)\)</span> is drawn from a <em>multivariate Gaussian</em> (or multivariate normal) distribution, with a class-specific mean vector and a common covariance matrix.</p>
<p>The multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution, as in <a href="#eq-normal-x" class="quarto-xref">Equation&nbsp;<span>14.8</span></a>, with some correlation between each pair of predictors.</p>
<p><img src="files/images/4-multivariate-normal.png" class="img-fluid" style="width:50.0%"></p>
<p>To indicate that a <span class="math inline">\(p\)</span>-dimensional random variable <span class="math inline">\(X\)</span> has a multivariate Gaussian distribution, we write <span class="math inline">\(X \sim \text{N}\,(\mu, \Sigma)\)</span>. Here, <span class="math inline">\(E(X) = \mu\)</span> is the mean of <span class="math inline">\(X\)</span> (a vector with <span class="math inline">\(p\)</span> components), and <span class="math inline">\(\text{Cov}(X) = \Sigma\)</span> is the <span class="math inline">\(p \times p\)</span> covariance matrix of <span class="math inline">\(X\)</span>. Formally, the multivariate Gaussian density is defined as</p>
<p><span class="math display">\[
f(x) = \frac{1}{(2\pi)^{p/2} \lvert \Sigma \rvert^{1/2}} \exp\Big(-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu)\Big)
\]</span></p>
<p>In the case of <span class="math inline">\(p &gt; 1\)</span> predictors, the LDA classifier assumes that the observations in the <span class="math inline">\(k\)</span>th class are drawn from a multivariate Gaussian distribution <span class="math inline">\(N(\mu_k, \Sigma)\)</span>, where <span class="math inline">\(\mu_k\)</span> is a class-specific mean vector, and <span class="math inline">\(\Sigma\)</span> is a covariance matrix that is common to all <span class="math inline">\(K\)</span> classes. Plugging the density function for the <span class="math inline">\(k\)</span>th class, <span class="math inline">\(f_k(X = x)\)</span>, into <a href="#eq-bayes-theorem" class="quarto-xref">Equation&nbsp;<span>14.7</span></a> and performing a little bit of algebra reveals that the Bayes classifier assigns an observation <span class="math inline">\(X = x\)</span> to the class for which</p>
<p><span id="eq-bayes-decision-boundary3"><span class="math display">\[
\delta_k(x) = x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T \Sigma^{-1}\mu_k + \log \pi_k
\tag{14.11}\]</span></span></p>
<p>is largest. This is the vector / matrix version of <a href="#eq-bayes-decision-boundary" class="quarto-xref">Equation&nbsp;<span>14.9</span></a>.</p>
<p>An example is shown in the left-hand panel of Figure 4.6.</p>
<p><img src="files/images/4-lda-example.png" class="img-fluid" style="width:50.0%"></p>
<p>Three equally-sized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix. The three ellipses represent regions that contain 95% of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries. In other words, they represent the set of values x for which <span class="math inline">\(\delta_k(x) = \delta_l(x)\)</span>; i.e.</p>
<p><span class="math display">\[
x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T \Sigma^{-1}\mu_k = x^T \Sigma^{-1}\mu_l - \frac{1}{2}\mu_l^T \Sigma^{-1}\mu_l
\]</span></p>
<p>for <span class="math inline">\(k \ne l\)</span>. (The <span class="math inline">\(\log \pi_k\)</span>) term has disappeared because each of the three classes has the same number of training observations). Note that there are three lines representing the Bayes decision boundaries because there are three <em>pairs of classes</em> among the three classes. That is, one Bayes decision boundary separates class 1 from class 2, one separates class 1 from class 3, and one separates class 2 from class 3. These three Bayes decision boundaries divide the predictor space into three regions. The Bayes classifier will classify an observation according to the region in which it is located.</p>
<p>Once again, estimate the all the unknown parameters, similarly to how performed with <span class="math inline">\(p = 1\)</span> and plug into <a href="#eq-bayes-decision-boundary3" class="quarto-xref">Equation&nbsp;<span>14.11</span></a>. Then assign the class to that which results in the largest <span class="math inline">\(\hat{\delta}_k(x)\)</span>. Note that this is still a <em>linear</em> function of <span class="math inline">\(x\)</span></p>
<p>Results of classification can be compactly shown in <em>confusion matrices</em>.</p>
<p><img src="files/images/4-confusion-matrix.png" class="img-fluid" style="width:50.0%"></p>
<p>For the default data, the LDA model fit to the 10,000 training samples results in a training error rate of 2.75%. This sounds like a low error rate, but two caveats must be noted.</p>
<ul>
<li><p>First of all, training error rates will usually be lower than test error rates, which are the real quantity of interest.</p></li>
<li><p>Second, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that an individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3.33 %. In other words, the trivial null classifier will achieve an error rate that null is only a bit higher than the LDA training set error rate.</p></li>
</ul>
<p>In practice, a binary classifier such as this one can make two types of errors: it can incorrectly assign an individual who defaults to the no default category, or it can incorrectly assign an individual who does not default to the default category. It is often of interest to determine which of these two types of errors are being made. The table reveals that LDA predicted that a total of 104 people would default. Of these people, 81 actually defaulted and 23 did not. Hence only 23 out of 9,667 of the individuals who did not default were incorrectly labeled. This looks like a pretty low error rate! However, of the 333 individuals who defaulted, 252 (or 75.7%) were missed by LDA. <strong>So while the overall error rate is low, the error rate among individuals who defaulted is very high.</strong></p>
<p>Also, notice that in this example student status is qualitative – thus, the normality assumption made by LDA is clearly violated! <strong>However, LDA is often remarkably robust to model violations, as this example shows. Naive Bayes, discussed later, provides an alternative to LDA that does not assume normally distributed predictors.</strong></p>
<p>Class-specific performance is also important in medicine and biology, where the terms <em>sensitivity</em> and <em>specificity</em> characterize the performance of a classifier or screening test.</p>
<ul>
<li><p>Sensitivity = true positive % (e.g.&nbsp;true defaulters that are identified; 81/333 = 24.3%)</p></li>
<li><p>Specificity = true negative % (e.g.&nbsp;true non-defaulters that are identified; 9644/9667 = 1 - 23/9667 = 99.8%)</p></li>
</ul>
<p>Why does LDA do such a poor job of classifying the customers who default? In other words, why does it have such low sensitivity? As we have seen, LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers. That is, the Bayes classifier will yield the smallest possible total number of misclassified observations, <em>regardless of the class from which the errors stem</em>. Some misclassifications will result from incorrectly assigning a customer who does not default to the default class, and others will result from incorrectly assigning a customer who defaults to the non-default class. In contrast, a credit card company might particularly wish to avoid incorrectly classifying an individual who will default, whereas incorrectly classifying an individual who will not default, though still to be avoided, is less problematic. We will now see that it is possible to modify LDA in order to develop a classifier that better meets the credit card company’s needs.</p>
<p>The Bayes classifier works by assigning an observation to the class for which the posterior probability <span class="math inline">\(p_k(X)\)</span> is greatest. In the two-class case, this amounts to assigning an observation to the default class if</p>
<p><span class="math display">\[
P(\text{default} = \text{Yes} \mid X = x) &gt; 0.5
\]</span></p>
<p>Thus, the Bayes classifier, and by extension LDA, uses a threshold of 50 % for the posterior probability of default in order to assign an observation to the <em>default</em> class. However, if we are concerned about incorrectly predicting the default status for individuals who default, then we can consider lowering this threshold. For instance, we might label any customer with a posterior probability of default above 20% to the <em>default</em> class:</p>
<p><span class="math display">\[
P(\text{default} = \text{Yes} \mid X = x) &gt; 0.2
\]</span></p>
<p>The error rates that result from taking this approach are shown in Table 4.5. Now LDA predicts that 430 individuals will default. Of the 333 individuals who default, LDA correctly predicts all but 138, or 41.4 %. This is a vast improvement over the error rate of 75.7% that resulted from using the threshold of 50%. However, this improvement comes at a cost: now 235 individuals who do not default are incorrectly classified. As a result, the overall error rate has increased slightly to 3.73 %.</p>
<p><img src="files/images/4-confusion-matrix2.png" class="img-fluid" style="width:50.0%"></p>
<p>Figure 4.7 illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. Various error rates are shown as a function of the threshold value. How can we decide which threshold value is best? Such a decision must be based on <em>domain knowledge</em>, such as detailed information about the costs associated with default.</p>
<p>Increasing threshold to say 80% causes:</p>
<ul>
<li><p><strong>Sensitivity to DECREASE and specificity to INCREASE (<a href="[https://workshops.tidymodels.org/slides/intro-04-evaluating-models.html#/two-class-data">good source, for modelling too</a>).</strong></p></li>
<li><p>This means higher proportion of true negatives, with the cost of lower proportion of true positives.</p></li>
<li><p>Predict more as no (harder to predict as yes), so capture more true no’s, but miss some of the yes’s.</p></li>
<li><p>Just flip below: False positive rate decreases, but the false negative rate increases.</p></li>
</ul>
<p>Thus, decreasing the threshold:</p>
<ul>
<li><p>Just flip above: Sensitivity increases and specificity decreases.</p></li>
<li>
<p>Error rates:</p>
<ul>
<li><p>Decreases the false negative rate (error rate among defaulters), because more confident in the negatives.</p></li>
<li><p>But increases the false positive rate (error rate among non-defaulters), because easier to be classified as yes.</p></li>
<li><p>Thus increasing the threshold **</p></li>
</ul>
</li>
</ul>
<p><img src="files/images/4-comparing-thresholds.png" class="img-fluid" style="width:50.0%"></p>
<p>The <em>ROC curve</em> is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. The overall performance of a classifier, summarized over all possible thresholds, is given by the <em>area under the (ROC) curve</em> (AUC). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. We expect a classifier that performs no better than chance to have an AUC of 0.5 (when evaluated on an independent test set not used in model training). ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds. As we have seen above, varying the classifier threshold changes its true positive and false positive rate. These are also called the <em>sensitivity</em> and one minus the <em>specificity</em> of our classifier.</p>
<p><img src="files/images/4-roc-curve.png" class="img-fluid" style="width:50.0%"></p>
<p>Here is a summary of the terms in this section. To make the connection with the epidemiology literature, we think of “+” as the “disease” that we are trying to detect, and “−” as the “non-disease” state. To make the connection to the classical hypothesis testing literature, we think of “−” as the null hypothesis and “’+” as the alternative (non-null) hypothesis. In the context of the Default data, “+” indicates an individual who defaults, and “−” indicates one who does not.</p>
<p><img src="files/images/4-definitions.png" class="img-fluid" style="width:50.0%"></p>
<ul>
<li><p>Sensitivity = true positive rate</p></li>
<li><p>Specificity = true negative rate</p></li>
</ul></section><section id="quadratic-discriminant-analysis" class="level4"><h4 class="anchored" data-anchor-id="quadratic-discriminant-analysis">Quadratic discriminant analysis</h4>
<p>As we have discussed, LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a covariance matrix that is common to all <span class="math inline">\(K\)</span> classes. <em>Quadratic discriminant analysis</em> (QDA) provides an alternative approach. Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction. However, unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form <span class="math inline">\(X \sim N(\mu_k,\Sigma_k)\)</span>, where <span class="math inline">\(\Sigma_k\)</span> is a covariance matrix for the <span class="math inline">\(k\)</span>th class. Under this assumption, the Bayes classifier assigns an observation <span class="math inline">\(X = x\)</span> to the class for which</p>
<p><img src="files/images/4-qda-formula.png" class="img-fluid" style="width:50.0%"></p>
<p>is the largest. Unlike before, the quantity <span class="math inline">\(x\)</span> appears as a quadratic function now.</p>
<p>Why does it matter whether or not we assume that the K classes share a common covariance matrix? In other words, why would one prefer LDA to QDA, or vice-versa? The answer lies in the bias-variance trade-off. When there are <span class="math inline">\(p\)</span> predictors, then estimating a covariance matrix requires estimating <span class="math inline">\(p(p+1)/2\)</span> parameters (lower triangle + diagonal). QDA estimates a separate covariance matrix for each class, for a total of <span class="math inline">\(Kp(p+1)/2\)</span> parameters. With 50 predictors this is some multiple of 1,275, which is a lot of parameters. By instead assuming that the <span class="math inline">\(K\)</span> classes share a common covariance matrix, the LDA model becomes linear in <span class="math inline">\(x\)</span>, which means there are <span class="math inline">\(Kp\)</span> linear coefficients to estimate. Consequently, LDA is a much less flexible classifier than QDA, and so has substantially lower variance. This can potentially lead to improved prediction performance. But there is a trade-off: if LDA’s assumption that the <span class="math inline">\(K\)</span> classes share a common covariance matrix is badly off, then LDA can suffer from high bias. <strong>Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the <span class="math inline">\(K\)</span> classes is clearly untenable.</strong></p>
<!-- ??? how to estimate sigma for LDA? why p parameters -->
<p>Figure 4.9 illustrates the performances of LDA and QDA in two scenarios. In the left-hand panel, the two Gaussian classes have a common correlation of 0.7 between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. As a result, the Bayes decision boundary is linear and is accurately approximated by the LDA decision boundary. The QDA decision boundary is inferior, because it suffers from higher variance without a corresponding decrease in bias. In contrast, the right-hand panel displays a situation in which the orange class has a correlation of 0.7 between the variables and the blue class has a correlation of −0.7. Now the Bayes decision boundary is quadratic, and so QDA more accurately approximates this boundary than does LDA.</p>
<p><img src="files/images/4-qda-example.png" class="img-fluid" style="width:50.0%"></p>
</section><section id="naive-bayes" class="level4"><h4 class="anchored" data-anchor-id="naive-bayes">Naive Bayes</h4>
<p>Here, we use Bayes’ theorem to motivate the popular naive Bayes classifier. Recall that Bayes’ theorem <a href="#eq-bayes-theorem" class="quarto-xref">Equation&nbsp;<span>14.7</span></a> provides an expression for the posterior probability <span class="math inline">\(p_k(x) = P(Y = k \mid X = x)\)</span> in terms of <span class="math inline">\(\pi_1, \ldots, \pi_K\)</span> and <span class="math inline">\(f_k(x), \ldots, f_K(x)\)</span>. To use this in practice, we need estimates for for these terms. Estimating the prior probabilities <span class="math inline">\(\pi_k\)</span> is typically straightforward.</p>
<p>However estimating <span class="math inline">\(f_k(x)\)</span> is more subtle. Recall that <span class="math inline">\(f_k(x)\)</span> is the <span class="math inline">\(p\)</span>-dimensional density function for an observation in the <span class="math inline">\(k\)</span>th class, for <span class="math inline">\(k = 1, \ldots, K\)</span>. In general, estimating a <span class="math inline">\(p\)</span>-dimensional density function is challenging. In LDA, we make a very strong assumption that greatly simplifies the task: we assume that <span class="math inline">\(f_k\)</span> is the density function for a multivariate normal random variable with class-specific mean <span class="math inline">\(\mu_k\)</span>, and shared covariance matrix <span class="math inline">\(\Sigma\)</span>. By contrast, in QDA we change it to have class-specific covariance matrix <span class="math inline">\(\Sigma_k\)</span>. By making these very strong assumptions, we are able to replace the very challenging problem of estimating <span class="math inline">\(K\)</span> <span class="math inline">\(p\)</span>-dimensional density functions with the much simpler problem of estimating <span class="math inline">\(K\)</span> <span class="math inline">\(p\)</span>-dimensional mean vectors and one (in the case of LDA) or <span class="math inline">\(K\)</span> (in the case of QDA) <span class="math inline">\((p \times p)\)</span>-dimensional covariance matrices.</p>
<p>The naive Bayes classifier takes a different tack for estimating <span class="math inline">\(f_1(x), \ldots, f_K(x)\)</span>. Instead of assuming that these functions belong to a particular family of distributions (e.g.&nbsp;multivariate normal), we instead make a single assumption:</p>
<p><strong>Within the kth class, the <span class="math inline">\(p\)</span> predictors are independent.</strong></p>
<p>Stated mathematically, this assumption means that for <span class="math inline">\(k = 1, \ldots, K\)</span>,</p>
<p><span id="eq-naive-bayes-assumption"><span class="math display">\[
f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times \cdots \times f_{kp}(x_p)
\tag{14.12}\]</span></span></p>
<p>where <span class="math inline">\(f_{kj}\)</span> is the density function of the <span class="math inline">\(j\)</span>th predictor among observations in the <span class="math inline">\(k\)</span>th class.</p>
<p>Why is this assumption so powerful? Essentially, estimating a <span class="math inline">\(p\)</span>-dimensional density function is challenging because we must consider not only the marginal distribution of each predictor – that is, the distribution of each predictor on its own – but also the <em>joint distribution</em> of the predictors – that is, the association between the different predictors. In the case of a multivariate normal distribution, the association between the different predictors is summarized by the off-diagonal elements of the covariance matrix. However, in general, this association can be very hard to characterize, and exceedingly challenging to estimate. But by assuming that the <span class="math inline">\(p\)</span> covariates are independent within each class, we completely eliminate the need to worry about the association between the <span class="math inline">\(p\)</span> predictors, because we have simply assumed that there is no association between the predictors!</p>
<p>Do we really believe the naive Bayes assumption that the <span class="math inline">\(p\)</span> covariates are independent within each class? In most settings, we do not. <strong>But even though this modeling assumption is made for convenience, it often leads to pretty decent results, especially in settings where <span class="math inline">\(n\)</span> is not large enough relative to <span class="math inline">\(p\)</span> for us to effectively estimate the joint distribution of the predictors within each class. In fact, since estimating a joint distribution requires such a huge amount of data, naive Bayes is a good choice in a wide range of settings.</strong> Essentially, the naive Bayes assumption introduces some bias, but reduces variance, leading to a classifier that works quite well in practice as a result of the bias-variance trade-off.</p>
<p>Once we have made the naive Bayes assumption, we can plug <a href="#eq-naive-bayes-assumption" class="quarto-xref">Equation&nbsp;<span>14.12</span></a> into <a href="#eq-bayes-theorem" class="quarto-xref">Equation&nbsp;<span>14.7</span></a> to obtain an expression for the posterior probability,</p>
<p><span class="math display">\[
P(Y = k \mid X = x) = \frac{\pi_k \times f_{k1}(x_1) \times \cdots \times f_{kp}(x_p)}{\sum_{l = 1}^K \pi_l \times f_{l1}(x_1) \times \cdots \times f_{lp}(x_p)}
\]</span></p>
<p>for <span class="math inline">\(k = 1, \ldots, K\)</span>.</p>
<p>To estimate the one-dimensional density function <span class="math inline">\(f_{kj}\)</span> using training data <span class="math inline">\(x_{1j}, \cdots, x_{nj}\)</span>, we have a few options.</p>
<ul>
<li><p>If <span class="math inline">\(X_j\)</span> is quantitative, then we can assume that <span class="math inline">\(X_j \mid Y = k \sim N(\mu_{jk}, \sigma_{jk}^2)\)</span>. In other words, we assume that within each class, the <span class="math inline">\(j\)</span>th predictor is drawn from a (univariate) normal distribution. While this may sound a bit like QDA, there is one key difference, in that here we are assuming that the predictors are independent; this amounts to QDA with an additional assumption that the class-specific covariance matrix is diagonal.</p></li>
<li><p>If <span class="math inline">\(X_j\)</span> is quantitative, then another option is to use a non-parametric estimate for <span class="math inline">\(f_{kj}\)</span>. A very simple way to do this is by making a histogram for the observations of the <span class="math inline">\(j\)</span>th predictor within each class. Then we can estimate <span class="math inline">\(f_{kj}(x_j)\)</span> as the fraction of the training observations in the <span class="math inline">\(k\)</span>th class that belong to the same histogram bin as <span class="math inline">\(x_j\)</span>. Alternatively, we can use a kernel density estimator, which is essentially a smoothed version of a histogram.</p></li>
<li><p>If <span class="math inline">\(X_j\)</span> is qualitative, then we can simply count the proportion of training observations for the <span class="math inline">\(j\)</span>th predictor corresponding to each class. For instance, suppose that <span class="math inline">\(X_j \in \{1, 2, 3\}\)</span>, and we have 100 observations in the <span class="math inline">\(k\)</span>th class. Suppose that the <span class="math inline">\(j\)</span>th predictor takes on values of 1, 2, and 3 in 32, 55, and 13 of those observations, respectively. Then we can estimate <span class="math inline">\(f_{kj}\)</span> as</p></li>
</ul>
<p><span class="math display">\[
\hat{f}_{kj}(x_j) =
\begin{cases}
0.32 &amp; \text{if } x_j = 1\\
0.55 &amp; \text{if } x_j = 2\\
0.13 &amp; \text{if } x_j = 3\\
\end{cases}
\]</span></p>
<p>Just as with LDA, we can easily adjust the probability threshold for predicting a default. In this example, it should not be too surprising that naive Bayes does not convincingly outperform LDA: this data set has <span class="math inline">\(n = 10,000\)</span> and <span class="math inline">\(p = 4\)</span>, and so the reduction in variance resulting from the naive Bayes assumption is not necessarily worthwhile. We expect to see a greater pay-off to using naive Bayes relative to LDA or QDA in instances where <span class="math inline">\(p\)</span> is larger or <span class="math inline">\(n\)</span> is smaller, so that reducing the variance is very important.</p>
</section></section><section id="a-comparison-of-classification-methods" class="level3" data-number="14.1.5"><h3 data-number="14.1.5" class="anchored" data-anchor-id="a-comparison-of-classification-methods">
<span class="header-section-number">14.1.5</span> A comparison of classification methods</h3>
<section id="an-analytical-comparison" class="level4"><h4 class="anchored" data-anchor-id="an-analytical-comparison">An analytical comparison</h4>
<p>We now perform an <em>analytical</em> (or mathematical) comparison of LDA, QDA, naive Bayes, and logistic regression. We consider these approaches in a setting with K classes, so that we assign an observation to the class that maximizes <span class="math inline">\(P(Y = k \mid X = x)\)</span>. Equivalently, we can set <span class="math inline">\(K\)</span> as the baseline class and assign an observation to the class that maximizes</p>
<p><span class="math display">\[
\log\Big(\frac{P(Y = k \mid X = x)}{P(Y = K \mid X = x)}\Big)
\]</span></p>
<p>for <span class="math inline">\(k = 1, \ldots, K\)</span>. Examining the specific form of this equation for each method provides a clear understanding of their similarities and differences.</p>
<p><img src="files/images/4-lda-derivation.png" class="img-fluid" style="width:50.0%"></p>
<p>where <span class="math inline">\(a_k = \log\Big(\frac{\pi_k}{\pi_K}\Big) - \frac{1}{2}(\mu_k + \mu_K)^T \Sigma^{-1}(\mu_k - \mu_K)\)</span> and <span class="math inline">\(b_{kj}\)</span> is the <span class="math inline">\(j\)</span>th component of <span class="math inline">\(\Sigma^{-1}(\mu_k - \mu_K)\)</span>. <strong>Hence LDA, like logistic regression, assumes that the log odds of the posterior probabilities is linear in <span class="math inline">\(x\)</span>.</strong></p>
<p>Using similar calculations, in the QDA setting this becomes</p>
<p><img src="files/images/4-qda-derivation.png" class="img-fluid" style="width:50.0%"></p>
<p>Again, as the name suggests, QDA assumes that the log odds of the posterior probabilities is quadratic in <span class="math inline">\(x\)</span>.</p>
<p>Finally with naive Bayes setting, we get</p>
<p><img src="files/images/4-naive-bayes-derivation.png" class="img-fluid" style="width:50.0%"></p>
<p>where <span class="math inline">\(a_k = \log\big(\frac{\pi_k}{\pi_K}\big)\)</span> and <span class="math inline">\(g_{kj}(x_j) = \log\big(\frac{f_{kj}(x_j)}{f_{Kj}(x_j)}\big)\)</span>. Hence, the right-hand side of above takes the form of a <em>generalized additive model</em>, which will be discussed later.</p>
<p>Inspection of the above results yields the following observations about LDA, QDA, and naive Bayes:</p>
<ul>
<li><p>LDA is a special case of QDA with <span class="math inline">\(c_{kjl} = 0\)</span> for all <span class="math inline">\(j = 1, \ldots, p, l = 1, \ldots, p\)</span>, and <span class="math inline">\(k = 1, \ldots, K\)</span>. (Of course, this is not surprising, since LDA is simply a restricted version of QDA with <span class="math inline">\(\Sigma_1 = \cdots = \Sigma_K = \Sigma\)</span>.)</p></li>
<li><p>Any classifier with a linear decision boundary is a special case of naive Bayes with <span class="math inline">\(g_{kj}(x_j) = b_{kj}x_j\)</span>. In particular, this means that LDA is a special case of naive Bayes! This is not at all obvious from the descriptions of LDA and naive Bayes earlier in the chapter, since each method makes very different assumptions: LDA assumes that the features are normally distributed with a common within-class covariance matrix, and naive Bayes instead assumes independence of the features.</p></li>
<li><p>If we model <span class="math inline">\(f_{kj}(x_j)\)</span> in the naive Bayes classifier using a one-dimensional Gaussian distribution <span class="math inline">\(N(\mu_{kj}, \sigma_j^2)\)</span>, then we end up with <span class="math inline">\(g_{kj}(x_j) = b_{kj}x_j\)</span> where <span class="math inline">\(b_{kj} = (\mu_{kj} - \mu_{Kj}) / \sigma_j^2\)</span>. In this case, naive Bayes is actually a special case of LDA with <span class="math inline">\(\Sigma\)</span> restricted to be a diagonal matrix with <span class="math inline">\(j\)</span>th diagonal element equal to <span class="math inline">\(\sigma_j^2\)</span>.</p></li>
<li><p>Neither QDA nor naive Bayes is a special case of the other. Naive Bayes can produce a more flexible fit, since any choice can be made for <span class="math inline">\(g_{kj}(x_j)\)</span>. However, it is restricted to a purely additive fit, in the sense that in the final derivation, a function of <span class="math inline">\(x_j\)</span> is added to a function of <span class="math inline">\(x\)</span>, for <span class="math inline">\(j \ne l\)</span>; however, these terms are never multiplied. By contrast, QDA includes multiplicative terms of the form <span class="math inline">\(c_{kjl} x_j x_l\)</span>. <strong>Therefore, QDA has the potential to be more accurate in settings where interactions among the predictors are important in discriminating between classes.</strong></p></li>
</ul>
<p>None of these methods uniformly dominates the others: in any setting, the choice of method will depend on the true distribution of the predictors in each of the <span class="math inline">\(K\)</span> classes, as well as other considerations, such as the values of <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>. The latter ties into the bias-variance trade-off.</p>
<p>How does logistic regression tie into this story? Recall from <a href="#eq-multinomial-logreg" class="quarto-xref">Equation&nbsp;<span>14.6</span></a> that multinomial logistic regression takes the form</p>
<p><span class="math display">\[
\log\Big(\frac{P(Y = k \mid X = x)}{P(Y = K \mid X = x)}\Big) = \beta_{k0} + \sum_{j=1}^p \beta_{kj} x_j
\]</span></p>
<p>This is identical to the <em>linear form</em> of LDA the derivation: in both cases $() $ is a linear function of the predictors. In LDA, the coefficients in this linear function are functions of estimates for <span class="math inline">\(\pi_k\)</span>, <span class="math inline">\(\pi_K\)</span>, <span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(\mu_K\)</span>, and <span class="math inline">\(\Sigma\)</span> obtained by assuming that <span class="math inline">\(X_1,\ldots,X_p\)</span> follow a normal distribution within each class. By contrast, in logistic regression, the coefficients are chosen to maximize the likelihood function <a href="#eq-logistic-likelihood" class="quarto-xref">Equation&nbsp;<span>14.3</span></a>. <strong>Thus, we expect LDA to outperform logistic regression when the normality assumption (approximately) holds, and we expect logistic regression to perform better when it does not.</strong></p>
<p>We close with a brief discussion of <span class="math inline">\(K\)</span>-nearest neighbors (KNN), introduced in earlier. Recall that KNN takes a completely different approach from the classifiers seen in this chapter. In order to make a prediction for an observation <span class="math inline">\(X = x\)</span>, the training observations that are closest to <span class="math inline">\(x\)</span> are identified. Then <span class="math inline">\(X\)</span> is assigned to the class to which the plurality of these observations belong. Hence KNN is a completely non-parametric approach: no assumptions are made about the shape of the decision boundary. We make the following observations about KNN:</p>
<ul>
<li><p><strong>Because KNN is completely non-parametric, we can expect this ap- proach to dominate LDA and logistic regression when the decision boundary is highly non-linear, provided that n is very large and p is small.</strong></p></li>
<li><p><strong>In order to provide accurate classification, KNN requires a lot of observations relative to the number of predictors</strong> – that is, <span class="math inline">\(n\)</span> much larger than <span class="math inline">\(p\)</span>. This has to do with the fact that KNN is non-parametric, and thus tends to reduce the bias while incurring a lot of variance.</p></li>
<li><p><strong>In settings where the decision boundary is non-linear but <span class="math inline">\(n\)</span> is only modest, or <span class="math inline">\(p\)</span> is not very small, then QDA may be preferred to KNN</strong>. This is because QDA can provide a non-linear decision boundary while taking advantage of a parametric form, which means that it requires a smaller sample size for accurate classification, relative to KNN.</p></li>
<li><p>Note that the of <span class="math inline">\(K\)</span> in KNN is really important and is often chosen via <em>cross-validation</em>.</p></li>
<li><p>Unlike logistic regression, KNN does not tell us which predictors are important: we don’t get a table of coefficients as in Table 4.3.</p></li>
</ul></section><section id="an-empirical-comparison" class="level4"><h4 class="anchored" data-anchor-id="an-empirical-comparison">An empirical comparison</h4>
<p>Checkout section 4.5.2 of textbook for some simulation results. This could help when determining which model to use for a particular situation.</p>
<ul>
<li><p><strong>When the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well.</strong></p></li>
<li><p><strong>When the boundaries are moderately non-linear, QDA or naive Bayes may give better results.</strong></p></li>
<li><p><strong>Finally, for much more complicated decision boundaries, a non-parametric approach such as KNN can be superior. But the level of smoothness for a non-parametric approach must be chosen carefully.</strong></p></li>
</ul>
<p>In the next chapter we examine a number of approaches for choosing the correct level of smoothness and, in general, for selecting the best overall method.</p>
<p>Finally, recall from the previous chapter that in the regression setting we can accommodate a non-linear relationship between the predictors and the response by performing regression using transformations of the predictors. A similar approach could be taken in the classification setting. For instance, we could create a more flexible version of logistic regression by including <span class="math inline">\(X^2\)</span>, <span class="math inline">\(X^3\)</span>, and even <span class="math inline">\(X^4\)</span> as predictors. This may or may not improve logistic regression’s performance, depending on whether the increase in variance due to the added flexibility is offset by a sufficiently large reduction in bias. We could do the same for LDA. If we added all possible quadratic terms and cross-products to LDA, the form of the model would be the same as the QDA model, although the parameter estimates would be different. This device allows us to move somewhere between an LDA and a QDA model.</p>
</section></section><section id="generalized-linear-models" class="level3" data-number="14.1.6"><h3 data-number="14.1.6" class="anchored" data-anchor-id="generalized-linear-models">
<span class="header-section-number">14.1.6</span> Generalized linear models</h3>
<p>In the previous chapter, we assumed that the response <span class="math inline">\(Y\)</span> is quantitative, and explored the use of least squares linear regression to predict <span class="math inline">\(Y\)</span>. Thus far in this chapter, we have instead assumed that <span class="math inline">\(Y\)</span> is qualitative. However, we may sometimes be faced with situations in which <span class="math inline">\(Y\)</span> is neither qualitative nor quantitative, and so neither linear regression nor the classification approaches is applicable.</p>
<p>This occurs when we are working with <em>count</em> data (non-negative integers). If we fit linear regression to count data, here are some issues that can occur:</p>
<ul>
<li><p>Can predict negative counts. This calls into question our ability to perform meaningful predictions on the data, and it also raises concerns about the accuracy of the coefficient estimates, confidence intervals, and other outputs of the regression model.</p></li>
<li><p>Furthermore, it is reasonable to suspect that with the expected value of the response is small, the variance of the response should be small as well; however, when the expected value of counts is large, the variance should increase as well. This is a major violation of the assumptions of a linear model, which state that <span class="math inline">\(Y = \sum_{j=1}^p X_j \beta_j + \epsilon\)</span>, where <span class="math inline">\(\epsilon\)</span> is a mean-zero error term with variance <span class="math inline">\(\sigma^2\)</span> that is constant, and not a function of the covariates. Therefore, the heteroscedasticity of the data calls into question the suitability of a linear regression model. This is bad:</p></li>
</ul>
<p><img src="files/images/4-error-variance-violation.png" class="img-fluid" style="width:50.0%"></p>
<ul>
<li>Finally, with a continuous-valued error term, we can get a continuous-valued response, which doesn’t match the scenario.</li>
</ul>
<p>Some of the problems that arise when fitting a linear regression model to count data can be overcome by transforming the response; for instance, we can fit the model</p>
<p><span class="math display">\[
\log(Y) = \sum_{j=1}^p X_j \beta_j = \epsilon
\]</span></p>
<p>Transforming the response avoids the possibility of negative predictions, and it overcomes much of the heteroscedasticity in the untransformed data, as is shown in the right-hand panel of Figure 4.14. However, it is not quite a satisfactory solution, since predictions and inference are made in terms of the log of the response, rather than the response. This leads to challenges in interpretation, e.g.&nbsp;“a one-unit increase in <span class="math inline">\(X_j\)</span> is associated with an increase in the mean of the log of <span class="math inline">\(Y\)</span> by an amount <span class="math inline">\(\beta_j\)</span>”. Furthermore, a log transformation of the response cannot be applied in settings where the response can take on a value of 0. Thus, while fitting a linear model to a transformation of the response may be an adequate approach for some count-valued data sets, it often leaves something to be desired. We will see in the next section that a Poisson regression model provides a much more natural and elegant approach for this task.</p>
<section id="poisson-regression" class="level4"><h4 class="anchored" data-anchor-id="poisson-regression">Poisson regression</h4>
<p>To overcome the inadequacies of linear regression for analyzing count data, we will make use of an alternative approach, called <em>Poisson regression</em>. Recall if <span class="math inline">\(Y \sim \text{Poisson}\,(\lambda)\)</span>, then</p>
<p><span id="eq-poisson"><span class="math display">\[
P(Y = k) = \frac{\mathrm{e}^{-\lambda}\lambda^k}{k!}, \quad \text{for } k = 1, 2, \ldots
\tag{14.13}\]</span></span></p>
<p>Here, <span class="math inline">\(\lambda &gt; 0\)</span> is the expected value of <span class="math inline">\(Y\)</span>, i.e.&nbsp;<span class="math inline">\(E(Y)\)</span>. It turns out that <span class="math inline">\(\lambda\)</span> also equals the variance of <span class="math inline">\(Y\)</span> , i.e.&nbsp;<span class="math inline">\(\lambda = E(Y) = V(Y)\)</span>. This means that if <span class="math inline">\(Y\)</span> follows the Poisson distribution, then the larger the mean of <span class="math inline">\(Y\)</span>, the larger its variance.</p>
<p>The Poisson distribution is typically used to model counts. To see how we might use the Poisson distribution in practice, let <span class="math inline">\(Y\)</span> denote the number of users of the bike sharing program during a particular hour of the day, under a particular set of weather conditions, and during a particular month of the year. We might model <span class="math inline">\(Y\)</span> as a Poisson distribution with mean <span class="math inline">\(E(Y) = \lambda = 5\)</span>. This means that the probability of no users during this particular hour is <span class="math inline">\(P(Y = 0) = \frac{\mathrm{e}^{-5} 5^0}{0!} = e^{-5} = 0.0067\)</span>. Of course, in reality, we expect the mean number of users of the bike sharing program, <span class="math inline">\(\lambda = E(Y)\)</span>, to vary as a function of the hour of the day, the month of the year, the weather conditions, and so forth. So rather than modeling the number of bikers, <span class="math inline">\(Y\)</span>, as a Poisson distribution with a <em>fixed</em> mean value like <span class="math inline">\(\lambda = 5\)</span>, we would like to <em>allow the mean to vary as a function of the covariates</em>. In particular, we consider the following model for the mean <span class="math inline">\(\lambda = E(Y)\)</span>, which we now write as <span class="math inline">\(\lambda(X_1, \ldots, X_p)\)</span> to emphasize that it is a function of the covariates <span class="math inline">\(X_1, \ldots, X_p\)</span>:</p>
<p><span id="eq-log-lambda"><span class="math display">\[
\log(\lambda(X_1, \ldots, X_p)) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
\tag{14.14}\]</span></span></p>
<p>or equivalently</p>
<p><span id="eq-lambda"><span class="math display">\[
\lambda(X_1, \ldots, X_p) = \mathrm{e}^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}
\tag{14.15}\]</span></span></p>
<p>Here, <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span> are parameters to be estimated. Together, <a href="#eq-poisson" class="quarto-xref">Equation&nbsp;<span>14.13</span></a> and <a href="#eq-log-lambda" class="quarto-xref">Equation&nbsp;<span>14.14</span></a> define the Poisson regression model. Notice that in <a href="#eq-log-lambda" class="quarto-xref">Equation&nbsp;<span>14.14</span></a>, we take the <em>log</em> of <span class="math inline">\(\lambda(X_1, \ldots, X_p)\)</span> to be linear in <span class="math inline">\(X_1, \ldots, X_p\)</span>, in order to ensure that <span class="math inline">\(\lambda(X_1, \ldots, X_p)\)</span> takes on nonnegative values for all values of the covariates.</p>
<p>To estimate the coefficients <span class="math inline">\(\beta_0, \beta_1, \ldots, \beta_p\)</span>, we use the use the same maximum likelihood approach that we adopted for logistic regression. Specifically, given <span class="math inline">\(n\)</span> independent observations from the Poisson regression model, the likelihood takes the form</p>
<p><span class="math display">\[
\ell(\beta_0, \beta_1, \ldots, \beta_p) = \prod_{i = 1}^n \frac{e^{-\lambda(x_i)} \lambda(x_i)^{y_i}}{y_i!}
\]</span></p>
<p>where <span class="math inline">\(\lambda(x_i) = \mathrm{e}^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}\)</span> due to <a href="#eq-lambda" class="quarto-xref">Equation&nbsp;<span>14.15</span></a>. We estimate the coefficients that maximize the likelihood <span class="math inline">\(\ell(\beta_0, \beta_1, \ldots, \beta_p)\)</span>, i.e.&nbsp;that make the observed data as likely as possible.</p>
<p>Some important distinctions between the Poisson regression model and the linear regression model are as follows:</p>
<ul>
<li><p><em>Interpretation</em>: To interpret the coefficients in the Poisson regression model, we must pay close attention to <a href="#eq-lambda" class="quarto-xref">Equation&nbsp;<span>14.15</span></a>, which states that an increase in <span class="math inline">\(X_j\)</span> by one unit is associated with a change in <span class="math inline">\(E(Y) = \lambda\)</span> by a factor of <span class="math inline">\(\mathrm{e}^{\beta_j}\)</span>. For example, a change in weather from clear to cloudy skies is associated with a change in mean bike usage by a factor of <span class="math inline">\(\exp(−0.08) = 0.923\)</span>, i.e.&nbsp;on average, only 92.3% as many people will use bikes when it is cloudy relative to when it is clear.</p></li>
<li><p><em>Mean-variance relationship</em>: As mentioned earlier, under the Poisson model, <span class="math inline">\(\lambda = E(Y) = V(Y)\)</span>. Thus, by modeling bike usage with a Poisson regression, we implicitly assume that mean bike usage in a given hour equals the variance of bike usage during that hour. By contrast, under a linear regression model, the variance of bike usage always takes on a constant value.</p></li>
<li><p><em>nonnegative fitted values</em>: There are no negative predictions using the Poisson regression model.</p></li>
</ul></section><section id="generalized-linear-models-in-greater-generality" class="level4"><h4 class="anchored" data-anchor-id="generalized-linear-models-in-greater-generality">Generalized linear models in greater generality</h4>
<p>We have now discussed three types of regression models: linear, logistic and Poisson. These approaches share some common characteristics:</p>
<ul>
<li><p>Each approach uses predictors <span class="math inline">\(X_1, \ldots, X_p\)</span> to predict a response <span class="math inline">\(Y\)</span>. We assume that, conditional on <span class="math inline">\(X_1, \ldots, X_p\)</span>, <span class="math inline">\(Y\)</span> belongs to a certain family of distributions. For linear regression, we typically assume that <span class="math inline">\(Y\)</span> follows a Gaussian or normal distribution. For logistic regression, we assume that <span class="math inline">\(Y\)</span> follows a Bernoulli distribution. Finally, for Poisson regression, we assume that <span class="math inline">\(Y\)</span> follows a Poisson distribution.</p></li>
<li><p>Each approach models the mean of <span class="math inline">\(Y\)</span> as a function of the predictors. In linear regression, the mean of <span class="math inline">\(Y\)</span> takes the form</p></li>
</ul>
<p><span class="math display">\[
E(Y \mid X_1, \ldots, X_p) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
\]</span></p>
<p>i.e., it is a linear function of the predictors. For logistic regression, the mean instead takes the form</p>
<p><span class="math display">\[
\begin{align*}
E(Y \mid X_1, \ldots, X_p) &amp;= P(Y = 1 \mid X_1, \ldots, X_p)\\
&amp;= \frac{\mathrm{e}^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}{1 + \mathrm{e}^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}
\end{align*}
\]</span></p>
<p>(this is because mean(Bernoulli) = <span class="math inline">\(p\)</span> = … &lt; logistic function &gt;) while for Poisson regression it takes the form</p>
<p><span class="math display">\[
E(Y \mid X_1, \ldots, X_p) = \lambda(X_1, \ldots, X_p) = \mathrm{e}^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}
\]</span></p>
<p>All of these above equations can be expressed using a <em>link function</em>, <span class="math inline">\(\eta\)</span>, which applies a transformation to $E(Y X_1, , X_p) $ so that the transformed mean is a linear function of the predictors. That is,</p>
<p><span id="eq-link-function"><span class="math display">\[
\eta(E(Y \mid X_1, \ldots, X_p)) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p
\tag{14.16}\]</span></span></p>
<p>The link functions for linear, logistic and Poisson regression are <span class="math inline">\(\eta(\mu) = \mu\)</span>, <span class="math inline">\(\eta(\mu) = \log(\mu/(1 − \mu))\)</span>, and <span class="math inline">\(\eta(\mu) = \log(\mu)\)</span>, respectively.</p>
<p>The Gaussian, Bernoulli and Poisson distributions are all members of a wider class of distributions, known as the <em>exponential family</em>. Other well-known members of this family are the exponential distribution, the Gamma distribution, and the negative binomial distribution. In general, we can perform a regression by modeling the response Y as coming from a particular member of the exponential family, and then transforming the mean of the response so that the transformed mean is a linear function of the predictors via <a href="#eq-link-function" class="quarto-xref">Equation&nbsp;<span>14.16</span></a>. Any regression approach that follows this very general recipe is known as a generalized linear model (GLM). Thus, linear regression, logistic regression, and Poisson regression are three examples of GLMs. Other examples not covered here include Gamma regression and negative binomial regression.</p>
</section></section></section><section id="lab" class="level2" data-number="14.2"><h2 data-number="14.2" class="anchored" data-anchor-id="lab">
<span class="header-section-number">14.2</span> Lab</h2>
<section id="load-data" class="level3" data-number="14.2.1"><h3 data-number="14.2.1" class="anchored" data-anchor-id="load-data">
<span class="header-section-number">14.2.1</span> Load data</h3>
<div class="cell">
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load data</span></span>
<span><span class="va">data_stock</span> <span class="op">&lt;-</span> <span class="fu">ISLR2</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ISLR2/man/Smarket.html">Smarket</a></span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html">colnames</a></span><span class="op">(</span><span class="va">data_stock</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Year"      "Lag1"      "Lag2"      "Lag3"      "Lag4"      "Lag5"     
[7] "Volume"    "Today"     "Direction"</code></pre>
</div>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># view correlations</span></span>
<span><span class="va">data_stock</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="fu">where</span><span class="op">(</span><span class="va">is.numeric</span><span class="op">)</span><span class="op">)</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">as.matrix</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="va">cor</span> <span class="op">%&gt;%</span> </span>
<span>  <span class="fu">corrplot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/corrplot/man/corrplot.html">corrplot</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
</div>
</section><section id="logisitic-regression" class="level3" data-number="14.2.2"><h3 data-number="14.2.2" class="anchored" data-anchor-id="logisitic-regression">
<span class="header-section-number">14.2.2</span> Logisitic regression</h3>
<p>Lets fit the full model and view some model summaries.</p>
<div class="cell">
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load packages</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://magrittr.tidyverse.org">magrittr</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># fit logistic regression model</span></span>
<span><span class="va">mod_logreg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span> <span class="op">+</span> <span class="va">Lag3</span> <span class="op">+</span> <span class="va">Lag4</span> <span class="op">+</span> <span class="va">Lag5</span> <span class="op">+</span> <span class="va">Volume</span>,</span>
<span>                  data <span class="op">=</span> <span class="va">data_stock</span>,</span>
<span>                  family <span class="op">=</span> <span class="st">"binomial"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># view model summary</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mod_logreg</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 
    Volume, family = "binomial", data = data_stock)

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -0.126000   0.240736  -0.523    0.601
Lag1        -0.073074   0.050167  -1.457    0.145
Lag2        -0.042301   0.050086  -0.845    0.398
Lag3         0.011085   0.049939   0.222    0.824
Lag4         0.009359   0.049974   0.187    0.851
Lag5         0.010313   0.049511   0.208    0.835
Volume       0.135441   0.158360   0.855    0.392

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1731.2  on 1249  degrees of freedom
Residual deviance: 1727.6  on 1243  degrees of freedom
AIC: 1741.6

Number of Fisher Scoring iterations: 3</code></pre>
</div>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># view coding of response</span></span>
<span><span class="co"># -&gt; thus model is predicting P(Direction = Up | X = x)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/contrasts.html">contrasts</a></span><span class="op">(</span><span class="va">data_stock</span><span class="op">$</span><span class="va">Direction</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     Up
Down  0
Up    1</code></pre>
</div>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># get model fits for probabilities</span></span>
<span><span class="co"># -&gt; base R</span></span>
<span><span class="co"># -&gt; to get the fitted log odds, use type = "link"</span></span>
<span><span class="va">preds_logreg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod_logreg</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span>
<span><span class="co"># -&gt; broom</span></span>
<span><span class="co"># --&gt; uses the same arguments as type for .fitted</span></span>
<span><span class="co"># --&gt; then classify</span></span>
<span><span class="va">preds_logreg</span> <span class="op">&lt;-</span> <span class="fu">broom</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/augment.html">augment</a></span><span class="op">(</span><span class="va">mod_logreg</span>, type.predict <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">Direction</span>, <span class="va">.fitted</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>predicted <span class="op">=</span> <span class="fu">if_else</span><span class="op">(</span><span class="va">.fitted</span> <span class="op">&gt;</span> <span class="fl">0.5</span>, <span class="st">"Up"</span>, <span class="st">"Down"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span>levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Down"</span>, <span class="st">"Up"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># create confusion matrix</span></span>
<span><span class="co"># -&gt; base R</span></span>
<span><span class="va">preds_logreg</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/exposition.html">%$%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">predicted</span>, </span>
<span>                     <span class="va">Direction</span>,</span>
<span>                     dnn <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"predicted"</span>, <span class="st">"actual"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         actual
predicted Down  Up
     Down  145 141
     Up    457 507</code></pre>
</div>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># -&gt; tidymodels</span></span>
<span><span class="op">(</span><span class="va">c_mat</span> <span class="op">&lt;-</span> <span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/conf_mat.html">conf_mat</a></span><span class="op">(</span><span class="va">preds_logreg</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>                      <span class="fu">mutate</span><span class="op">(</span>predicted <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">as.factor</a></span><span class="op">(</span><span class="va">predicted</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                    truth <span class="op">=</span> <span class="st">"Direction"</span>,</span>
<span>                    estimate <span class="op">=</span> <span class="st">"predicted"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          Truth
Prediction Down  Up
      Down  145 141
      Up    457 507</code></pre>
</div>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">c_mat</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 13 × 3
   .metric              .estimator .estimate
   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;
 1 accuracy             binary        0.522 
 2 kap                  binary        0.0237
 3 sens                 binary        0.241 
 4 spec                 binary        0.782 
 5 ppv                  binary        0.507 
 6 npv                  binary        0.526 
 7 mcc                  binary        0.0277
 8 j_index              binary        0.0233
 9 bal_accuracy         binary        0.512 
10 detection_prevalence binary        0.229 
11 precision            binary        0.507 
12 recall               binary        0.241 
13 f_meas               binary        0.327 </code></pre>
</div>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate accuracy and error rate = 1 - accuracy</span></span>
<span><span class="co"># -&gt; base R</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">preds_logreg</span><span class="op">$</span><span class="va">predicted</span> <span class="op">==</span> <span class="va">data_stock</span><span class="op">$</span><span class="va">Direction</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5216</code></pre>
</div>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fl">1</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">preds_logreg</span><span class="op">$</span><span class="va">predicted</span> <span class="op">==</span> <span class="va">data_stock</span><span class="op">$</span><span class="va">Direction</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4784</code></pre>
</div>
</div>
<p>Now repeat analysis, but use a holdout sample.</p>
<div class="cell">
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># sample data </span></span>
<span><span class="va">data_train</span> <span class="op">&lt;-</span> <span class="va">data_stock</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">Year</span> <span class="op">&lt;</span> <span class="fl">2005</span><span class="op">)</span></span>
<span><span class="va">data_test</span> <span class="op">&lt;-</span> <span class="va">data_stock</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">Year</span> <span class="op">&gt;=</span> <span class="fl">2005</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># fit logistic regression model on training data</span></span>
<span><span class="va">mod_logreg2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span> <span class="op">+</span> <span class="va">Lag3</span> <span class="op">+</span> <span class="va">Lag4</span> <span class="op">+</span> <span class="va">Lag5</span> <span class="op">+</span> <span class="va">Volume</span>,</span>
<span>                   data <span class="op">=</span> <span class="va">data_train</span>,</span>
<span>                   family <span class="op">=</span> <span class="st">"binomial"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># predict on holdout data</span></span>
<span><span class="va">preds_logreg2</span> <span class="op">&lt;-</span> <span class="fu">broom</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/augment.html">augment</a></span><span class="op">(</span><span class="va">mod_logreg2</span>,</span>
<span>                              newdata <span class="op">=</span> <span class="va">data_test</span>,</span>
<span>                              type.predict <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">Direction</span>, <span class="va">.fitted</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>predicted <span class="op">=</span> <span class="fu">if_else</span><span class="op">(</span><span class="va">.fitted</span> <span class="op">&gt;</span> <span class="fl">0.5</span>, <span class="st">"Up"</span>, <span class="st">"Down"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span>levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Down"</span>, <span class="st">"Up"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># view results</span></span>
<span><span class="va">preds_logreg2</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/conf_mat.html">conf_mat</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="st">"Direction"</span>,</span>
<span>                      estimate <span class="op">=</span> <span class="st">"predicted"</span><span class="op">)</span> <span class="co">#%&gt;% </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          Truth
Prediction Down Up
      Down   77 97
      Up     34 44</code></pre>
</div>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span>  <span class="co">#summary(event_level = "second")</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can do some model selection to get a better model. <strong>After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement.</strong></p>
<div class="cell">
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># fit smaller model</span></span>
<span><span class="va">mod_logreg3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span>,</span>
<span>                   data <span class="op">=</span> <span class="va">data_train</span>,</span>
<span>                   family <span class="op">=</span> <span class="st">"binomial"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># view model summary</span></span>
<span><span class="va">mod_logreg3</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">broom</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/glance.html">glance</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 1 × 8
  null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs
          &lt;dbl&gt;   &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;       &lt;int&gt; &lt;int&gt;
1         1383.     997  -691. 1387. 1402.    1381.         995   998</code></pre>
</div>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># predict on holdout data</span></span>
<span><span class="va">preds_logreg3</span> <span class="op">&lt;-</span> <span class="fu">broom</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/augment.html">augment</a></span><span class="op">(</span><span class="va">mod_logreg3</span>,</span>
<span>                              newdata <span class="op">=</span> <span class="va">data_test</span>,</span>
<span>                              type.predict <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">Direction</span>, <span class="va">.fitted</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>predicted <span class="op">=</span> <span class="fu">if_else</span><span class="op">(</span><span class="va">.fitted</span> <span class="op">&gt;</span> <span class="fl">0.5</span>, <span class="st">"Up"</span>, <span class="st">"Down"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span>levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Down"</span>, <span class="st">"Up"</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># view results</span></span>
<span><span class="va">preds_logreg3</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/conf_mat.html">conf_mat</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="st">"Direction"</span>,</span>
<span>                      estimate <span class="op">=</span> <span class="st">"predicted"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span>event_level <span class="op">=</span> <span class="st">"second"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 13 × 3
   .metric              .estimator .estimate
   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;
 1 accuracy             binary        0.560 
 2 kap                  binary        0.0698
 3 sens                 binary        0.752 
 4 spec                 binary        0.315 
 5 ppv                  binary        0.582 
 6 npv                  binary        0.5   
 7 mcc                  binary        0.0744
 8 j_index              binary        0.0671
 9 bal_accuracy         binary        0.534 
10 detection_prevalence binary        0.722 
11 precision            binary        0.582 
12 recall               binary        0.752 
13 f_meas               binary        0.656 </code></pre>
</div>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate accuracy of naive approach, which is just predicting yes everyday</span></span>
<span><span class="co"># -&gt; get also numbers from truth confusion matrix</span></span>
<span><span class="va">data_test</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">summarize</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">==</span> <span class="st">"Up"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  mean(Direction == "Up")
1               0.5595238</code></pre>
</div>
</div>
</section><section id="linear-discriminant-analysis" class="level3" data-number="14.2.3"><h3 data-number="14.2.3" class="anchored" data-anchor-id="linear-discriminant-analysis">
<span class="header-section-number">14.2.3</span> Linear discriminant analysis</h3>
<div class="cell">
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># fit LDA model</span></span>
<span><span class="va">mod_lda</span> <span class="op">&lt;-</span> <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span>, data <span class="op">=</span> <span class="va">data_train</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># view model</span></span>
<span><span class="va">mod_lda</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
lda(Direction ~ Lag1 + Lag2, data = data_train)

Prior probabilities of groups:
    Down       Up 
0.491984 0.508016 

Group means:
            Lag1        Lag2
Down  0.04279022  0.03389409
Up   -0.03954635 -0.03132544

Coefficients of linear discriminants:
            LD1
Lag1 -0.6420190
Lag2 -0.5135293</code></pre>
</div>
<div class="sourceCode" id="cb29"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># plot model</span></span>
<span><span class="co"># -&gt; produces plots of the linear discriminants, obtained by computing −0.642 × Lag1 − 0.514 × Lag2 for each of the training observations.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">mod_lda</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The <em>coefficients of linear discriminants</em> output provides the linear combination of Lag1 and Lag2 that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of X = x in <a href="#eq-bayes-decision-boundary3" class="quarto-xref">Equation&nbsp;<span>14.11</span></a>.</p>
<!-- ??? could do more research into exactly what these are -->
<p>If −0.642 × Lag1 − 0.514 × Lag2 is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.</p>
<div class="cell">
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># predict</span></span>
<span><span class="va">preds_lda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod_lda</span>, newdata <span class="op">=</span> <span class="va">data_test</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># view prediction output</span></span>
<span><span class="co"># -&gt; class prediction</span></span>
<span><span class="va">preds_lda</span><span class="op">$</span><span class="va">class</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">head</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] Up Up Up Up Up Up
Levels: Down Up</code></pre>
</div>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># -&gt; posterior probabilities</span></span>
<span><span class="va">preds_lda</span><span class="op">$</span><span class="va">posterior</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">head</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       Down        Up
1 0.4901792 0.5098208
2 0.4792185 0.5207815
3 0.4668185 0.5331815
4 0.4740011 0.5259989
5 0.4927877 0.5072123
6 0.4938562 0.5061438</code></pre>
</div>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># -&gt; linear discriminants</span></span>
<span><span class="va">preds_lda</span><span class="op">$</span><span class="va">x</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">head</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          LD1
1  0.08293096
2  0.59114102
3  1.16723063
4  0.83335022
5 -0.03792892
6 -0.08743142</code></pre>
</div>
<div class="sourceCode" id="cb36"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># analyze predictions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">preds_lda</span><span class="op">$</span><span class="va">class</span>,</span>
<span>      <span class="va">data_test</span><span class="op">$</span><span class="va">Direction</span>,</span>
<span>      dnn <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"predicted"</span>, <span class="st">"Direction"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         Direction
predicted Down  Up
     Down   35  35
     Up     76 106</code></pre>
</div>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate accuracy</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">preds_lda</span><span class="op">$</span><span class="va">class</span> <span class="op">==</span> <span class="va">data_test</span><span class="op">$</span><span class="va">Direction</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5595238</code></pre>
</div>
</div>
</section><section id="quadratic-discriminant-analysis-1" class="level3" data-number="14.2.4"><h3 data-number="14.2.4" class="anchored" data-anchor-id="quadratic-discriminant-analysis-1">
<span class="header-section-number">14.2.4</span> Quadratic discriminant analysis</h3>
<div class="cell">
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># fit QDA model</span></span>
<span><span class="va">mod_qda</span> <span class="op">&lt;-</span> <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/qda.html">qda</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span>, data <span class="op">=</span> <span class="va">data_train</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># view model</span></span>
<span><span class="co"># -&gt; now output does not contain the coefficients of the linear discriminants, because no longer linear</span></span>
<span><span class="va">mod_qda</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Call:
qda(Direction ~ Lag1 + Lag2, data = data_train)

Prior probabilities of groups:
    Down       Up 
0.491984 0.508016 

Group means:
            Lag1        Lag2
Down  0.04279022  0.03389409
Up   -0.03954635 -0.03132544</code></pre>
</div>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># predict</span></span>
<span><span class="va">preds_qda</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod_qda</span>, newdata <span class="op">=</span> <span class="va">data_test</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># analyze predictions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">preds_qda</span><span class="op">$</span><span class="va">class</span>,</span>
<span>      <span class="va">data_test</span><span class="op">$</span><span class="va">Direction</span>,</span>
<span>      dnn <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"predicted"</span>, <span class="st">"Direction"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         Direction
predicted Down  Up
     Down   30  20
     Up     81 121</code></pre>
</div>
<div class="sourceCode" id="cb44"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate accuracy</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">preds_qda</span><span class="op">$</span><span class="va">class</span> <span class="op">==</span> <span class="va">data_test</span><span class="op">$</span><span class="va">Direction</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5992063</code></pre>
</div>
</div>
<p>A higher accuracy for the test data suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression.</p>
</section><section id="naive-bayes-1" class="level3" data-number="14.2.5"><h3 data-number="14.2.5" class="anchored" data-anchor-id="naive-bayes-1">
<span class="header-section-number">14.2.5</span> Naive Bayes</h3>
<div class="cell">
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># fit model</span></span>
<span><span class="co"># -&gt; by default, this models each quantitative feature with a Gaussian distribution</span></span>
<span><span class="co"># --&gt; but a kernal density method can also be used to estimate the distributions</span></span>
<span><span class="va">mod_nb</span> <span class="op">&lt;-</span> <span class="fu">e1071</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/naiveBayes.html">naiveBayes</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span>, data <span class="op">=</span> <span class="va">data_train</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># view model output</span></span>
<span><span class="va">mod_nb</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Naive Bayes Classifier for Discrete Predictors

Call:
naiveBayes.default(x = X, y = Y, laplace = laplace)

A-priori probabilities:
Y
    Down       Up 
0.491984 0.508016 

Conditional probabilities:
      Lag1
Y             [,1]     [,2]
  Down  0.04279022 1.227446
  Up   -0.03954635 1.231668

      Lag2
Y             [,1]     [,2]
  Down  0.03389409 1.239191
  Up   -0.03132544 1.220765</code></pre>
</div>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># predict</span></span>
<span><span class="va">preds_nb</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod_nb</span>, newdata <span class="op">=</span> <span class="va">data_test</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># analyze predictions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">preds_nb</span>,</span>
<span>      <span class="va">data_test</span><span class="op">$</span><span class="va">Direction</span>,</span>
<span>      dnn <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"predicted"</span>, <span class="st">"Direction"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         Direction
predicted Down  Up
     Down   28  20
     Up     83 121</code></pre>
</div>
<div class="sourceCode" id="cb50"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate accuracy</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">preds_nb</span> <span class="op">==</span> <span class="va">data_test</span><span class="op">$</span><span class="va">Direction</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5912698</code></pre>
</div>
</div>
<p>Naive Bayes performs very well on this data, with accurate predictions over 59% of the time. This is slightly worse than QDA, but much better than LDA.</p>
</section><section id="knn" class="level3" data-number="14.2.6"><h3 data-number="14.2.6" class="anchored" data-anchor-id="knn">
<span class="header-section-number">14.2.6</span> KNN</h3>
<div class="cell">
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># fit model</span></span>
<span><span class="co"># -&gt; this function forms prediction in a single command (not in two steps like the other models: fit then predict)</span></span>
<span><span class="co"># -&gt; NOTE: if there is a tie (say k = 2 and one observation from each class), then R randomly breaks the tie</span></span>
<span><span class="co"># --&gt; so need to set seed if want reproducibility</span></span>
<span><span class="va">preds_knn1</span> <span class="op">&lt;-</span> <span class="fu">class</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span><span class="op">(</span>train <span class="op">=</span> <span class="va">data_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">select</span><span class="op">(</span><span class="va">Lag1</span>, <span class="va">Lag2</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">as.matrix</span>, <span class="co"># predictors of train set as matrix</span></span>
<span>                      test <span class="op">=</span> <span class="va">data_test</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">select</span><span class="op">(</span><span class="va">Lag1</span>, <span class="va">Lag2</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">as.matrix</span>, <span class="co"># predictors of test set as matrix</span></span>
<span>                      cl <span class="op">=</span> <span class="va">data_train</span><span class="op">$</span><span class="va">Direction</span>, <span class="co"># truth for train set</span></span>
<span>                      k <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># analyze predictions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">preds_knn1</span>,</span>
<span>      <span class="va">data_test</span><span class="op">$</span><span class="va">Direction</span>,</span>
<span>      dnn <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"predicted"</span>, <span class="st">"Direction"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         Direction
predicted Down Up
     Down   43 58
     Up     68 83</code></pre>
</div>
<div class="sourceCode" id="cb54"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate accuracy</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">preds_knn1</span> <span class="op">==</span> <span class="va">data_test</span><span class="op">$</span><span class="va">Direction</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5</code></pre>
</div>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># overfitting...</span></span>
<span></span>
<span><span class="co"># refit with better k</span></span>
<span><span class="va">preds_knn3</span> <span class="op">&lt;-</span> <span class="fu">class</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span><span class="op">(</span>train <span class="op">=</span> <span class="va">data_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">select</span><span class="op">(</span><span class="va">Lag1</span>, <span class="va">Lag2</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">as.matrix</span>, <span class="co"># predictors of train set as matrix</span></span>
<span>                      test <span class="op">=</span> <span class="va">data_test</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">select</span><span class="op">(</span><span class="va">Lag1</span>, <span class="va">Lag2</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">as.matrix</span>, <span class="co"># predictors of test set as matrix</span></span>
<span>                      cl <span class="op">=</span> <span class="va">data_train</span><span class="op">$</span><span class="va">Direction</span>, <span class="co"># truth for train set</span></span>
<span>                      k <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># analyze predictions</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">preds_knn3</span>,</span>
<span>      <span class="va">data_test</span><span class="op">$</span><span class="va">Direction</span>,</span>
<span>      dnn <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"predicted"</span>, <span class="st">"Direction"</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>         Direction
predicted Down Up
     Down   48 56
     Up     63 85</code></pre>
</div>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate accuracy</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="va">preds_knn3</span> <span class="op">==</span> <span class="va">data_test</span><span class="op">$</span><span class="va">Direction</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.5277778</code></pre>
</div>
</div>
<p>The results have improved slightly. But increasing <span class="math inline">\(K\)</span> further turns out to provide no further improvements.</p>
<p>It appears that for this data, QDA provides the best results of the methods that we have examined so far.</p>
<p>Notes about KNN in general:</p>
<ul>
<li><p>Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the <em>distance</em> between the observations, and hence on the KNN classifier, than variables that are on a small scale.</p></li>
<li><p>For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of $1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. Same for scale of the response, changing units from dollars to cents will lead to different classification results.</p></li>
<li><p><strong>A good way to handle this problem is to <em>standardize</em> the data so that all variables are given a mean of zero and a standard deviation of one. Can use <code><a href="https://rdrr.io/r/base/scale.html">scale()</a></code> to accomplish this.</strong></p></li>
</ul>
<p>When using classifying methods in general, can look at the naive approach of guessing all positives as a baseline.</p>
</section><section id="poisson-regression-1" class="level3" data-number="14.2.7"><h3 data-number="14.2.7" class="anchored" data-anchor-id="poisson-regression-1">
<span class="header-section-number">14.2.7</span> Poisson regression</h3>
<p>First, try to fit a linear regression model.</p>
<div class="cell">
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load data</span></span>
<span><span class="va">data_bike</span> <span class="op">&lt;-</span> <span class="fu">ISLR2</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ISLR2/man/Bikeshare.html">Bikeshare</a></span></span>
<span></span>
<span><span class="co"># fit linear regression model</span></span>
<span><span class="va">mod_lr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">bikers</span> <span class="op">~</span> <span class="va">mnth</span> <span class="op">+</span> <span class="va">hr</span> <span class="op">+</span> <span class="va">workingday</span> <span class="op">+</span> <span class="va">temp</span> <span class="op">+</span> <span class="va">weathersit</span>, data <span class="op">=</span> <span class="va">data_bike</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># view model summary</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mod_lr</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
    data = data_bike)

Residuals:
    Min      1Q  Median      3Q     Max 
-299.00  -45.70   -6.23   41.08  425.29 

Coefficients:
                          Estimate Std. Error t value             Pr(&gt;|t|)    
(Intercept)                -68.632      5.307 -12.932 &lt; 0.0000000000000002 ***
mnthFeb                      6.845      4.287   1.597             0.110398    
mnthMarch                   16.551      4.301   3.848             0.000120 ***
mnthApril                   41.425      4.972   8.331 &lt; 0.0000000000000002 ***
mnthMay                     72.557      5.641  12.862 &lt; 0.0000000000000002 ***
mnthJune                    67.819      6.544  10.364 &lt; 0.0000000000000002 ***
mnthJuly                    45.324      7.081   6.401  0.00000000016282293 ***
mnthAug                     53.243      6.640   8.019  0.00000000000000121 ***
mnthSept                    66.678      5.925  11.254 &lt; 0.0000000000000002 ***
mnthOct                     75.834      4.950  15.319 &lt; 0.0000000000000002 ***
mnthNov                     60.310      4.610  13.083 &lt; 0.0000000000000002 ***
mnthDec                     46.458      4.271  10.878 &lt; 0.0000000000000002 ***
hr1                        -14.579      5.699  -2.558             0.010536 *  
hr2                        -21.579      5.733  -3.764             0.000168 ***
hr3                        -31.141      5.778  -5.389  0.00000007260801066 ***
hr4                        -36.908      5.802  -6.361  0.00000000021092958 ***
hr5                        -24.135      5.737  -4.207  0.00002611246755715 ***
hr6                         20.600      5.704   3.612             0.000306 ***
hr7                        120.093      5.693  21.095 &lt; 0.0000000000000002 ***
hr8                        223.662      5.690  39.310 &lt; 0.0000000000000002 ***
hr9                        120.582      5.693  21.182 &lt; 0.0000000000000002 ***
hr10                        83.801      5.705  14.689 &lt; 0.0000000000000002 ***
hr11                       105.423      5.722  18.424 &lt; 0.0000000000000002 ***
hr12                       137.284      5.740  23.916 &lt; 0.0000000000000002 ***
hr13                       136.036      5.760  23.617 &lt; 0.0000000000000002 ***
hr14                       126.636      5.776  21.923 &lt; 0.0000000000000002 ***
hr15                       132.087      5.780  22.852 &lt; 0.0000000000000002 ***
hr16                       178.521      5.772  30.927 &lt; 0.0000000000000002 ***
hr17                       296.267      5.749  51.537 &lt; 0.0000000000000002 ***
hr18                       269.441      5.736  46.976 &lt; 0.0000000000000002 ***
hr19                       186.256      5.714  32.596 &lt; 0.0000000000000002 ***
hr20                       125.549      5.704  22.012 &lt; 0.0000000000000002 ***
hr21                        87.554      5.693  15.378 &lt; 0.0000000000000002 ***
hr22                        59.123      5.689  10.392 &lt; 0.0000000000000002 ***
hr23                        26.838      5.688   4.719  0.00000241267941359 ***
workingday                   1.270      1.784   0.711             0.476810    
temp                       157.209     10.261  15.321 &lt; 0.0000000000000002 ***
weathersitcloudy/misty     -12.890      1.964  -6.562  0.00000000005600358 ***
weathersitlight rain/snow  -66.494      2.965 -22.425 &lt; 0.0000000000000002 ***
weathersitheavy rain/snow -109.745     76.667  -1.431             0.152341    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 76.5 on 8605 degrees of freedom
Multiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 
F-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 0.00000000000000022</code></pre>
</div>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">mod_lr</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)     mnthFeb   mnthMarch   mnthApril     mnthMay    mnthJune 
 -68.631704    6.845203   16.551438   41.424907   72.557084   67.818749 </code></pre>
</div>
<div class="sourceCode" id="cb64"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># view predictions #check negative ones</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-error">
<pre><code>Error in UseMethod("predict"): no applicable method for 'predict' applied to an object of class "function"</code></pre>
</div>
</div>
<p>Now we can change contrasts so can get a coefficient estimate for every level of the predictors.</p>
<ul>
<li><p><a href="https://learnb4ss.github.io/learnB4SS/articles/contrasts.html">Contrasts</a> are an attribute of the column.</p></li>
<li><p>Default coding is dummy variable (aka treatment contrasts), where coefficients are set relative to the reference level.</p></li>
<li><p>For sum contrasts, all contrasts sum to 0 for each dummy variable and the reference level is in fact the grand mean. Also, now coefficients are the difference relative to the grand mean (which is now the intercept).</p></li>
<li><p>Lastly for sum contrasts, the last coefficient isn’t given in the output, but it can be easily calculated: <em>it is the negative sum of the coefficient estimates for all of the other levels</em>.</p></li>
</ul>
<div class="cell">
<div class="sourceCode" id="cb66"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># set new contrasts</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/contrasts.html">contrasts</a></span><span class="op">(</span><span class="va">data_bike</span><span class="op">$</span><span class="va">hr</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="st">"contr.sum"</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/contrasts.html">contrasts</a></span><span class="op">(</span><span class="va">data_bike</span><span class="op">$</span><span class="va">mnth</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="st">"contr.sum"</span></span>
<span></span>
<span><span class="co"># refit model</span></span>
<span><span class="va">mod_lr2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html">lm</a></span><span class="op">(</span><span class="va">bikers</span> <span class="op">~</span> <span class="va">mnth</span> <span class="op">+</span> <span class="va">hr</span> <span class="op">+</span> <span class="va">workingday</span> <span class="op">+</span> <span class="va">temp</span> <span class="op">+</span> <span class="va">weathersit</span>, data <span class="op">=</span> <span class="va">data_bike</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># view model summary</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mod_lr2</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
    data = data_bike)

Residuals:
    Min      1Q  Median      3Q     Max 
-299.00  -45.70   -6.23   41.08  425.29 

Coefficients:
                           Estimate Std. Error t value             Pr(&gt;|t|)    
(Intercept)                 73.5974     5.1322  14.340 &lt; 0.0000000000000002 ***
mnth1                      -46.0871     4.0855 -11.281 &lt; 0.0000000000000002 ***
mnth2                      -39.2419     3.5391 -11.088 &lt; 0.0000000000000002 ***
mnth3                      -29.5357     3.1552  -9.361 &lt; 0.0000000000000002 ***
mnth4                       -4.6622     2.7406  -1.701              0.08895 .  
mnth5                       26.4700     2.8508   9.285 &lt; 0.0000000000000002 ***
mnth6                       21.7317     3.4651   6.272   0.0000000003747098 ***
mnth7                       -0.7626     3.9084  -0.195              0.84530    
mnth8                        7.1560     3.5347   2.024              0.04295 *  
mnth9                       20.5912     3.0456   6.761   0.0000000000146005 ***
mnth10                      29.7472     2.6995  11.019 &lt; 0.0000000000000002 ***
mnth11                      14.2229     2.8604   4.972   0.0000006740476467 ***
hr1                        -96.1420     3.9554 -24.307 &lt; 0.0000000000000002 ***
hr2                       -110.7213     3.9662 -27.916 &lt; 0.0000000000000002 ***
hr3                       -117.7212     4.0165 -29.310 &lt; 0.0000000000000002 ***
hr4                       -127.2828     4.0808 -31.191 &lt; 0.0000000000000002 ***
hr5                       -133.0495     4.1168 -32.319 &lt; 0.0000000000000002 ***
hr6                       -120.2775     4.0370 -29.794 &lt; 0.0000000000000002 ***
hr7                        -75.5424     3.9916 -18.925 &lt; 0.0000000000000002 ***
hr8                         23.9511     3.9686   6.035   0.0000000016537185 ***
hr9                        127.5199     3.9500  32.284 &lt; 0.0000000000000002 ***
hr10                        24.4399     3.9360   6.209   0.0000000005566528 ***
hr11                       -12.3407     3.9361  -3.135              0.00172 ** 
hr12                         9.2814     3.9447   2.353              0.01865 *  
hr13                        41.1417     3.9571  10.397 &lt; 0.0000000000000002 ***
hr14                        39.8939     3.9750  10.036 &lt; 0.0000000000000002 ***
hr15                        30.4940     3.9910   7.641   0.0000000000000239 ***
hr16                        35.9445     3.9949   8.998 &lt; 0.0000000000000002 ***
hr17                        82.3786     3.9883  20.655 &lt; 0.0000000000000002 ***
hr18                       200.1249     3.9638  50.488 &lt; 0.0000000000000002 ***
hr19                       173.2989     3.9561  43.806 &lt; 0.0000000000000002 ***
hr20                        90.1138     3.9400  22.872 &lt; 0.0000000000000002 ***
hr21                        29.4071     3.9362   7.471   0.0000000000000874 ***
hr22                        -8.5883     3.9332  -2.184              0.02902 *  
hr23                       -37.0194     3.9344  -9.409 &lt; 0.0000000000000002 ***
workingday                   1.2696     1.7845   0.711              0.47681    
temp                       157.2094    10.2612  15.321 &lt; 0.0000000000000002 ***
weathersitcloudy/misty     -12.8903     1.9643  -6.562   0.0000000000560036 ***
weathersitlight rain/snow  -66.4944     2.9652 -22.425 &lt; 0.0000000000000002 ***
weathersitheavy rain/snow -109.7446    76.6674  -1.431              0.15234    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 76.5 on 8605 degrees of freedom
Multiple R-squared:  0.6745,    Adjusted R-squared:  0.6731 
F-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 0.00000000000000022</code></pre>
</div>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">mod_lr2</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/utils/head.html">head</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/utils/head.html">tail</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span><span class="op">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              (Intercept)                     mnth1                     mnth2 
                73.597428                -46.087090                -39.241888 
                    mnth3                     mnth4                     mnth5 
               -29.535652                 -4.662183                 26.469993 
                     hr23                workingday                      temp 
               -37.019399                  1.269601                157.209366 
   weathersitcloudy/misty weathersitlight rain/snow weathersitheavy rain/snow 
               -12.890266                -66.494365               -109.744577 </code></pre>
</div>
<div class="sourceCode" id="cb70"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># create plot of the coefficients for one of the factor variables</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">mod_lr2</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">12</span><span class="op">]</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">.</span>, <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span><span class="op">}</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"b"</span>, xlab <span class="op">=</span> <span class="st">"Month"</span>, ylab <span class="op">=</span> <span class="st">"Coefficient"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Now we can fit a Poisson regression model.</p>
<div class="cell">
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># fit poisson regression model</span></span>
<span><span class="va">mod_pois</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">bikers</span> <span class="op">~</span> <span class="va">mnth</span> <span class="op">+</span> <span class="va">hr</span> <span class="op">+</span> <span class="va">workingday</span> <span class="op">+</span> <span class="va">temp</span> <span class="op">+</span> <span class="va">weathersit</span>,</span>
<span>                data <span class="op">=</span> <span class="va">data_bike</span>,</span>
<span>                family <span class="op">=</span> <span class="st">"poisson"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># view model summary</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">mod_pois</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
    family = "poisson", data = data_bike)

Coefficients:
                           Estimate Std. Error  z value             Pr(&gt;|z|)
(Intercept)                4.118245   0.006021  683.964 &lt; 0.0000000000000002
mnth1                     -0.670170   0.005907 -113.445 &lt; 0.0000000000000002
mnth2                     -0.444124   0.004860  -91.379 &lt; 0.0000000000000002
mnth3                     -0.293733   0.004144  -70.886 &lt; 0.0000000000000002
mnth4                      0.021523   0.003125    6.888   0.0000000000056631
mnth5                      0.240471   0.002916   82.462 &lt; 0.0000000000000002
mnth6                      0.223235   0.003554   62.818 &lt; 0.0000000000000002
mnth7                      0.103617   0.004125   25.121 &lt; 0.0000000000000002
mnth8                      0.151171   0.003662   41.281 &lt; 0.0000000000000002
mnth9                      0.233493   0.003102   75.281 &lt; 0.0000000000000002
mnth10                     0.267573   0.002785   96.091 &lt; 0.0000000000000002
mnth11                     0.150264   0.003180   47.248 &lt; 0.0000000000000002
hr1                       -0.754386   0.007879  -95.744 &lt; 0.0000000000000002
hr2                       -1.225979   0.009953 -123.173 &lt; 0.0000000000000002
hr3                       -1.563147   0.011869 -131.702 &lt; 0.0000000000000002
hr4                       -2.198304   0.016424 -133.846 &lt; 0.0000000000000002
hr5                       -2.830484   0.022538 -125.586 &lt; 0.0000000000000002
hr6                       -1.814657   0.013464 -134.775 &lt; 0.0000000000000002
hr7                       -0.429888   0.006896  -62.341 &lt; 0.0000000000000002
hr8                        0.575181   0.004406  130.544 &lt; 0.0000000000000002
hr9                        1.076927   0.003563  302.220 &lt; 0.0000000000000002
hr10                       0.581769   0.004286  135.727 &lt; 0.0000000000000002
hr11                       0.336852   0.004720   71.372 &lt; 0.0000000000000002
hr12                       0.494121   0.004392  112.494 &lt; 0.0000000000000002
hr13                       0.679642   0.004069  167.040 &lt; 0.0000000000000002
hr14                       0.673565   0.004089  164.722 &lt; 0.0000000000000002
hr15                       0.624910   0.004178  149.570 &lt; 0.0000000000000002
hr16                       0.653763   0.004132  158.205 &lt; 0.0000000000000002
hr17                       0.874301   0.003784  231.040 &lt; 0.0000000000000002
hr18                       1.294635   0.003254  397.848 &lt; 0.0000000000000002
hr19                       1.212281   0.003321  365.084 &lt; 0.0000000000000002
hr20                       0.914022   0.003700  247.065 &lt; 0.0000000000000002
hr21                       0.616201   0.004191  147.045 &lt; 0.0000000000000002
hr22                       0.364181   0.004659   78.173 &lt; 0.0000000000000002
hr23                       0.117493   0.005225   22.488 &lt; 0.0000000000000002
workingday                 0.014665   0.001955    7.502   0.0000000000000627
temp                       0.785292   0.011475   68.434 &lt; 0.0000000000000002
weathersitcloudy/misty    -0.075231   0.002179  -34.528 &lt; 0.0000000000000002
weathersitlight rain/snow -0.575800   0.004058 -141.905 &lt; 0.0000000000000002
weathersitheavy rain/snow -0.926287   0.166782   -5.554   0.0000000279379459
                             
(Intercept)               ***
mnth1                     ***
mnth2                     ***
mnth3                     ***
mnth4                     ***
mnth5                     ***
mnth6                     ***
mnth7                     ***
mnth8                     ***
mnth9                     ***
mnth10                    ***
mnth11                    ***
hr1                       ***
hr2                       ***
hr3                       ***
hr4                       ***
hr5                       ***
hr6                       ***
hr7                       ***
hr8                       ***
hr9                       ***
hr10                      ***
hr11                      ***
hr12                      ***
hr13                      ***
hr14                      ***
hr15                      ***
hr16                      ***
hr17                      ***
hr18                      ***
hr19                      ***
hr20                      ***
hr21                      ***
hr22                      ***
hr23                      ***
workingday                ***
temp                      ***
weathersitcloudy/misty    ***
weathersitlight rain/snow ***
weathersitheavy rain/snow ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 1052921  on 8644  degrees of freedom
Residual deviance:  228041  on 8605  degrees of freedom
AIC: 281159

Number of Fisher Scoring iterations: 5</code></pre>
</div>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># plot estimated coefficients (still using sum contrasts)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/stats/coef.html">coef</a></span><span class="op">(</span><span class="va">mod_pois</span><span class="op">)</span><span class="op">[</span><span class="fl">2</span><span class="op">:</span><span class="fl">12</span><span class="op">]</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="va">.</span>, <span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">.</span><span class="op">)</span><span class="op">)</span><span class="op">}</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>type <span class="op">=</span> <span class="st">"b"</span>, xlab <span class="op">=</span> <span class="st">"Month"</span>, ylab <span class="op">=</span> <span class="st">"Coefficient"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb74"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># make predictions</span></span>
<span><span class="va">preds_pois</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod_pois</span>, type <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section></section><section id="exercises" class="level2" data-number="14.3"><h2 data-number="14.3" class="anchored" data-anchor-id="exercises">
<span class="header-section-number">14.3</span> Exercises</h2>
<section id="conceptual" class="level3" data-number="14.3.1"><h3 data-number="14.3.1" class="anchored" data-anchor-id="conceptual">
<span class="header-section-number">14.3.1</span> Conceptual</h3>
<section id="question-1" class="level4"><h4 class="anchored" data-anchor-id="question-1">Question 1</h4>
<blockquote class="blockquote">
<p>Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.</p>
</blockquote>
<p>&lt; already showed in notes &gt;</p>
</section><section id="question-2" class="level4"><h4 class="anchored" data-anchor-id="question-2">Question 2</h4>
<blockquote class="blockquote">
<p>It was stated in the text that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest. Prove that this is the case. In other words, under the assumption that the observations in the <span class="math inline">\(k\)</span>th class are drawn from a <span class="math inline">\(N(\mu_k,\sigma^2)\)</span> distribution, the Bayes’ classifier assigns an observation to the class for which the discriminant function is maximized.</p>
</blockquote>
<p><img src="files/images/4-q2.png" class="img-fluid"></p>
</section><section id="question-3" class="level4"><h4 class="anchored" data-anchor-id="question-3">Question 3</h4>
<blockquote class="blockquote">
<p>This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where <span class="math inline">\(p = 1\)</span>; i.e.&nbsp;there is only one feature.</p>
</blockquote>
<blockquote class="blockquote">
<p>Suppose that we have <span class="math inline">\(K\)</span> classes, and that if an observation belongs to the kth class then <span class="math inline">\(X\)</span> comes from a one-dimensional normal distribution, <span class="math inline">\(X \sim N(\mu_k,\sigma^2_k)\)</span>. Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is <em>not</em> linear. Argue that it is in fact quadratic.</p>
</blockquote>
<blockquote class="blockquote">
<p><em>Hint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that <span class="math inline">\(\sigma_1^2 = ... = \sigma_K^2\)</span>.</em></p>
</blockquote>
<p><img src="files/images/4-q3.png" class="img-fluid"></p>
</section><section id="question-4" class="level4"><h4 class="anchored" data-anchor-id="question-4">Question 4</h4>
<blockquote class="blockquote">
<p>When the number of features <span class="math inline">\(p\)</span> is large, there tends to be a deterioration in the performance of KNN and other <em>local</em> approaches that perform prediction using only observations that are <em>near</em> the test observation for which a prediction must be made. This phenomenon is known as the <em>curse of dimensionality</em>, and it ties into the fact that non-parametric approaches often perform poorly when <span class="math inline">\(p\)</span> is large. We will now investigate this curse.</p>
</blockquote>
<blockquote class="blockquote">
<ol type="a">
<li>Suppose that we have a set of observations, each with measurements on <span class="math inline">\(p = 1\)</span> feature, <span class="math inline">\(X\)</span>. We assume that <span class="math inline">\(X\)</span> is uniformly (evenly) distributed on <span class="math inline">\([0, 1]\)</span>. Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within 10% of the range of <span class="math inline">\(X\)</span> closest to that test observation. For instance, in order to predict the response for a test observation with <span class="math inline">\(X = 0.6\)</span>, we will use observations in the range <span class="math inline">\([0.55, 0.65]\)</span>. On average, what fraction of the available observations will we use to make the prediction?</li>
</ol>
</blockquote>
<p>For values in <span class="math inline">\([0,0.05]\)</span>, we use less than 10% of observations (between 5% and 10%, 7.5% on average), similarly with values in <span class="math inline">\([0.95,1]\)</span>. For values in <span class="math inline">\([0.05,0.95]\)</span> we use 10% of available observations. The (weighted) average is then <span class="math inline">\(7.5 \times 0.1 + 10 \times 0.9 = 9.75\%\)</span>.</p>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>Now suppose that we have a set of observations, each with measurements on <span class="math inline">\(p = 2\)</span> features, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. We assume that <span class="math inline">\((X_1, X_2)\)</span> are uniformly distributed on <span class="math inline">\([0, 1] \times [0, 1]\)</span>. We wish to predict a test observation’s response using only observations that are within 10% of the range of <span class="math inline">\(X_1\)</span> <em>and</em> within 10% of the range of <span class="math inline">\(X_2\)</span> closest to that test observation. For instance, in order to predict the response for a test observation with <span class="math inline">\(X_1 = 0.6\)</span> and <span class="math inline">\(X_2 = 0.35\)</span>, we will use observations in the range <span class="math inline">\([0.55, 0.65]\)</span> for <span class="math inline">\(X_1\)</span> and in the range <span class="math inline">\([0.3, 0.4]\)</span> for <span class="math inline">\(X_2\)</span>. On average, what fraction of the available observations will we use to make the prediction?</li>
</ol>
</blockquote>
<p>Since we need the observation to be within range for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> we square 9.75% = <span class="math inline">\(0.0975^2 \times 100 = 0.95\%\)</span></p>
<blockquote class="blockquote">
<ol start="3" type="a">
<li>Now suppose that we have a set of observations on <span class="math inline">\(p = 100\)</span> features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10% of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?</li>
</ol>
</blockquote>
<p>Similar to above, we use: <span class="math inline">\(0.0975^{100} \times 100 = 8 \times 10^{-100}\%\)</span>, essentially zero.</p>
<blockquote class="blockquote">
<ol start="4" type="a">
<li>Using your answers to parts (a)–(c), argue that a drawback of KNN when <span class="math inline">\(p\)</span> is large is that there are very few training observations “near” any given test observation.</li>
</ol>
</blockquote>
<p>As <span class="math inline">\(p\)</span> increases, the fraction of observations near any given point rapidly approaches zero. For instance, even if you use 50% of the nearest observations for each <span class="math inline">\(p\)</span>, with <span class="math inline">\(p = 10\)</span>, only <span class="math inline">\(0.5^{10} \times 100 \approx 0.1\%\)</span> points are “near”.</p>
<blockquote class="blockquote">
<ol start="5" type="a">
<li>Now suppose that we wish to make a prediction for a test observation by creating a <span class="math inline">\(p\)</span>-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For $p = $1,2, and 100, what is the length of each side of the hypercube? Comment on your answer.</li>
</ol>
</blockquote>
<p>When <span class="math inline">\(p = 1\)</span>, clearly the length is 0.1. When <span class="math inline">\(p = 2\)</span>, we need the value <span class="math inline">\(l\)</span> such that <span class="math inline">\(l^2 = 0.1\)</span>, so <span class="math inline">\(l = \sqrt{0.1} \approx 0.32\)</span>. With <span class="math inline">\(p\)</span> variables, <span class="math inline">\(l = 0.1^{1/p}\)</span>, so in the case of <span class="math inline">\(p = 100\)</span>, <span class="math inline">\(l = 0.977\)</span>. Therefore, the length of each side of the hypercube rapidly approaches 1 (or 100%) of the range of each <span class="math inline">\(p\)</span>.</p>
</section><section id="question-5" class="level4"><h4 class="anchored" data-anchor-id="question-5">Question 5</h4>
<blockquote class="blockquote">
<p>We now examine the differences between LDA and QDA.</p>
</blockquote>
<blockquote class="blockquote">
<ol type="a">
<li>If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?</li>
</ol>
</blockquote>
<p>QDA because because of overfitting, will always perform better on the training set. But because the decision boundary is linear, on the testing set LDA will perform better.</p>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?</li>
</ol>
</blockquote>
<p>QDA because because it is a more flexible model will perform better. And because the decision boundary is non-linear, on the testing set QDA will still perform better.</p>
<blockquote class="blockquote">
<ol start="3" type="a">
<li>In general, as the sample size <span class="math inline">\(n\)</span> increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?</li>
</ol>
</blockquote>
<p>Improve, will have more data points to take into account the added parameters from LDA (less overfitting), more data to pick up on smaller effects.</p>
<blockquote class="blockquote">
<ol start="4" type="a">
<li>True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.</li>
</ol>
</blockquote>
<p>Not necessarily. The decrease in error rate from flexibility may not offset the increase in bias due to overfitting.</p>
</section><section id="question-6" class="level4"><h4 class="anchored" data-anchor-id="question-6">Question 6</h4>
<blockquote class="blockquote">
<p>Suppose we collect data for a group of students in a statistics class with variables <span class="math inline">\(X_1 =\)</span> hours studied, <span class="math inline">\(X_2 =\)</span> undergrad GPA, and <span class="math inline">\(Y =\)</span> receive an A. We fit a logistic regression and produce estimated coefficient, <span class="math inline">\(\hat\beta_0 = -6\)</span>, <span class="math inline">\(\hat\beta_1= 0.05\)</span>, <span class="math inline">\(\hat\beta_2 = 1\)</span>.</p>
</blockquote>
<blockquote class="blockquote">
<ol type="a">
<li>Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class.</li>
</ol>
</blockquote>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>How many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?</li>
</ol>
</blockquote>
<p><img src="files/images/4-q6.png" class="img-fluid"></p>
</section><section id="question-7" class="level4"><h4 class="anchored" data-anchor-id="question-7">Question 7</h4>
<blockquote class="blockquote">
<p>Suppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on <span class="math inline">\(X\)</span>, last year’s percent profit. We examine a large number of companies and discover that the mean value of <span class="math inline">\(X\)</span> for companies that issued a dividend was <span class="math inline">\(\bar{X} = 10\)</span>, while the mean for those that didn’t was <span class="math inline">\(\bar{X} = 0\)</span>. In addition, the variance of <span class="math inline">\(X\)</span> for these two sets of companies was <span class="math inline">\(\hat{\sigma}^2 = 36\)</span>. Finally, 80% of companies issued dividends. Assuming that <span class="math inline">\(X\)</span> follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was <span class="math inline">\(X = 4\)</span> last year.</p>
</blockquote>
<blockquote class="blockquote">
<p><em>Hint: Recall that the density function for a normal random variable is <span class="math inline">\(f(x) =\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}\)</span>. You will need to use Bayes’ theorem.</em></p>
</blockquote>
<p><img src="files/images/4-q7.png" class="img-fluid"></p>
</section><section id="question-8" class="level4"><h4 class="anchored" data-anchor-id="question-8">Question 8</h4>
<blockquote class="blockquote">
<p>Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e.&nbsp;<span class="math inline">\(K = 1\)</span>) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?</p>
</blockquote>
<p>For <span class="math inline">\(K = 1\)</span>, performance on the training set is perfect and the error rate is zero, implying a test error rate of 36%. Logistic regression outperforms 1-nearest neighbor on the test set and therefore should be preferred.</p>
</section><section id="question-9" class="level4"><h4 class="anchored" data-anchor-id="question-9">Question 9</h4>
<blockquote class="blockquote">
<p>This problem has to do with <em>odds</em>.</p>
</blockquote>
<blockquote class="blockquote">
<ol type="a">
<li>On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?</li>
</ol>
</blockquote>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?</li>
</ol>
</blockquote>
<p><img src="files/images/4-q9.png" class="img-fluid"></p>
</section><section id="question-10" class="level4"><h4 class="anchored" data-anchor-id="question-10">Question 10</h4>
<blockquote class="blockquote">
<p>Equation 4.32 derived an expression for <span class="math inline">\(\log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})\)</span> in the setting where <span class="math inline">\(p &gt; 1\)</span>, so that the mean for the <span class="math inline">\(k\)</span>th class, <span class="math inline">\(\mu_k\)</span>, is a <span class="math inline">\(p\)</span>-dimensional vector, and the shared covariance <span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(p \times p\)</span> matrix. However, in the setting with <span class="math inline">\(p = 1\)</span>, (4.32) takes a simpler form, since the means <span class="math inline">\(\mu_1, ..., \mu_k\)</span> and the variance <span class="math inline">\(\sigma^2\)</span> are scalars. In this simpler setting, repeat the calculation in (4.32), and provide expressions for <span class="math inline">\(a_k\)</span> and <span class="math inline">\(b_{kj}\)</span> in terms of <span class="math inline">\(\pi_k, \pi_K, \mu_k, \mu_K,\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
</blockquote>
<p><img src="files/images/4-q10.png" class="img-fluid"></p>
</section><section id="question-11" class="level4"><h4 class="anchored" data-anchor-id="question-11">Question 11</h4>
<blockquote class="blockquote">
<p>Work out the detailed forms of <span class="math inline">\(a_k\)</span>, <span class="math inline">\(b_{kj}\)</span>, and <span class="math inline">\(b_{kjl}\)</span> in (4.33). Your answer should involve <span class="math inline">\(\pi_k\)</span>, <span class="math inline">\(\pi_K\)</span>, <span class="math inline">\(\mu_k\)</span>, <span class="math inline">\(\mu_K\)</span>, <span class="math inline">\(\Sigma_k\)</span>, and <span class="math inline">\(\Sigma_K\)</span>.</p>
</blockquote>
<p>&lt; skipping &gt;</p>
</section><section id="question-12" class="level4"><h4 class="anchored" data-anchor-id="question-12">Question 12</h4>
<blockquote class="blockquote">
<p>Suppose that you wish to classify an observation <span class="math inline">\(X \in \mathbb{R}\)</span> into <code>apples</code> and <code>oranges</code>. You fit a logistic regression model and find that</p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math display">\[
\hat{Pr}(Y=orange|X=x) =
\frac{\exp(\hat\beta_0 + \hat\beta_1x)}{1 + \exp(\hat\beta_0 + \hat\beta_1x)}
\]</span></p>
</blockquote>
<blockquote class="blockquote">
<p>Your friend fits a logistic regression model to the same data using the <em>softmax</em> formulation in (4.13), and finds that</p>
</blockquote>
<blockquote class="blockquote">
<p><span class="math display">\[
\hat{Pr}(Y=orange|X=x) =
\frac{\exp(\hat\alpha_{orange0} + \hat\alpha_{orange1}x)}
{\exp(\hat\alpha_{orange0} + \hat\alpha_{orange1}x) + \exp(\hat\alpha_{apple0} + \hat\alpha_{apple1}x)}
\]</span></p>
</blockquote>
<blockquote class="blockquote">
<ol type="a">
<li>What is the log odds of <code>orange</code> versus <code>apple</code> in your model?</li>
</ol>
</blockquote>
<blockquote class="blockquote">
<ol start="2" type="a">
<li>What is the log odds of <code>orange</code> versus <code>apple</code> in your friend’s model?</li>
</ol>
</blockquote>
<blockquote class="blockquote">
<ol start="3" type="a">
<li>Suppose that in your model, <span class="math inline">\(\hat\beta_0 = 2\)</span> and <span class="math inline">\(\hat\beta_1 = −1\)</span>. What are the coefficient estimates in your friend’s model? Be as specific as possible.</li>
</ol>
</blockquote>
<blockquote class="blockquote">
<ol start="4" type="a">
<li>Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates <span class="math inline">\(\hat\alpha_{orange0} = 1.2\)</span>, <span class="math inline">\(\hat\alpha_{orange1} = −2\)</span>, <span class="math inline">\(\hat\alpha_{apple0} = 3\)</span>, <span class="math inline">\(\hat\alpha_{apple1} = 0.6\)</span>. What are the coefficient estimates in your model?</li>
</ol>
</blockquote>
<blockquote class="blockquote">
<ol start="5" type="a">
<li>Finally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend’s model? Explain your answer.</li>
</ol>
</blockquote>
<p><img src="files/images/4-q12.png" class="img-fluid"></p>
</section></section><section id="applied" class="level3" data-number="14.3.2"><h3 data-number="14.3.2" class="anchored" data-anchor-id="applied">
<span class="header-section-number">14.3.2</span> Applied</h3>
<section id="question-13" class="level4"><h4 class="anchored" data-anchor-id="question-13">Question 13</h4>
<p>EDA</p>
<div class="cell">
<div class="sourceCode" id="cb75"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load data</span></span>
<span><span class="va">data_weekly</span> <span class="op">&lt;-</span> <span class="fu">ISLR2</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ISLR2/man/Weekly.html">Weekly</a></span></span>
<span></span>
<span><span class="co"># table for the response</span></span>
<span><span class="va">data_weekly</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">summarize</span><span class="op">(</span>.by <span class="op">=</span> <span class="va">Direction</span>,</span>
<span>            n <span class="op">=</span> <span class="fu">n</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">gt</span><span class="fu">::</span><span class="fu"><a href="https://gt.rstudio.com/reference/gt.html">gt</a></span><span class="op">(</span><span class="op">)</span> <span class="co"># try gtSummary to add total row</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div id="wihdpsrnkp" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#wihdpsrnkp table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#wihdpsrnkp thead, #wihdpsrnkp tbody, #wihdpsrnkp tfoot, #wihdpsrnkp tr, #wihdpsrnkp td, #wihdpsrnkp th {
  border-style: none;
}

#wihdpsrnkp p {
  margin: 0;
  padding: 0;
}

#wihdpsrnkp .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#wihdpsrnkp .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#wihdpsrnkp .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#wihdpsrnkp .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#wihdpsrnkp .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#wihdpsrnkp .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#wihdpsrnkp .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#wihdpsrnkp .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#wihdpsrnkp .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#wihdpsrnkp .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#wihdpsrnkp .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#wihdpsrnkp .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#wihdpsrnkp .gt_spanner_row {
  border-bottom-style: hidden;
}

#wihdpsrnkp .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#wihdpsrnkp .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#wihdpsrnkp .gt_from_md > :first-child {
  margin-top: 0;
}

#wihdpsrnkp .gt_from_md > :last-child {
  margin-bottom: 0;
}

#wihdpsrnkp .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#wihdpsrnkp .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#wihdpsrnkp .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#wihdpsrnkp .gt_row_group_first td {
  border-top-width: 2px;
}

#wihdpsrnkp .gt_row_group_first th {
  border-top-width: 2px;
}

#wihdpsrnkp .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#wihdpsrnkp .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#wihdpsrnkp .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#wihdpsrnkp .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#wihdpsrnkp .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#wihdpsrnkp .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#wihdpsrnkp .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#wihdpsrnkp .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#wihdpsrnkp .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#wihdpsrnkp .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#wihdpsrnkp .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#wihdpsrnkp .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#wihdpsrnkp .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#wihdpsrnkp .gt_left {
  text-align: left;
}

#wihdpsrnkp .gt_center {
  text-align: center;
}

#wihdpsrnkp .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#wihdpsrnkp .gt_font_normal {
  font-weight: normal;
}

#wihdpsrnkp .gt_font_bold {
  font-weight: bold;
}

#wihdpsrnkp .gt_font_italic {
  font-style: italic;
}

#wihdpsrnkp .gt_super {
  font-size: 65%;
}

#wihdpsrnkp .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#wihdpsrnkp .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#wihdpsrnkp .gt_indent_1 {
  text-indent: 5px;
}

#wihdpsrnkp .gt_indent_2 {
  text-indent: 10px;
}

#wihdpsrnkp .gt_indent_3 {
  text-indent: 15px;
}

#wihdpsrnkp .gt_indent_4 {
  text-indent: 20px;
}

#wihdpsrnkp .gt_indent_5 {
  text-indent: 25px;
}

#wihdpsrnkp .katex-display {
  display: inline-flex !important;
  margin-bottom: 0.75em !important;
}

#wihdpsrnkp div.Reactable > div.rt-table > div.rt-thead > div.rt-tr.rt-tr-group-header > div.rt-th-group:after {
  height: 0px !important;
}
</style>
<table class="gt_table caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
<thead><tr class="header gt_col_headings">
<th id="Direction" class="gt_col_heading gt_columns_bottom_border gt_center" data-quarto-table-cell-role="th" scope="col">Direction</th>
<th id="n" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">n</th>
</tr></thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_center" headers="Direction">Down</td>
<td class="gt_row gt_right" headers="n">484</td>
</tr>
<tr class="even">
<td class="gt_row gt_center" headers="Direction">Up</td>
<td class="gt_row gt_right" headers="n">605</td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sourceCode" id="cb76"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># correlation plot</span></span>
<span><span class="va">data_weekly</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="fu">where</span><span class="op">(</span><span class="va">is.numeric</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="va">as.matrix</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="va">cor</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">corrplot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/corrplot/man/corrplot.html">corrplot</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># density plots of numeric predictors</span></span>
<span><span class="va">data_weekly</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">pivot_longer</span><span class="op">(</span>cols <span class="op">=</span> <span class="fu">starts_with</span><span class="op">(</span><span class="st">"Lag"</span><span class="op">)</span>,</span>
<span>               names_to <span class="op">=</span> <span class="st">"lag"</span>,</span>
<span>               values_to <span class="op">=</span> <span class="st">"value"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_density</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">value</span>,</span>
<span>                   color <span class="op">=</span> <span class="va">lag</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">facet_grid</span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">.</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-14-2.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb78"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># line plot of number of increases and decreases per year</span></span>
<span><span class="va">data_weekly</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">group_by</span><span class="op">(</span><span class="va">Year</span>, <span class="va">Direction</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">summarize</span><span class="op">(</span>n <span class="op">=</span> <span class="fu">n</span><span class="op">(</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">ggplot</span><span class="op">(</span><span class="fu">aes</span><span class="op">(</span>x <span class="op">=</span> <span class="va">Year</span>,</span>
<span>                y <span class="op">=</span> <span class="va">n</span>,</span>
<span>                color <span class="op">=</span> <span class="va">Direction</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_line</span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>  <span class="fu">geom_smooth</span><span class="op">(</span>se <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-14-3.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb79"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># histograms of other predictors</span></span>
<span><span class="co"># -&gt; some by class of response</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">data_weekly</span><span class="op">$</span><span class="va">Today</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-14-4.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="va">data_weekly</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/split.html">split</a></span><span class="op">(</span><span class="va">.</span><span class="op">$</span><span class="va">Direction</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">map</span><span class="op">(</span>\<span class="op">(</span><span class="va">df</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/graphics/hist.html">hist</a></span><span class="op">(</span><span class="va">df</span><span class="op">$</span><span class="va">Volume</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-14-5.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-14-6.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>$Down
$breaks
 [1]  0  1  2  3  4  5  6  7  8  9 10

$counts
 [1] 230 132  39  19  30  19  11   3   0   1

$density
 [1] 0.475206612 0.272727273 0.080578512 0.039256198 0.061983471 0.039256198
 [7] 0.022727273 0.006198347 0.000000000 0.002066116

$mids
 [1] 0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 9.5

$xname
[1] "df$Volume"

$equidist
[1] TRUE

attr(,"class")
[1] "histogram"

$Up
$breaks
 [1] 0 1 2 3 4 5 6 7 8 9

$counts
[1] 314 138  52  29  31  23  11   6   1

$density
[1] 0.519008264 0.228099174 0.085950413 0.047933884 0.051239669 0.038016529
[7] 0.018181818 0.009917355 0.001652893

$mids
[1] 0.5 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5

$xname
[1] "df$Volume"

$equidist
[1] TRUE

attr(,"class")
[1] "histogram"</code></pre>
</div>
</div>
<p>Observations</p>
<ul>
<li>Balanced-ish response</li>
<li>Fluctuation in number of weeks with Up/Down by year, no real pattern though</li>
<li>Lag variables are roughly normal within class; Volume is not (right skewed)</li>
<li>Year and Volume have a strong, positive correlation</li>
</ul>
<p>Now we can fit a logistic regression model</p>
<div class="cell">
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># fit logistic regression model</span></span>
<span><span class="va">mod_logreg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">Direction</span> <span class="op">~</span> <span class="va">Lag1</span> <span class="op">+</span> <span class="va">Lag2</span> <span class="op">+</span> <span class="va">Lag3</span> <span class="op">+</span> <span class="va">Lag4</span> <span class="op">+</span> <span class="va">Lag5</span> <span class="op">+</span> <span class="va">Volume</span>,</span>
<span>                  data <span class="op">=</span> <span class="va">data_weekly</span>,</span>
<span>                  family <span class="op">=</span> <span class="st">"binomial"</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># view model summary</span></span>
<span><span class="fu">broom</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/tidy.html">tidy</a></span><span class="op">(</span><span class="va">mod_logreg</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 7 × 5
  term        estimate std.error statistic p.value
  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;
1 (Intercept)   0.267     0.0859     3.11  0.00190
2 Lag1         -0.0413    0.0264    -1.56  0.118  
3 Lag2          0.0584    0.0269     2.18  0.0296 
4 Lag3         -0.0161    0.0267    -0.602 0.547  
5 Lag4         -0.0278    0.0265    -1.05  0.294  
6 Lag5         -0.0145    0.0264    -0.549 0.583  
7 Volume       -0.0227    0.0369    -0.616 0.538  </code></pre>
</div>
</div>
<p>Only Lag2 is significant.</p>
<p>Analyze predictions on training data.</p>
<div class="cell">
<div class="sourceCode" id="cb84"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># calculate confusion matrix</span></span>
<span><span class="va">mod_logreg</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">broom</span><span class="fu">::</span><span class="fu"><a href="https://generics.r-lib.org/reference/augment.html">augment</a></span><span class="op">(</span>type.predict <span class="op">=</span> <span class="st">"response"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>predicted <span class="op">=</span> <span class="fu">if_else</span><span class="op">(</span><span class="va">.fitted</span> <span class="op">&gt;</span> <span class="fl">0.5</span>, <span class="st">"Up"</span>, <span class="st">"Down"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">as.factor</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/conf_mat.html">conf_mat</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="st">"Direction"</span>,</span>
<span>                      estimate <span class="op">=</span> <span class="st">"predicted"</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 13 × 3
   .metric              .estimator .estimate
   &lt;chr&gt;                &lt;chr&gt;          &lt;dbl&gt;
 1 accuracy             binary        0.561 
 2 kap                  binary        0.0350
 3 sens                 binary        0.112 
 4 spec                 binary        0.921 
 5 ppv                  binary        0.529 
 6 npv                  binary        0.564 
 7 mcc                  binary        0.0550
 8 j_index              binary        0.0322
 9 bal_accuracy         binary        0.516 
10 detection_prevalence binary        0.0937
11 precision            binary        0.529 
12 recall               binary        0.112 
13 f_meas               binary        0.184 </code></pre>
</div>
</div>
<p>Model is misclassifying the true “Down”s at a high rate; model is not specific.</p>
<div class="cell">
<div class="sourceCode" id="cb86"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># split data</span></span>
<span><span class="va">data_train</span> <span class="op">&lt;-</span> <span class="va">data_weekly</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">Year</span> <span class="op">&lt;=</span> <span class="fl">2008</span><span class="op">)</span></span>
<span><span class="va">data_test</span> <span class="op">&lt;-</span> <span class="va">data_weekly</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/filter.html">filter</a></span><span class="op">(</span><span class="va">Year</span> <span class="op">&gt;</span> <span class="fl">2008</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># define function to fit different types of models</span></span>
<span><span class="co"># -&gt; NOTE: only works with first order models</span></span>
<span><span class="va">fit_model</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">model</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"logreg"</span>, <span class="st">"lda"</span>, <span class="st">"qda"</span>, <span class="st">"knn"</span>, <span class="st">"nb"</span><span class="op">)</span>, <span class="va">formula</span>, <span class="va">response_levels</span>, <span class="va">df_train</span>, <span class="va">df_test</span>, <span class="va">threshold</span> <span class="op">=</span> <span class="fl">0.5</span>, <span class="va">k</span> <span class="op">=</span> <span class="fl">1</span><span class="op">)</span><span class="op">{</span></span>
<span>  </span>
<span>  <span class="co"># set items</span></span>
<span>  <span class="va">model</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/match.arg.html">match.arg</a></span><span class="op">(</span><span class="va">model</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="co"># knn does model and prediction in one step</span></span>
<span>  <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/identical.html">identical</a></span><span class="op">(</span><span class="va">model</span>, <span class="st">"knn"</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span></span>
<span>    </span>
<span>    <span class="co"># extract string of predictors (and format as vector) and response from formula</span></span>
<span>    <span class="va">x</span> <span class="op">=</span> <span class="va">formula</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">str_sub</span><span class="op">(</span><span class="va">.</span>,</span>
<span>                            start <span class="op">=</span>  <span class="fu">str_locate</span><span class="op">(</span><span class="va">.</span>, <span class="st">"~"</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">+</span><span class="fl">2</span>,</span>
<span>                            end <span class="op">=</span> <span class="op">-</span><span class="fl">1</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">.</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu">separate_wider_delim</span><span class="op">(</span>cols <span class="op">=</span> <span class="va">x</span>,</span>
<span>                           delim <span class="op">=</span> <span class="st">" + "</span>,</span>
<span>                           names_sep <span class="op">=</span> <span class="st">""</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu">reduce</span><span class="op">(</span>.f <span class="op">=</span> <span class="va">c</span><span class="op">)</span></span>
<span>    <span class="va">y</span> <span class="op">=</span> <span class="va">formula</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">str_sub</span><span class="op">(</span><span class="va">.</span>,</span>
<span>                            start <span class="op">=</span> <span class="fl">1</span>,</span>
<span>                            end <span class="op">=</span> <span class="fu">str_locate</span><span class="op">(</span><span class="va">.</span>, <span class="st">"~"</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">-</span><span class="fl">2</span><span class="op">)</span> </span>
<span>    </span>
<span>    <span class="co"># fit model and calculate predictions</span></span>
<span>    <span class="va">pred_class</span> <span class="op">=</span> <span class="fu">class</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/class/man/knn.html">knn</a></span><span class="op">(</span>train <span class="op">=</span> <span class="va">df_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">select</span><span class="op">(</span><span class="fu">any_of</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">as.matrix</span>,</span>
<span>                            test <span class="op">=</span> <span class="va">df_test</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">select</span><span class="op">(</span><span class="fu">any_of</span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">as.matrix</span>,</span>
<span>                            cl <span class="op">=</span> <span class="va">df_train</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">pull</span><span class="op">(</span><span class="fu">any_of</span><span class="op">(</span><span class="va">y</span><span class="op">)</span><span class="op">)</span>,</span>
<span>                            k <span class="op">=</span> <span class="va">k</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>pred_class <span class="op">=</span> <span class="va">.</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># initialize empty dataframe for returning</span></span>
<span>    <span class="va">pred_prob</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>pred_prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/rep.html">rep</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="fu"><a href="https://rdrr.io/r/base/length.html">length</a></span><span class="op">(</span><span class="va">pred_class</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span>    </span>
<span>  <span class="op">}</span></span>
<span>  <span class="kw">else</span><span class="op">{</span></span>
<span>    </span>
<span>    <span class="co"># set item</span></span>
<span>    <span class="va">formula</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/formula.html">as.formula</a></span><span class="op">(</span><span class="va">formula</span><span class="op">)</span></span>
<span>    </span>
<span>    <span class="co"># fit model</span></span>
<span>    <span class="va">mod</span> <span class="op">=</span> <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/identical.html">identical</a></span><span class="op">(</span><span class="va">model</span>, <span class="st">"logreg"</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>      </span>
<span>      <span class="fu"><a href="https://rdrr.io/r/stats/glm.html">glm</a></span><span class="op">(</span><span class="va">formula</span>, data <span class="op">=</span> <span class="va">df_train</span>, family <span class="op">=</span> <span class="st">"binomial"</span><span class="op">)</span></span>
<span>      </span>
<span>    <span class="op">}</span></span>
<span>    <span class="kw">else</span> <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/identical.html">identical</a></span><span class="op">(</span><span class="va">model</span>, <span class="st">"lda"</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>      </span>
<span>      <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/lda.html">lda</a></span><span class="op">(</span><span class="va">formula</span>, data <span class="op">=</span> <span class="va">df_train</span><span class="op">)</span></span>
<span>      </span>
<span>    <span class="op">}</span></span>
<span>    <span class="kw">else</span> <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/identical.html">identical</a></span><span class="op">(</span><span class="va">model</span>, <span class="st">"qda"</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>      </span>
<span>      <span class="fu">MASS</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/MASS/man/qda.html">qda</a></span><span class="op">(</span><span class="va">formula</span>, data <span class="op">=</span> <span class="va">df_train</span><span class="op">)</span></span>
<span>      </span>
<span>    <span class="op">}</span></span>
<span>    <span class="kw">else</span> <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/identical.html">identical</a></span><span class="op">(</span><span class="va">model</span>, <span class="st">"nb"</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>      </span>
<span>      <span class="fu">e1071</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/naiveBayes.html">naiveBayes</a></span><span class="op">(</span><span class="va">formula</span>, data <span class="op">=</span> <span class="va">df_train</span><span class="op">)</span></span>
<span></span>
<span>    <span class="op">}</span></span>
<span>    <span class="kw">else</span><span class="op">{</span></span>
<span>      </span>
<span>      <span class="fu">e1071</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/e1071/man/naiveBayes.html">naiveBayes</a></span><span class="op">(</span><span class="va">formula</span>, data <span class="op">=</span> <span class="va">df_train</span><span class="op">)</span></span>
<span>    <span class="op">}</span></span>
<span>    </span>
<span>    <span class="co"># make predictions</span></span>
<span>    <span class="va">pred_prob</span> <span class="op">=</span> <span class="kw">if</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/identical.html">identical</a></span><span class="op">(</span><span class="va">model</span>, <span class="st">"logreg"</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>      </span>
<span>      <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod</span>, type <span class="op">=</span> <span class="st">"response"</span>, newdata <span class="op">=</span> <span class="va">df_test</span><span class="op">)</span></span>
<span>      </span>
<span>    <span class="op">}</span></span>
<span>    <span class="kw">else</span> <span class="kw">if</span><span class="op">(</span><span class="va">model</span> <span class="op"><a href="https://rdrr.io/r/base/match.html">%in%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"lda"</span>, <span class="st">"qda"</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>      </span>
<span>      <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod</span>, newdata <span class="op">=</span> <span class="va">df_test</span><span class="op">)</span><span class="op">$</span><span class="va">posterior</span><span class="op">[</span>,<span class="va">response_levels</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span>      </span>
<span>    <span class="op">}</span></span>
<span>    <span class="kw">else</span><span class="op">{</span></span>
<span>      </span>
<span>      <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">mod</span>, type <span class="op">=</span> <span class="st">"raw"</span>, newdata <span class="op">=</span> <span class="va">df_test</span><span class="op">)</span><span class="op">[</span>,<span class="va">response_levels</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span></span>
<span>    <span class="op">}</span></span>
<span>    </span>
<span>    <span class="co"># classify based on predicted probability and threshold</span></span>
<span>    <span class="va">pred_class</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">pred_prob</span> <span class="op">&gt;</span> <span class="va">threshold</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>pred_class <span class="op">=</span> <span class="va">.</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu">mutate</span><span class="op">(</span>pred_class <span class="op">=</span> <span class="fu">case_when</span><span class="op">(</span><span class="va">pred_class</span> <span class="op">==</span> <span class="fl">1</span> <span class="op">~</span> <span class="va">response_levels</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span>,</span>
<span>                                    .default <span class="op">=</span> <span class="va">response_levels</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html">factor</a></span><span class="op">(</span>levels <span class="op">=</span> <span class="va">response_levels</span><span class="op">)</span><span class="op">)</span></span>
<span>  </span>
<span>  <span class="op">}</span></span>
<span>  </span>
<span>  <span class="fu">bind_cols</span><span class="op">(</span>pred_prob <span class="op">=</span> <span class="va">pred_prob</span>, <span class="va">pred_class</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">return</span></span>
<span>  </span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># initialize items</span></span>
<span><span class="va">models</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"logreg"</span>, <span class="st">"lda"</span>, <span class="st">"qda"</span>, <span class="st">"knn"</span>, <span class="st">"nb"</span><span class="op">)</span>  </span>
<span><span class="va">formula</span> <span class="op">&lt;-</span> <span class="st">"Direction ~ Lag2"</span></span>
<span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fl">1</span></span>
<span></span>
<span><span class="co"># define function to calculate confusion matrix / summary statistics</span></span>
<span><span class="co"># -&gt; NOTE: preds is output of fit_model()</span></span>
<span><span class="va">calc_conf_mat</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">preds</span>, <span class="va">df_test</span>, <span class="va">truth</span>, <span class="va">estimate</span><span class="op">)</span> <span class="op">{</span></span>
<span>  </span>
<span>  <span class="va">df_test</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu">bind_cols</span><span class="op">(</span><span class="va">preds</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>      <span class="fu">yardstick</span><span class="fu">::</span><span class="fu"><a href="https://yardstick.tidymodels.org/reference/conf_mat.html">conf_mat</a></span><span class="op">(</span>truth <span class="op">=</span> <span class="va">truth</span>,</span>
<span>                          estimate <span class="op">=</span> <span class="va">estimate</span><span class="op">)</span></span>
<span>     </span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="co"># fit models and assess quality</span></span>
<span><span class="co"># -&gt; good enough, could loop over a set of formulas</span></span>
<span><span class="co"># -&gt; just manually change formula / k above and look at results</span></span>
<span><span class="co"># -&gt; change event_level if needed</span></span>
<span><span class="va">models</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://magrittr.tidyverse.org/reference/aliases.html">set_names</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">.</span>, nm <span class="op">=</span> <span class="va">.</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">map</span><span class="op">(</span>\<span class="op">(</span><span class="va">model</span><span class="op">)</span> <span class="fu">fit_model</span><span class="op">(</span>model <span class="op">=</span> <span class="va">model</span>, formula <span class="op">=</span> <span class="va">formula</span>, response_levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"Down"</span>, <span class="st">"Up"</span><span class="op">)</span>, df_train <span class="op">=</span> <span class="va">data_train</span>, df_test <span class="op">=</span> <span class="va">data_test</span>, threshold <span class="op">=</span> <span class="fl">0.5</span>, k <span class="op">=</span> <span class="va">k</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">map</span><span class="op">(</span>\<span class="op">(</span><span class="va">preds</span><span class="op">)</span> <span class="fu">calc_conf_mat</span><span class="op">(</span><span class="va">preds</span>, df_test <span class="op">=</span> <span class="va">data_test</span>, truth <span class="op">=</span> <span class="st">"Direction"</span>, estimate <span class="op">=</span> <span class="st">"pred_class"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">map</span><span class="op">(</span>\<span class="op">(</span><span class="va">conf_mat</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">conf_mat</span>, event_level <span class="op">=</span> <span class="st">"second"</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">reduce</span><span class="op">(</span><span class="va">bind_rows</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">bind_cols</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">models</span>,</span>
<span>                       formula <span class="op">=</span> <span class="va">formula</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">model</span>, <span class="va">formula</span>, <span class="va">.estimate</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>k <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">model</span> <span class="op">==</span> <span class="st">"knn"</span>, <span class="va">k</span>, <span class="cn">NA</span><span class="op">)</span>,</span>
<span>         .after <span class="op">=</span> <span class="va">formula</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 4
  model  formula              k .estimate
  &lt;chr&gt;  &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;
1 logreg Direction ~ Lag2    NA     0.625
2 lda    Direction ~ Lag2    NA     0.625
3 qda    Direction ~ Lag2    NA     0.587
4 knn    Direction ~ Lag2     1     0.5  
5 nb     Direction ~ Lag2    NA     0.587</code></pre>
</div>
</div>
<p>Logistic regression and LDA are the best performing.</p>
<p>&lt; Didn’t experiment too much with different predictor sets / transformations &gt;</p>
</section><section id="question-14" class="level4"><h4 class="anchored" data-anchor-id="question-14">Question 14</h4>
<div class="cell">
<div class="sourceCode" id="cb88"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># load data and modify</span></span>
<span><span class="va">data_mpg</span> <span class="op">&lt;-</span> <span class="fu">ISLR2</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/ISLR2/man/Auto.html">Auto</a></span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>mpg01 <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">&lt;=</span> <span class="fu"><a href="https://rdrr.io/r/stats/median.html">median</a></span><span class="op">(</span><span class="va">mpg</span><span class="op">)</span>, <span class="fl">0</span>, <span class="fl">1</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="va">as.factor</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># comparative boxplots of the response against each numeric X</span></span>
<span><span class="va">nms_x</span> <span class="op">&lt;-</span> <span class="va">data_mpg</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="fu">where</span><span class="op">(</span><span class="va">is.numeric</span><span class="op">)</span>, <span class="op">-</span><span class="va">mpg</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="va">colnames</span></span>
<span><span class="fu">map2</span><span class="op">(</span><span class="va">data_mpg</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">select</span><span class="op">(</span><span class="fu">where</span><span class="op">(</span><span class="va">is.numeric</span><span class="op">)</span>, <span class="op">-</span><span class="va">mpg</span><span class="op">)</span>, <span class="va">nms_x</span>, <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">nm</span><span class="op">)</span> <span class="op">{</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/boxplot.html">boxplot</a></span><span class="op">(</span><span class="va">x</span> <span class="op">~</span> <span class="va">mpg01</span>, main <span class="op">=</span> <span class="va">nm</span>, data <span class="op">=</span> <span class="va">data_mpg</span><span class="op">)</span></span>
<span><span class="op">}</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-18-2.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-18-3.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-18-4.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-18-5.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-18-6.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-18-7.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>$cylinders
$cylinders$stats
     [,1] [,2]
[1,]    3    4
[2,]    6    4
[3,]    8    4
[4,]    8    4
[5,]    8    4

$cylinders$n
[1] 196 196

$cylinders$conf
         [,1] [,2]
[1,] 7.774286    4
[2,] 8.225714    4

$cylinders$out
 [1] 6 6 5 8 8 6 6 5 6 3 6 6 6 6 8 6 6

$cylinders$group
 [1] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2

$cylinders$names
[1] "0" "1"


$displacement
$displacement$stats
     [,1] [,2]
[1,]   70   68
[2,]  225   91
[3,]  261  105
[4,]  350  134
[5,]  455  198

$displacement$n
[1] 196 196

$displacement$conf
         [,1]     [,2]
[1,] 246.8929 100.1471
[2,] 275.1071 109.8529

$displacement$out
[1] 200 350 260 350 262

$displacement$group
[1] 2 2 2 2 2

$displacement$names
[1] "0" "1"


$horsepower
$horsepower$stats
     [,1]  [,2]
[1,]   72  46.0
[2,]  100  67.0
[3,]  125  76.5
[4,]  150  90.0
[5,]  225 120.0

$horsepower$n
[1] 196 196

$horsepower$conf
         [,1]     [,2]
[1,] 119.3571 73.90429
[2,] 130.6429 79.09571

$horsepower$out
[1] 230 125 132

$horsepower$group
[1] 1 2 2

$horsepower$names
[1] "0" "1"


$weight
$weight$stats
       [,1] [,2]
[1,] 2124.0 1613
[2,] 3139.5 2045
[3,] 3607.0 2229
[4,] 4159.5 2610
[5,] 5140.0 3420

$weight$n
[1] 196 196

$weight$conf
         [,1]     [,2]
[1,] 3491.886 2165.236
[2,] 3722.114 2292.764

$weight$out
[1] 3530 3900 3725

$weight$group
[1] 2 2 2

$weight$names
[1] "0" "1"


$acceleration
$acceleration$stats
     [,1]  [,2]
[1,]  8.0 11.30
[2,] 12.9 14.70
[3,] 14.5 16.20
[4,] 16.3 17.95
[5,] 21.0 22.20

$acceleration$n
[1] 196 196

$acceleration$conf
         [,1]     [,2]
[1,] 14.11629 15.83321
[2,] 14.88371 16.56679

$acceleration$out
[1] 21.9 23.5 24.8 23.7 24.6

$acceleration$group
[1] 1 2 2 2 2

$acceleration$names
[1] "0" "1"


$year
$year$stats
     [,1] [,2]
[1,]   70   70
[2,]   72   75
[3,]   74   78
[4,]   77   81
[5,]   82   82

$year$n
[1] 196 196

$year$conf
         [,1]     [,2]
[1,] 73.43571 77.32286
[2,] 74.56429 78.67714

$year$out
numeric(0)

$year$group
numeric(0)

$year$names
[1] "0" "1"


$origin
$origin$stats
     [,1] [,2]
[1,]    1    1
[2,]    1    1
[3,]    1    2
[4,]    1    3
[5,]    1    3

$origin$n
[1] 196 196

$origin$conf
     [,1]     [,2]
[1,]    1 1.774286
[2,]    1 2.225714

$origin$out
 [1] 3 2 2 2 3 3 3 2 2 3 2 2 2 3 2 3 2 3 3 2 2 2 2

$origin$group
 [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1

$origin$names
[1] "0" "1"</code></pre>
</div>
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># scatterplot matrix</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/pairs.html">pairs</a></span><span class="op">(</span><span class="va">data_mpg</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">select</span><span class="op">(</span><span class="fu">where</span><span class="op">(</span><span class="va">is.numeric</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure"><p><img src="islr-4_files/figure-html/unnamed-chunk-18-8.png" class="img-fluid figure-img" data-fig-pos="hold" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Displacement, horsepower, weight, and year appear to have a relationship with mpg.</p>
<p>Now we can split into train and test sets.</p>
<div class="cell">
<div class="sourceCode" id="cb91"><pre class="downlit sourceCode r code-with-copy"><code class="sourceCode R"><span><span class="co"># make test / train split</span></span>
<span><span class="va">split_mpg</span> <span class="op">&lt;-</span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">initial_split</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">data_mpg</span>, prop <span class="op">=</span> <span class="fl">.8</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># save train and data</span></span>
<span><span class="va">data_train</span> <span class="op">&lt;-</span> <span class="va">split_mpg</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">training</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="va">data_test</span> <span class="op">&lt;-</span> <span class="va">split_mpg</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="fu">rsample</span><span class="fu">::</span><span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">testing</a></span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># fit all models and get error rates</span></span>
<span><span class="va">models</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"logreg"</span>, <span class="st">"lda"</span>, <span class="st">"qda"</span>, <span class="st">"knn"</span>, <span class="st">"nb"</span><span class="op">)</span></span>
<span><span class="va">k</span> <span class="op">&lt;-</span> <span class="fl">7</span></span>
<span><span class="va">models</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu"><a href="https://magrittr.tidyverse.org/reference/aliases.html">set_names</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">.</span>, nm <span class="op">=</span> <span class="va">.</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">map</span><span class="op">(</span>\<span class="op">(</span><span class="va">model</span><span class="op">)</span> <span class="fu">fit_model</span><span class="op">(</span>model <span class="op">=</span> <span class="va">model</span>, formula <span class="op">=</span> <span class="st">"mpg01 ~ displacement + horsepower + weight + year"</span>, response_levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html">c</a></span><span class="op">(</span><span class="st">"0"</span>, <span class="st">"1"</span><span class="op">)</span>, df_train <span class="op">=</span> <span class="va">data_train</span>, df_test <span class="op">=</span> <span class="va">data_test</span>, threshold <span class="op">=</span> <span class="fl">0.5</span>, k <span class="op">=</span> <span class="va">k</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">map</span><span class="op">(</span>\<span class="op">(</span><span class="va">preds</span><span class="op">)</span> <span class="fu">calc_conf_mat</span><span class="op">(</span><span class="va">preds</span>, df_test <span class="op">=</span> <span class="va">data_test</span>, truth <span class="op">=</span> <span class="st">"mpg01"</span>, estimate <span class="op">=</span> <span class="st">"pred_class"</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">map</span><span class="op">(</span>\<span class="op">(</span><span class="va">conf_mat</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">conf_mat</span>, event_level <span class="op">=</span> <span class="st">"second"</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span>,<span class="op">]</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="co"># pull accuracy (first row)</span></span>
<span>  <span class="fu">reduce</span><span class="op">(</span><span class="va">bind_rows</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">mutate</span><span class="op">(</span>error_rate <span class="op">=</span> <span class="fl">1</span> <span class="op">-</span> <span class="va">.estimate</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span>  <span class="co"># add error rates</span></span>
<span>  <span class="fu">bind_cols</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>model <span class="op">=</span> <span class="va">models</span><span class="op">)</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> </span>
<span>  <span class="fu">select</span><span class="op">(</span><span class="va">model</span>, accuracy <span class="op">=</span> <span class="va">.estimate</span>, <span class="va">error_rate</span><span class="op">)</span> <span class="op"><a href="https://magrittr.tidyverse.org/reference/pipe.html">%&gt;%</a></span> <span class="co"># rename summary of interest</span></span>
<span>  <span class="fu">mutate</span><span class="op">(</span>k <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/ifelse.html">ifelse</a></span><span class="op">(</span><span class="va">model</span> <span class="op">==</span> <span class="st">"knn"</span>, <span class="va">k</span>, <span class="cn">NA</span><span class="op">)</span>,</span>
<span>         .after <span class="op">=</span> <span class="va">model</span><span class="op">)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># A tibble: 5 × 4
  model      k accuracy error_rate
  &lt;chr&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
1 logreg    NA    0.797      0.203
2 lda       NA    0.823      0.177
3 qda       NA    0.835      0.165
4 knn        7    0.835      0.165
5 nb        NA    0.810      0.190</code></pre>
</div>
</div>
<p>For the models tested here, <span class="math inline">\(k = 7\)</span> appears to perform best. QDA has a lower error rate overall than LDA and logistic regression, but only slightly.</p>
<p>These results suggests the decision boundary is slightly more complicated than linear.</p>
</section><section id="question-15" class="level4"><h4 class="anchored" data-anchor-id="question-15">Question 15</h4>
<p>&lt; function writing &gt;</p>
</section><section id="question-16" class="level4"><h4 class="anchored" data-anchor-id="question-16">Question 16</h4>
<p>&lt; another model fitting problem with a different dataset, save for learning tidy models &gt;</p>


<!-- -->

</section></section></section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./islr-3.html" class="pagination-link" aria-label="ISLR -- Linear regression">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">ISLR – Linear regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./islr-5.html" class="pagination-link" aria-label="ISLR -- Resampling methods">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">ISLR – Resampling methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb93" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="fu"># ISLR -- Classification</span></span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a><span class="co">#| label: load-prereqs</span></span>
<span id="cb93-7"><a href="#cb93-7" aria-hidden="true" tabindex="-1"></a><span class="co">#| echo: false</span></span>
<span id="cb93-8"><a href="#cb93-8" aria-hidden="true" tabindex="-1"></a><span class="co">#| message: false</span></span>
<span id="cb93-9"><a href="#cb93-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-10"><a href="#cb93-10" aria-hidden="true" tabindex="-1"></a><span class="co"># knitr options</span></span>
<span id="cb93-11"><a href="#cb93-11" aria-hidden="true" tabindex="-1"></a><span class="fu">source</span>(<span class="st">"_common.R"</span>)</span>
<span id="cb93-12"><a href="#cb93-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-13"><a href="#cb93-13" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-14"><a href="#cb93-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-15"><a href="#cb93-15" aria-hidden="true" tabindex="-1"></a>\newcommand{\follow}<span class="co">[</span><span class="ot">1</span><span class="co">]</span>{\sim \text{#1}\,}</span>
<span id="cb93-16"><a href="#cb93-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-17"><a href="#cb93-17" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % (followsp is short for 'follow special') shortcut that can be used for iid or ind ~ 'Named dist ' in normal font with space before parameters would go --&gt;</span></span>
<span id="cb93-18"><a href="#cb93-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-19"><a href="#cb93-19" aria-hidden="true" tabindex="-1"></a>\newcommand{\e}{\mathrm{e}}</span>
<span id="cb93-20"><a href="#cb93-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-21"><a href="#cb93-21" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- % shortcut for matrix notation --&gt;</span></span>
<span id="cb93-22"><a href="#cb93-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-23"><a href="#cb93-23" aria-hidden="true" tabindex="-1"></a><span class="fu">## Notes</span></span>
<span id="cb93-24"><a href="#cb93-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-25"><a href="#cb93-25" aria-hidden="true" tabindex="-1"></a><span class="fu">### An overview of classification</span></span>
<span id="cb93-26"><a href="#cb93-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-27"><a href="#cb93-27" aria-hidden="true" tabindex="-1"></a>The linear regression model discussed in earlier assumes that the response variable $Y$ is *quantitative*. But in many situations, the response variable is instead *qualitative*. In this chapter, we study approaches for predicting qualitative responses, a process that is known as *classification*.</span>
<span id="cb93-28"><a href="#cb93-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-29"><a href="#cb93-29" aria-hidden="true" tabindex="-1"></a>Just as in the regression setting, in the classification setting we have a set of training observations $(x_1,y_1), \ldots, (x_n,y_n)$ that we can use to build a classifier. We want our classifier to perform well not only on the training data, but also on test observations that were not used to train the classifier.</span>
<span id="cb93-30"><a href="#cb93-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-31"><a href="#cb93-31" aria-hidden="true" tabindex="-1"></a><span class="fu">### Why not linear regression?</span></span>
<span id="cb93-32"><a href="#cb93-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-33"><a href="#cb93-33" aria-hidden="true" tabindex="-1"></a>Could try to code categories to numbers such as </span>
<span id="cb93-34"><a href="#cb93-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-35"><a href="#cb93-35" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-36"><a href="#cb93-36" aria-hidden="true" tabindex="-1"></a>Y = \begin{cases}</span>
<span id="cb93-37"><a href="#cb93-37" aria-hidden="true" tabindex="-1"></a>   1  &amp; \text{if a} <span class="sc">\\</span></span>
<span id="cb93-38"><a href="#cb93-38" aria-hidden="true" tabindex="-1"></a>   2  &amp; \text{if b} <span class="sc">\\</span></span>
<span id="cb93-39"><a href="#cb93-39" aria-hidden="true" tabindex="-1"></a>   3  &amp; \text{if c} <span class="sc">\\</span></span>
<span id="cb93-40"><a href="#cb93-40" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb93-41"><a href="#cb93-41" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-42"><a href="#cb93-42" aria-hidden="true" tabindex="-1"></a>However, this implies an ordering of the outcomes, which means 'b' is above 'a', 'c' is above 'b' and the difference between 'a' and 'b' is the same as the difference between 'b' and 'c'. Typically with categorical variables, order is arbitrary, so it could easily be switched around and lead to a drastically different model. If there is a natural ordering, such as mild, moderate, and severe AND we felt the gap mild and moderate was similar to the gap between moderate and severe, then a 1, 2, 3 coding would be reasonable. Unfortunately, in general there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.</span>
<span id="cb93-43"><a href="#cb93-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-44"><a href="#cb93-44" aria-hidden="true" tabindex="-1"></a>For a *binary* (two level) qualitative response, the situation is better. We can use a dummy variable approach to code the response:</span>
<span id="cb93-45"><a href="#cb93-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-46"><a href="#cb93-46" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-47"><a href="#cb93-47" aria-hidden="true" tabindex="-1"></a>Y = \begin{cases}</span>
<span id="cb93-48"><a href="#cb93-48" aria-hidden="true" tabindex="-1"></a>   0   &amp; \text{if a} <span class="sc">\\</span></span>
<span id="cb93-49"><a href="#cb93-49" aria-hidden="true" tabindex="-1"></a>   1   &amp; \text{if b} <span class="sc">\\</span></span>
<span id="cb93-50"><a href="#cb93-50" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb93-51"><a href="#cb93-51" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-52"><a href="#cb93-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-53"><a href="#cb93-53" aria-hidden="true" tabindex="-1"></a>We could then fit a linear regression to this binary response, and predict 'b' if $\hat{Y} &gt; 0.5$ and 'a' otherwise. In this case, even if we flip the coding, the linear regression will produce the same final predictions (so coding doesn't matter).</span>
<span id="cb93-54"><a href="#cb93-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-55"><a href="#cb93-55" aria-hidden="true" tabindex="-1"></a>For a binary response with a 0/1 coding as above, regression by least squares is not completely unreasonable: it can be shown that the $X\hat{\beta}$ obtained using linear regression is in fact an estimate of $P(Y = 1 \mid X)$ in this special case. However, if we use linear regression, some of our estimates might be outside the <span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span> interval, making them hard to interpret as probabilities!</span>
<span id="cb93-56"><a href="#cb93-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-57"><a href="#cb93-57" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-classification-linreg.png)</span>{width="50%"}</span>
<span id="cb93-58"><a href="#cb93-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-59"><a href="#cb93-59" aria-hidden="true" tabindex="-1"></a>Nevertheless, the predictions provide an ordering and can be interpreted as crude probability estimates. **Curiously, it turns out that the classifications that we get if we use linear regression to predict a binary response will be the same as for the linear discriminant analysis (LDA) procedure shown later.**</span>
<span id="cb93-60"><a href="#cb93-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-61"><a href="#cb93-61" aria-hidden="true" tabindex="-1"></a>To summarize, there are at least two reasons not to perform classification using a regression method: (a) a regression method cannot accommodate a qualitative response with more than two classes; (b) a regression method will not provide meaningful estimates of $P(Y \mid X)$, even with just two classes. Thus, it is preferable to use a classification method that is truly suited for qualitative response values.</span>
<span id="cb93-62"><a href="#cb93-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-63"><a href="#cb93-63" aria-hidden="true" tabindex="-1"></a><span class="fu">### Logistic regression</span></span>
<span id="cb93-64"><a href="#cb93-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-65"><a href="#cb93-65" aria-hidden="true" tabindex="-1"></a>Consider the Default data set, where the response default falls into one of two categories, Yes or No. **Rather than modeling this response Y directly, logistic regression models the *probability* that Y belongs to a particular category.**</span>
<span id="cb93-66"><a href="#cb93-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-67"><a href="#cb93-67" aria-hidden="true" tabindex="-1"></a>For the Default data, logistic regression models the probability of default. For example, the probability of default given balance can be written as</span>
<span id="cb93-68"><a href="#cb93-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-69"><a href="#cb93-69" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-70"><a href="#cb93-70" aria-hidden="true" tabindex="-1"></a>P(\text{default} = \text{Yes} \mid \text{balance})</span>
<span id="cb93-71"><a href="#cb93-71" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-72"><a href="#cb93-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-73"><a href="#cb93-73" aria-hidden="true" tabindex="-1"></a>The values of $P(\text{default} = \text{Yes} \mid \text{balance})$, which we abbreviate $p(\text{balance})$, will range between 0 and 1. Then for any given value of balance, a prediction can be made for default. For example, one might predict default = Yes for any individual for whom $p(\text{balance}) &gt; 0.5$. Alternatively, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as $p(\text{balance}) &gt; 0.1$.</span>
<span id="cb93-74"><a href="#cb93-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-75"><a href="#cb93-75" aria-hidden="true" tabindex="-1"></a><span class="fu">#### The logisitic model {#sec-logistic-model}</span></span>
<span id="cb93-76"><a href="#cb93-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-77"><a href="#cb93-77" aria-hidden="true" tabindex="-1"></a>How should we model the relationship between $p(X) = P(Y = 1 \mid X)$ and $X$? Above, we considered using a linear regression model to represent these probabilities:</span>
<span id="cb93-78"><a href="#cb93-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-79"><a href="#cb93-79" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-80"><a href="#cb93-80" aria-hidden="true" tabindex="-1"></a>p(X) = \beta_0 + \beta_1 X</span>
<span id="cb93-81"><a href="#cb93-81" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-82"><a href="#cb93-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-83"><a href="#cb93-83" aria-hidden="true" tabindex="-1"></a>If we use this approach to predict default=Yes using balance, then we obtain the model shown in the left-hand panel of Figure 4.2, which results in the obvious problem of predictions out of bounds. Any time a straight line is fit to a binary response that is coded as 0 or 1, in principle we can always predict $p(X) &lt; 0$ for some values of $X$ and $p(X) &gt; 1$ for others (unless the range of $X$ is limited).</span>
<span id="cb93-84"><a href="#cb93-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-85"><a href="#cb93-85" aria-hidden="true" tabindex="-1"></a>To avoid this problem, we must model $p(X)$ using a function that gives outputs between 0 and 1 for all values of X. Many functions meet this description. In logistic regression, we use the *logistic function*,</span>
<span id="cb93-86"><a href="#cb93-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-87"><a href="#cb93-87" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-88"><a href="#cb93-88" aria-hidden="true" tabindex="-1"></a>p(X) = \frac{\e^{\beta_0 + \beta_1 X}}{1 + \e^{\beta_0 + \beta_1 X}}  = \frac{1}{1 + \e^{-(\beta_0 + \beta_1 X)}}</span>
<span id="cb93-89"><a href="#cb93-89" aria-hidden="true" tabindex="-1"></a>$${#eq-logistic-function}</span>
<span id="cb93-90"><a href="#cb93-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-91"><a href="#cb93-91" aria-hidden="true" tabindex="-1"></a>To fit this model, we use *maximum likelihood*. The right-hand panel of Figure 4.2 illustrates the fit of the logistic regression model to the Default data. Notice that for low balances we now predict the probability of default as close to, but never below, zero. Likewise, for high balances we predict a default probability close to, but never above, one. The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of $X$, we will obtain a sensible prediction. We also see that the logistic model is better able to capture the range of probabilities than is the linear regression. The average fitted probability in both cases is 0.0333 (averaged over the training data), which is the same as the overall proportion of defaulters in the data set. We can show:</span>
<span id="cb93-92"><a href="#cb93-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-93"><a href="#cb93-93" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-94"><a href="#cb93-94" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb93-95"><a href="#cb93-95" aria-hidden="true" tabindex="-1"></a>p(X) = p &amp;= \frac{\e^{\beta_0 + \beta_1 X}}{1 + \e^{\beta_0 + \beta_1 X}} \quad\text{for simplicity}<span class="sc">\\</span></span>
<span id="cb93-96"><a href="#cb93-96" aria-hidden="true" tabindex="-1"></a>p(1 + \e^{\beta_0 + \beta_1 X}) &amp;= \e^{\beta_0 + \beta_1 X}<span class="sc">\\</span></span>
<span id="cb93-97"><a href="#cb93-97" aria-hidden="true" tabindex="-1"></a>p + p\e^{\beta_0 + \beta_1 X} &amp;= \e^{\beta_0 + \beta_1 X}<span class="sc">\\</span></span>
<span id="cb93-98"><a href="#cb93-98" aria-hidden="true" tabindex="-1"></a>p + p\e^{\beta_0 + \beta_1 X} - \e^{\beta_0 + \beta_1 X} &amp;= 0<span class="sc">\\</span></span>
<span id="cb93-99"><a href="#cb93-99" aria-hidden="true" tabindex="-1"></a>p + \e^{\beta_0 + \beta_1 X}(p - 1) &amp;= 0<span class="sc">\\</span></span>
<span id="cb93-100"><a href="#cb93-100" aria-hidden="true" tabindex="-1"></a>\e^{\beta_0 + \beta_1 X}(p - 1) &amp;= -p<span class="sc">\\</span></span>
<span id="cb93-101"><a href="#cb93-101" aria-hidden="true" tabindex="-1"></a>\e^{\beta_0 + \beta_1 X} &amp;= \frac{-p}{p - 1}<span class="sc">\\</span></span>
<span id="cb93-102"><a href="#cb93-102" aria-hidden="true" tabindex="-1"></a>\e^{\beta_0 + \beta_1 X} &amp;= \frac{p(X)}{1 - p(X)}<span class="sc">\\</span></span>
<span id="cb93-103"><a href="#cb93-103" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb93-104"><a href="#cb93-104" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-105"><a href="#cb93-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-106"><a href="#cb93-106" aria-hidden="true" tabindex="-1"></a>The quantity $p(X) / <span class="co">[</span><span class="ot">1 - p(X)</span><span class="co">]</span>$ is called the *odds*, and can take on any value between 0 and $\infty$. Values of the odds close to 0 and $\infty$ indicate very low and very high probabilities of default, respectively.For example, on average 1 in 5 people with an odds of 1/4 will default, since $p(X) = 0.2$ implies an odds of $\frac{0.2}{1 - 0.2} = 1/4$.</span>
<span id="cb93-107"><a href="#cb93-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-108"><a href="#cb93-108" aria-hidden="true" tabindex="-1"></a>By taking the logarithm of both sides, we arrive at the *log odds* or *logit*.</span>
<span id="cb93-109"><a href="#cb93-109" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-110"><a href="#cb93-110" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-111"><a href="#cb93-111" aria-hidden="true" tabindex="-1"></a>\log\Big(\frac{p(X)}{1 - p(X)}\Big) = \beta_0 + \beta_1 X</span>
<span id="cb93-112"><a href="#cb93-112" aria-hidden="true" tabindex="-1"></a>$${#eq-logit}</span>
<span id="cb93-113"><a href="#cb93-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-114"><a href="#cb93-114" aria-hidden="true" tabindex="-1"></a>Thus, we see that the logistic regression model $p(X) = \frac{\e^{\beta_0 + \beta_1 X}}{1 + \e^{\beta_0 + \beta_1 X}}$ has a logit that is linear in $X$.</span>
<span id="cb93-115"><a href="#cb93-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-116"><a href="#cb93-116" aria-hidden="true" tabindex="-1"></a>Now in a logistic regression model, increasing $X$ by one unit changes the log odds by $\beta_1$. Equivalently, it multiplies the odds by $\e^{\beta_1}$. However, because the relationship between $p(X)$ and $X$ is not a straight line anymore, $\beta_1$ does *not* correspond to the change in $p(X)$ associated with a one-unit increase in X. The amount that $p(X)$ changes due to a one-unit change in $X$ depends on the current value of $X$. But regardless of the value of $X$, if $\beta_1$ is positive then increasing $X$ will be associated with increasing $p(X)$, and if $\beta_1$ is negative then increasing $X$ will be associated with decreasing $p(X)$.</span>
<span id="cb93-117"><a href="#cb93-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-118"><a href="#cb93-118" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Estimating the regression coefficients</span></span>
<span id="cb93-119"><a href="#cb93-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-120"><a href="#cb93-120" aria-hidden="true" tabindex="-1"></a>The coefficients in @eq-logistic-function are unknown, and must be estimated based on the available training data. Although we could use (non-linear) least squares to fit the model @eq-logit, the more general method of maximum likelihood is preferred, since it has better statistical properties. The basic intuition behind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for $\beta_0$ and $\beta_1$ such that the predicted probability $\hat{p}(x_i)$ of default for each individual, using @eq-logistic-function, corresponds as closely as possible to the individual’s observed default status.  In other words, we try to find $\hat{\beta}_0$ and $\hat{\beta}_1$ such that plugging these estimates into the model for $p(X)$, given in @eq-logistic-function, yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not. This intuition can be formalized using a mathematical equation called a *likelihood function*:</span>
<span id="cb93-121"><a href="#cb93-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-122"><a href="#cb93-122" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-123"><a href="#cb93-123" aria-hidden="true" tabindex="-1"></a>\ell(\beta_0, \beta_1) = \prod_{i:y_i = 1} p(x_i) \prod_{i':y_{i'}=0} (1 - p(x_{i'}))</span>
<span id="cb93-124"><a href="#cb93-124" aria-hidden="true" tabindex="-1"></a>$${#eq-logistic-likelihood}</span>
<span id="cb93-125"><a href="#cb93-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-126"><a href="#cb93-126" aria-hidden="true" tabindex="-1"></a>(note that this notation is the same as the following, just without the powers)</span>
<span id="cb93-127"><a href="#cb93-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-128"><a href="#cb93-128" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-129"><a href="#cb93-129" aria-hidden="true" tabindex="-1"></a>\ell(\beta_0, \beta_1) = \prod_{i = 1}^n p(x_i)^{y_i} (1 - p(x_i))^{1 - y_i}</span>
<span id="cb93-130"><a href="#cb93-130" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-131"><a href="#cb93-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-132"><a href="#cb93-132" aria-hidden="true" tabindex="-1"></a>The estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ are chosen to *maximize* this likelihood function. Said another way: to minimize the mis-classification rate, we should predict $Y = 1$ when $p \ge 0.5$ and $Y = 0$ when $p &lt; 0.5$. This means guessing 1 whenever $\beta_0 + X\beta$ is non-negative (as seen in the last version of @eq-logistic-function, where $\frac{1}{1 + \e^{-0}} = 1/2$ is the cutoff point), and 0 otherwise. So logistic regression gives us a linear classifier. The decision boundary separating the two predicted classes is the solution of $\beta_0 + X \beta_1 = 0$, which is a point if $X$ is one dimensional, a line if it is two dimensional, etc.</span>
<span id="cb93-133"><a href="#cb93-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-134"><a href="#cb93-134" aria-hidden="true" tabindex="-1"></a>Maximum likelihood is a very general approach that is used to fit many of the non-linear models that we examine throughout this book. In the linear regression setting, the least squares approach is in fact a special case of maximum likelihood.</span>
<span id="cb93-135"><a href="#cb93-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-136"><a href="#cb93-136" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-logreg-output.png)</span>{width="50%"}</span>
<span id="cb93-137"><a href="#cb93-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-138"><a href="#cb93-138" aria-hidden="true" tabindex="-1"></a>Many aspects of the logistic regression output shown in Table 4.1 are similar to the linear regression output in the previous chapter. For example, we can measure the accuracy of the coefficient estimates by computing their standard errors. The z-statistic in Table 4.1 plays the same role as the t-statistic in the linear regression output. For instance, the z-statistic associated with $\beta_1$ is equal to $\beta_1/SE(\beta_1)$, and so a large (absolute) value of the z-statistic indicates evidence against the null hypothesis $H_0: \beta_1 = 0$. This null hypothesis implies that $p(X) = e^{\beta_0}$: in other words, that the probability of default does not depend on balance. The estimated intercept in Table 4.1 is typically not of interest; its main purpose is to adjust the average fitted probabilities to the proportion of ones in the data (in this case, the overall default rate).</span>
<span id="cb93-139"><a href="#cb93-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-140"><a href="#cb93-140" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Making predictions</span></span>
<span id="cb93-141"><a href="#cb93-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-142"><a href="#cb93-142" aria-hidden="true" tabindex="-1"></a>Once the coefficients have been estimated, we can compute the probability of default for any given credit card balance.</span>
<span id="cb93-143"><a href="#cb93-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-144"><a href="#cb93-144" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-logreg-prediction.png)</span>{width="50%"}</span>
<span id="cb93-145"><a href="#cb93-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-146"><a href="#cb93-146" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Multiple logistic regression</span></span>
<span id="cb93-147"><a href="#cb93-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-148"><a href="#cb93-148" aria-hidden="true" tabindex="-1"></a>We now consider the problem of predicting a binary response using multiple predictors. By analogy with the extension from simple to multiple linear regression in the previous chapter, we can generalize @eq-logit as follows:</span>
<span id="cb93-149"><a href="#cb93-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-150"><a href="#cb93-150" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-151"><a href="#cb93-151" aria-hidden="true" tabindex="-1"></a>\log\Big(\frac{p(X)}{1 - p(X)}\Big) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p</span>
<span id="cb93-152"><a href="#cb93-152" aria-hidden="true" tabindex="-1"></a>$${#eq-logit-multiple}</span>
<span id="cb93-153"><a href="#cb93-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-154"><a href="#cb93-154" aria-hidden="true" tabindex="-1"></a>This can be rewritten as:</span>
<span id="cb93-155"><a href="#cb93-155" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-156"><a href="#cb93-156" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-157"><a href="#cb93-157" aria-hidden="true" tabindex="-1"></a>p(X) = \frac{\e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}{1 + \e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}</span>
<span id="cb93-158"><a href="#cb93-158" aria-hidden="true" tabindex="-1"></a>$${#eq-logistic-function-multiple}</span>
<span id="cb93-159"><a href="#cb93-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-160"><a href="#cb93-160" aria-hidden="true" tabindex="-1"></a>Then we use  maximum likelihood method as before. Beware of interpreting the effects (mainly the sign) of coefficients when some important predictors may be missing from your model. *Confounding* is an issue, just like in MLR.</span>
<span id="cb93-161"><a href="#cb93-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-162"><a href="#cb93-162" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Multinomial logisitic regression</span></span>
<span id="cb93-163"><a href="#cb93-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-164"><a href="#cb93-164" aria-hidden="true" tabindex="-1"></a>We sometimes wish to classify a response variable that has more than two classes. It turns out that it is possible to extend the two-class logistic regression approach to the setting of $K &gt; 2$ classes. This extension is sometimes known as *multinomial logistic regression*. To do this, we first select a single class to serve as the *baseline*; without loss of generality, we select the $K$th class for this role. Then we replace the model @eq-logistic-function-multiple with the model</span>
<span id="cb93-165"><a href="#cb93-165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-166"><a href="#cb93-166" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-167"><a href="#cb93-167" aria-hidden="true" tabindex="-1"></a>p(Y = k \mid X = x) = \frac{\e^{\beta_{k0} + \beta_{k1} X_1 + \cdots + \beta_{kp} X_p}}{1 + \sum_{l = 1}^{K - 1}\e^{\beta_{l0} + \beta_{l1} X_1 + \cdots + \beta_{lp} X_p}}</span>
<span id="cb93-168"><a href="#cb93-168" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-169"><a href="#cb93-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-170"><a href="#cb93-170" aria-hidden="true" tabindex="-1"></a>for $k = 1, \ldots, K-1$ and </span>
<span id="cb93-171"><a href="#cb93-171" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-172"><a href="#cb93-172" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-173"><a href="#cb93-173" aria-hidden="true" tabindex="-1"></a>p(Y = K \mid X = x) = \frac{1}{1 + \sum_{l = 1}^{K - 1}\e^{\beta_{l0} + \beta_{l1} X_1 + \cdots + \beta_{lp} X_p}}</span>
<span id="cb93-174"><a href="#cb93-174" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-175"><a href="#cb93-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-176"><a href="#cb93-176" aria-hidden="true" tabindex="-1"></a>It is not hard to show that for $k = 1, \ldots, K-1$, </span>
<span id="cb93-177"><a href="#cb93-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-178"><a href="#cb93-178" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-179"><a href="#cb93-179" aria-hidden="true" tabindex="-1"></a>\log\Big(\frac{P(Y = k \mid X = x)}{P(Y = K \mid X = x)}\Big) = \beta_{k0} + \beta_{k1} X_1 + \cdots + \beta_{kp} X_p</span>
<span id="cb93-180"><a href="#cb93-180" aria-hidden="true" tabindex="-1"></a>$${#eq-multinomial-logreg}</span>
<span id="cb93-181"><a href="#cb93-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-182"><a href="#cb93-182" aria-hidden="true" tabindex="-1"></a>Notice that this is quite similar to @eq-logit and indicates that once again, the log odds between any pair of classes is linear in the features. The decision of which class to have as the baseline is unimportant. If we were to change the baseline, the coefficient estimates would differ between the two fitted models, but the fitted values (predictions), the log odds between any pair of classes, and the other key model outputs will remain the same. Nonetheless, interpretation of the coefficients in a multinomial logistic regression model must be done with care, since it is tied to the choice of baseline.</span>
<span id="cb93-183"><a href="#cb93-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-184"><a href="#cb93-184" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-multi-logreg-coefs.png)</span>{width="50%"}</span>
<span id="cb93-185"><a href="#cb93-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-186"><a href="#cb93-186" aria-hidden="true" tabindex="-1"></a>Textbook presents an alternative coding for multinomial logistic regression, known as the *softmax* coding. This is used a lot in machine learning; skipping for now.</span>
<span id="cb93-187"><a href="#cb93-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-188"><a href="#cb93-188" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generative models for classification</span></span>
<span id="cb93-189"><a href="#cb93-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-190"><a href="#cb93-190" aria-hidden="true" tabindex="-1"></a>**Logistic regression involves directly modeling $P(Y = k \mid X = x)$ using the logistic function, given by eq-logistic-function-multiple for the case of two response classes.** In statistical jargon, we model the conditional distribution of the response $Y$, given the predictor(s) $X$. We now consider an alternative and less direct approach to estimating these probabilities. **In this new approach, we model the distribution of the predictors $X$ separately in each of the response classes (i.e. for each value of $Y$). We then use Bayes' theorem to flip these around into estimates for $P(Y = k \mid X = x)$.** When the distribution of $X$ within each class is assumed to be normal, it turns out that the model is very similar in form to logistic regression.</span>
<span id="cb93-191"><a href="#cb93-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-192"><a href="#cb93-192" aria-hidden="true" tabindex="-1"></a>Why do we need another method, when we have logistic regression? There are several reasons:</span>
<span id="cb93-193"><a href="#cb93-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-194"><a href="#cb93-194" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>When there is substantial separation between the two classes, the parameter estimates for the logistic regression model are surprisingly unstable. The methods that we consider in this section do not suffer from this problem.</span>
<span id="cb93-195"><a href="#cb93-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-196"><a href="#cb93-196" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If the distribution of the predictors $X$ is approximately normal in each of the classes and the sample size is small, then the approaches in this section may be more accurate than logistic regression.</span>
<span id="cb93-197"><a href="#cb93-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-198"><a href="#cb93-198" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>The methods in this section can be naturally extended to the case of more than two response classes.</span>
<span id="cb93-199"><a href="#cb93-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-200"><a href="#cb93-200" aria-hidden="true" tabindex="-1"></a>Suppose that we wish to classify an observation into one of $K$ classes, where $K \ge 2$. In other words, the qualitative response variable $Y$ can take on $ K$ possible distinct and unordered values. Let $\pi_k$ represent the overall or prior probability that a randomly chosen observation comes from the $k$th class. Let $f_k(X) \equiv P(X \mid Y = k)$ denote the *density function* of $X$ for an observation that comes from the $k$th class. In other words, $f_k(x)$ is relatively large if there is a high probability that an observation in the $k$th class has $X \approx x$, and $f_k(x)$ is small if it is very unlikely that an observation in the $k$th class has $X \approx x$. Then Bayes' theorem states that </span>
<span id="cb93-201"><a href="#cb93-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-202"><a href="#cb93-202" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-203"><a href="#cb93-203" aria-hidden="true" tabindex="-1"></a>P(Y = k \mid X = x) = \frac{\pi_k f_k(x)}{\sum_{l = 1}^K \pi_l f_l(x)}</span>
<span id="cb93-204"><a href="#cb93-204" aria-hidden="true" tabindex="-1"></a>$${#eq-bayes-theorem}</span>
<span id="cb93-205"><a href="#cb93-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-206"><a href="#cb93-206" aria-hidden="true" tabindex="-1"></a>!!! draw tree picture</span>
<span id="cb93-207"><a href="#cb93-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-208"><a href="#cb93-208" aria-hidden="true" tabindex="-1"></a>In accordance with our earlier notation, we will use the abbreviation $p_k(x) = P(Y = k \mid X = x)$; this is the posterior probability that an observation $X = x$ belongs to the $k$th class. That is, it is the probability that the observation belongs to the $k$th class, *given* the predictor value for that observation.</span>
<span id="cb93-209"><a href="#cb93-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-210"><a href="#cb93-210" aria-hidden="true" tabindex="-1"></a>The above equation suggests that instead of directly computing the posterior probability $p_k(x)$ as in @sec-logistic-model, we can simply plug in estimates of $\pi_k$ and $f_k(x)$ into @eq-bayes-theorem. In general, estimating $\pi_k$ is easy if we have a random sample from the population: we simply compute the fraction of the training observations that belong to the $k$th class. However, estimating the density function $f_k(x)$ is much more challenging. As we will see, to estimate $f_k(x)$, we will typically have to make some simplifying assumptions.</span>
<span id="cb93-211"><a href="#cb93-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-212"><a href="#cb93-212" aria-hidden="true" tabindex="-1"></a>We know from earlier that the Bayes classifier, which classifies an observation $x$ to the class for which $p_k(x)$ is largest, has the lowest possible error rate out of all classifiers. (Of course, this is only true if all of the terms in @eq-bayes-theorem are correctly specified.) Therefore, if we can find a way to estimate $f_k(x)$, then we can plug it into @eq-bayes-theorem in order to approximate the Bayes classifier.</span>
<span id="cb93-213"><a href="#cb93-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-214"><a href="#cb93-214" aria-hidden="true" tabindex="-1"></a>In the following sections, we discuss three classifiers that use different estimates of $f_k(x)$ in @eq-bayes-theorem to approximate the Bayes classifier: *linear discriminant analysis*, *quadratic discriminant analysis*, and *naive Bayes*.</span>
<span id="cb93-215"><a href="#cb93-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-216"><a href="#cb93-216" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Linear discrimant analysis for $p = 1$</span></span>
<span id="cb93-217"><a href="#cb93-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-218"><a href="#cb93-218" aria-hidden="true" tabindex="-1"></a>For now, assume that $p = 1$ —- that is, we have only one predictor. We would like to obtain an estimate for $f_k(x)$ that we can plug into @eq-bayes-theorem in order to estimate $p_k(x)$. We will then classify an observation to the class for which $p_k(x)$ is greatest. To estimate $f_k(x)$, we will first make some assumptions about its form.</span>
<span id="cb93-219"><a href="#cb93-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-220"><a href="#cb93-220" aria-hidden="true" tabindex="-1"></a>In particular, we assume that $f_k(x)$ is *normal* or *Gaussian.* In the one- normal dimensional setting, the normal density takes the form</span>
<span id="cb93-221"><a href="#cb93-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-222"><a href="#cb93-222" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-223"><a href="#cb93-223" aria-hidden="true" tabindex="-1"></a>f_k(x) = \frac{1}{\sqrt{2\pi} \sigma_k} \exp\Big(-\frac{1}{2\sigma_k^2}(x - \mu_k)^2\Big)</span>
<span id="cb93-224"><a href="#cb93-224" aria-hidden="true" tabindex="-1"></a>$${#eq-normal-x}</span>
<span id="cb93-225"><a href="#cb93-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-226"><a href="#cb93-226" aria-hidden="true" tabindex="-1"></a>where $\mu_k$ and $\sigma_k^2$ are the mean and variance parameters for the $k$th class. For now, let us further assume that $\sigma_1^2 = \cdots = \sigma_K^2$, that is, there is a shared variance term across all $K$ classes, which for simplicity we can denote by $\sigma^2$. Plugging this into @eq-bayes-theorem, we find that </span>
<span id="cb93-227"><a href="#cb93-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-228"><a href="#cb93-228" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-229"><a href="#cb93-229" aria-hidden="true" tabindex="-1"></a>p_k(x) = \frac{\pi_k \frac{1}{\sqrt{2\pi} \sigma} \exp\big(-\frac{1}{2\sigma^2}(x - \mu_k)^2\big)}{\sum_{l = 1}^K \pi_l \frac{1}{\sqrt{2\pi} \sigma} \exp\big(-\frac{1}{2\sigma^2}(x - \mu_l)^2\big)}</span>
<span id="cb93-230"><a href="#cb93-230" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-231"><a href="#cb93-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-232"><a href="#cb93-232" aria-hidden="true" tabindex="-1"></a>The Bayes classifier involves assigning an observation $X = x$ to the class for which the above equation is the largest (recall this Bayes classifier is different than Bayes theorem @eq-bayes-theorem, which allows us to manipulate conditional distributions). Taking the log of this and rearranging the terms, it is not hard to show that this is equivalent to assigning the observation to the class for which</span>
<span id="cb93-233"><a href="#cb93-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-234"><a href="#cb93-234" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-235"><a href="#cb93-235" aria-hidden="true" tabindex="-1"></a>\delta_k(x) = x \frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log(\pi_k)</span>
<span id="cb93-236"><a href="#cb93-236" aria-hidden="true" tabindex="-1"></a>$${#eq-bayes-decision-boundary}</span>
<span id="cb93-237"><a href="#cb93-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-238"><a href="#cb93-238" aria-hidden="true" tabindex="-1"></a>is largest. For instance, if $K = 2$ and $\pi_1 = \pi_2$, then the Bayes classifier assignes an observation to class 1 if $2x(\mu_1 - \mu_2) &gt; \mu_1^2 - \mu_2^2$ and to class 2 otherwise. The Bayes decision boundary is the point for which $\delta_1(x) = \delta_2(x)$; one can show that this amounts to</span>
<span id="cb93-239"><a href="#cb93-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-240"><a href="#cb93-240" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-241"><a href="#cb93-241" aria-hidden="true" tabindex="-1"></a>x = \frac{\mu_1^2 - \mu_2^2}{2(\mu_1 - \mu_2)} = \frac{\mu_1 + \mu_2}{2}</span>
<span id="cb93-242"><a href="#cb93-242" aria-hidden="true" tabindex="-1"></a>$${#eq-bayes-decision-boundary2}</span>
<span id="cb93-243"><a href="#cb93-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-244"><a href="#cb93-244" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- !!! do this math --&gt;</span></span>
<span id="cb93-245"><a href="#cb93-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-246"><a href="#cb93-246" aria-hidden="true" tabindex="-1"></a>An example is shown in the left-hand panel of Figure 4.4. The two normal density functions that are displayed, $f_1(x)$ and $f_2(x)$, represent two distinct classes. The mean and variance parameters for the two density functions are $\mu_1 = −1.25, \mu_2 = 1.25$ and $\sigma_1^2 = \sigma_2^2 = 1$. The two densities overlap, and so given that $X = x$, there is some uncertainty about the class to which the observation belongs. If we assume that an observation is equally likely to come from either class -- that is, $\pi_1 = \pi_2 = 0.5$ --then by inspection of @bayes-decision-boundary2, we see that the Bayes classifier assigns the observation to class 1 if $x &lt; 0$ and class 2 otherwise. Note that in this case, we can compute the Bayes classifier because we know that $X$ is drawn from a Gaussian distribution within each class, and we know all of the parameters involved. In a real-life situation, we are not able to calculate the Bayes classifier.</span>
<span id="cb93-247"><a href="#cb93-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-248"><a href="#cb93-248" aria-hidden="true" tabindex="-1"></a>In practice, even if we are quite certain of our assumption that $X$ is drawn from a Gaussian distribution within each class, to apply the Bayes classifier we still have to estimate the parameters $\mu_1, \ldots, \mu_K, \pi_1, \ldots, \pi_K$, and $\sigma^2$. The *linear discriminant analysis* (LDA) method approximates the Bayes classifier by plugging estimates for $\pi_k$, $\mu_k$, and $\sigma^2$ into @bayes-decision-boundary. In particular, the following estimates are used:</span>
<span id="cb93-249"><a href="#cb93-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-250"><a href="#cb93-250" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-251"><a href="#cb93-251" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb93-252"><a href="#cb93-252" aria-hidden="true" tabindex="-1"></a>\hat{\mu}_k &amp;= \frac{1}{n_k} \sum_{i:y_i = k} x_i<span class="sc">\\</span></span>
<span id="cb93-253"><a href="#cb93-253" aria-hidden="true" tabindex="-1"></a>\hat{\sigma}^2 &amp;= \frac{1}{n - K} \sum_{k = 1}^K \sum_{i:y_i = k} (x_i - \hat{\mu}_k)^2<span class="sc">\\</span></span>
<span id="cb93-254"><a href="#cb93-254" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb93-255"><a href="#cb93-255" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-256"><a href="#cb93-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-257"><a href="#cb93-257" aria-hidden="true" tabindex="-1"></a>where $n$ is the total number of training observations, and $n_k$ is the number of training observations in the $k$th class. The estimate for $\mu_k$ is simply the average of all the training observations from the $k$th class, while $\sigma^2$ can be seen as a weighted average of the sample variances for each of the $K$ classes. Sometimes we have knowledge of the class membership probabilities $\pi_1,  \ldots, \pi_K$, which can be used directly. In the absence of any additional information, LDA estimates $\pi_k$ using the proportion of the training observations that belong to the $k$th class. In other words,</span>
<span id="cb93-258"><a href="#cb93-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-259"><a href="#cb93-259" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-260"><a href="#cb93-260" aria-hidden="true" tabindex="-1"></a>\hat{\pi}_k = n_k / n</span>
<span id="cb93-261"><a href="#cb93-261" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-262"><a href="#cb93-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-263"><a href="#cb93-263" aria-hidden="true" tabindex="-1"></a>The LDA classifier plugs the estimates above into @eq-bayes-decision-boundary and assigns an observation $X = x$ to the class for which </span>
<span id="cb93-264"><a href="#cb93-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-265"><a href="#cb93-265" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-266"><a href="#cb93-266" aria-hidden="true" tabindex="-1"></a>\hat{\delta}_k(x) = x \frac{\hat{\mu}_k}{\hat{\sigma}^2} - \frac{\hat{\mu}_k^2}{2\hat{\sigma}^2} + \log(\hat{\pi}_k)</span>
<span id="cb93-267"><a href="#cb93-267" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-268"><a href="#cb93-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-269"><a href="#cb93-269" aria-hidden="true" tabindex="-1"></a>is largest. The word *linear* in the classifier's name stems from the fact</span>
<span id="cb93-270"><a href="#cb93-270" aria-hidden="true" tabindex="-1"></a>that the discriminant functions $\hat{\delta}_k(x)$ above are linear functions of $x$ (as discriminant opposed to a more complex function of $x$).</span>
<span id="cb93-271"><a href="#cb93-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-272"><a href="#cb93-272" aria-hidden="true" tabindex="-1"></a>The right-hand panel of Figure 4.4 displays a histogram of a random sample of 20 observations from each class. In this case, since $n_1 = n_2 = 20$, we have $\hat{\pi}_1 = \hat{\pi}_2$. As a result, the decision boundary corresponds to the midpoint between the sample means for the two classes, $(\hat{\mu}_1 + \hat{\mu}_2) / 2$.</span>
<span id="cb93-273"><a href="#cb93-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-274"><a href="#cb93-274" aria-hidden="true" tabindex="-1"></a>To reiterate, the LDA classifier results from assuming that the observations within each class come from a normal distribution with a class-specific mean and a common variance $\sigma^2$, and plugging estimates for these parameters into the Bayes classifier. In a later section, we will consider a less stringent set of assumptions, by allowing the observations in the $k$th class to have a class-specific variance, $\sigma_k^2$.</span>
<span id="cb93-275"><a href="#cb93-275" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-276"><a href="#cb93-276" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Linear discriminant analysis for $p &gt; 1$</span></span>
<span id="cb93-277"><a href="#cb93-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-278"><a href="#cb93-278" aria-hidden="true" tabindex="-1"></a>We now extend the LDA classifier to the case of multiple predictors. To do this, we will assume that $X = (X_1, X_2, \ldots, X_p)$ is drawn from a *multivariate Gaussian* (or multivariate normal) distribution, with a class-specific  mean vector and a common covariance matrix.</span>
<span id="cb93-279"><a href="#cb93-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-280"><a href="#cb93-280" aria-hidden="true" tabindex="-1"></a>The multivariate Gaussian distribution assumes that each individual predictor follows a one-dimensional normal distribution, as in @eq-normal-x, with some correlation between each pair of predictors.  </span>
<span id="cb93-281"><a href="#cb93-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-282"><a href="#cb93-282" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-multivariate-normal.png)</span>{width="50%"}</span>
<span id="cb93-283"><a href="#cb93-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-284"><a href="#cb93-284" aria-hidden="true" tabindex="-1"></a>To indicate that a $p$-dimensional random variable $X$ has a multivariate Gaussian distribution, we write $X \follow{N}(\mu, \Sigma)$. Here, $E(X) = \mu$ is the mean of $X$ (a vector with $p$ components), and $\text{Cov}(X) = \Sigma$ is the $p \times p$ covariance matrix of $X$. Formally, the multivariate Gaussian density is defined as</span>
<span id="cb93-285"><a href="#cb93-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-286"><a href="#cb93-286" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-287"><a href="#cb93-287" aria-hidden="true" tabindex="-1"></a>f(x) = \frac{1}{(2\pi)^{p/2} \lvert \Sigma \rvert^{1/2}} \exp\Big(-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu)\Big)</span>
<span id="cb93-288"><a href="#cb93-288" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-289"><a href="#cb93-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-290"><a href="#cb93-290" aria-hidden="true" tabindex="-1"></a>In the case of $p &gt; 1$ predictors, the LDA classifier assumes that the observations in the $k$th class are drawn from a multivariate Gaussian distribution $N(\mu_k, \Sigma)$, where $\mu_k$ is a class-specific mean vector, and $\Sigma$ is a covariance matrix that is common to all $K$ classes. Plugging the density function for the $k$th class, $f_k(X = x)$, into @eq-bayes-theorem and performing a little bit of algebra reveals that the Bayes classifier assigns an observation $X = x$ to the class for which</span>
<span id="cb93-291"><a href="#cb93-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-292"><a href="#cb93-292" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-293"><a href="#cb93-293" aria-hidden="true" tabindex="-1"></a>\delta_k(x) = x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T \Sigma^{-1}\mu_k + \log \pi_k</span>
<span id="cb93-294"><a href="#cb93-294" aria-hidden="true" tabindex="-1"></a>$${#eq-bayes-decision-boundary3}</span>
<span id="cb93-295"><a href="#cb93-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-296"><a href="#cb93-296" aria-hidden="true" tabindex="-1"></a>is largest. This is the vector / matrix version of @eq-bayes-decision-boundary.</span>
<span id="cb93-297"><a href="#cb93-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-298"><a href="#cb93-298" aria-hidden="true" tabindex="-1"></a>An example is shown in the left-hand panel of Figure 4.6.</span>
<span id="cb93-299"><a href="#cb93-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-300"><a href="#cb93-300" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-lda-example.png)</span>{width="50%"}</span>
<span id="cb93-301"><a href="#cb93-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-302"><a href="#cb93-302" aria-hidden="true" tabindex="-1"></a>Three equally-sized Gaussian classes are shown with class-specific mean vectors and a common covariance matrix. The three ellipses represent regions that contain 95% of the probability for each of the three classes. The dashed lines are the Bayes decision boundaries. In other words, they represent the set of values x for which $\delta_k(x) = \delta_l(x)$; i.e.</span>
<span id="cb93-303"><a href="#cb93-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-304"><a href="#cb93-304" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-305"><a href="#cb93-305" aria-hidden="true" tabindex="-1"></a>x^T \Sigma^{-1}\mu_k - \frac{1}{2}\mu_k^T \Sigma^{-1}\mu_k = x^T \Sigma^{-1}\mu_l - \frac{1}{2}\mu_l^T \Sigma^{-1}\mu_l</span>
<span id="cb93-306"><a href="#cb93-306" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-307"><a href="#cb93-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-308"><a href="#cb93-308" aria-hidden="true" tabindex="-1"></a>for $k \ne l$. (The $\log \pi_k$) term has disappeared because each of the three classes has the same number of training observations). Note that there are three lines representing the Bayes decision boundaries because there are three *pairs of classes* among the three classes. That is, one Bayes decision boundary separates class 1 from class 2, one separates class 1 from class 3, and one separates class 2 from class 3. These three Bayes decision boundaries divide the predictor space into three regions. The Bayes classifier will classify an observation according to the region in which it is located.</span>
<span id="cb93-309"><a href="#cb93-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-310"><a href="#cb93-310" aria-hidden="true" tabindex="-1"></a>Once again, estimate the all the unknown parameters, similarly to how performed with $p = 1$ and plug into @eq-bayes-decision-boundary3. Then assign the class to that which results in the largest $\hat{\delta}_k(x)$. Note that this is still a *linear* function of $x$</span>
<span id="cb93-311"><a href="#cb93-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-312"><a href="#cb93-312" aria-hidden="true" tabindex="-1"></a>Results of classification can be compactly shown in *confusion matrices*.</span>
<span id="cb93-313"><a href="#cb93-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-314"><a href="#cb93-314" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-confusion-matrix.png)</span>{width="50%"}</span>
<span id="cb93-315"><a href="#cb93-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-316"><a href="#cb93-316" aria-hidden="true" tabindex="-1"></a>For the default data, the LDA model fit to the 10,000 training samples results in a training error rate of 2.75%. This sounds like a low error rate, but two caveats must be noted.</span>
<span id="cb93-317"><a href="#cb93-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-318"><a href="#cb93-318" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>First of all, training error rates will usually be lower than test error</span>
<span id="cb93-319"><a href="#cb93-319" aria-hidden="true" tabindex="-1"></a>rates, which are the real quantity of interest.</span>
<span id="cb93-320"><a href="#cb93-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-321"><a href="#cb93-321" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Second, since only 3.33% of the individuals in the training sample defaulted, a simple but useless classifier that always predicts that an individual will not default, regardless of his or her credit card balance and student status, will result in an error rate of 3.33 %. In other words, the trivial null classifier will achieve an error rate that null is only a bit higher than the LDA training set error rate.</span>
<span id="cb93-322"><a href="#cb93-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-323"><a href="#cb93-323" aria-hidden="true" tabindex="-1"></a>In practice, a binary classifier such as this one can make two types of</span>
<span id="cb93-324"><a href="#cb93-324" aria-hidden="true" tabindex="-1"></a>errors: it can incorrectly assign an individual who defaults to the no default category, or it can incorrectly assign an individual who does not default to</span>
<span id="cb93-325"><a href="#cb93-325" aria-hidden="true" tabindex="-1"></a>the default category. It is often of interest to determine which of these two</span>
<span id="cb93-326"><a href="#cb93-326" aria-hidden="true" tabindex="-1"></a>types of errors are being made. The table reveals that LDA predicted that a total of 104 people would default. Of these people, 81 actually defaulted and 23 did not. Hence only 23 out of 9,667 of the individuals who did not default were incorrectly labeled. This looks like a pretty low error rate! However, of the 333 individuals who defaulted, 252 (or 75.7%) were missed by LDA. **So while the overall error rate is low, the error rate among individuals who defaulted is very high.**</span>
<span id="cb93-327"><a href="#cb93-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-328"><a href="#cb93-328" aria-hidden="true" tabindex="-1"></a>Also, notice that in this example student status is qualitative -- thus, the normality assumption made by LDA is clearly violated! **However, LDA is often remarkably robust to model violations, as this example shows. Naive Bayes, discussed later, provides an alternative to LDA that does not assume normally distributed predictors.**</span>
<span id="cb93-329"><a href="#cb93-329" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-330"><a href="#cb93-330" aria-hidden="true" tabindex="-1"></a>Class-specific performance is also important in medicine and biology, where the terms *sensitivity* and *specificity* characterize the performance of a classifier or screening test.</span>
<span id="cb93-331"><a href="#cb93-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-332"><a href="#cb93-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sensitivity = true positive % (e.g. true defaulters that are identified; 81/333 = 24.3%)</span>
<span id="cb93-333"><a href="#cb93-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-334"><a href="#cb93-334" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Specificity = true negative % (e.g. true non-defaulters that are identified; 9644/9667 = 1 - 23/9667 = 99.8%)</span>
<span id="cb93-335"><a href="#cb93-335" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-336"><a href="#cb93-336" aria-hidden="true" tabindex="-1"></a>Why does LDA do such a poor job of classifying the customers who default? In other words, why does it have such low sensitivity? As we have seen, LDA is trying to approximate the Bayes classifier, which has the lowest total error rate out of all classifiers. That is, the Bayes classifier will yield the smallest possible total number of misclassified observations, *regardless of the class from which the errors stem*. Some misclassifications will result from incorrectly assigning a customer who does not default to the default class, and others will result from incorrectly assigning a customer who defaults to the non-default class. In contrast, a credit card company might particularly wish to avoid incorrectly classifying an individual who will default, whereas incorrectly classifying an individual who will not default, though still to be avoided, is less problematic. We will now see that it is possible to modify LDA in order to develop a classifier that better meets the credit card company’s needs.</span>
<span id="cb93-337"><a href="#cb93-337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-338"><a href="#cb93-338" aria-hidden="true" tabindex="-1"></a>The Bayes classifier works by assigning an observation to the class for which the posterior probability $p_k(X)$ is greatest. In the two-class case, this amounts to assigning an observation to the default class if</span>
<span id="cb93-339"><a href="#cb93-339" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-340"><a href="#cb93-340" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-341"><a href="#cb93-341" aria-hidden="true" tabindex="-1"></a>P(\text{default} = \text{Yes} \mid X = x) &gt; 0.5</span>
<span id="cb93-342"><a href="#cb93-342" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-343"><a href="#cb93-343" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-344"><a href="#cb93-344" aria-hidden="true" tabindex="-1"></a>Thus, the Bayes classifier, and by extension LDA, uses a threshold of 50 % for the posterior probability of default in order to assign an observation to the *default* class. However, if we are concerned about incorrectly predicting the default status for individuals who default, then we can consider lowering this threshold. For instance, we might label any customer with a posterior probability of default above 20% to the *default* class:</span>
<span id="cb93-345"><a href="#cb93-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-346"><a href="#cb93-346" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-347"><a href="#cb93-347" aria-hidden="true" tabindex="-1"></a>P(\text{default} = \text{Yes} \mid X = x) &gt; 0.2</span>
<span id="cb93-348"><a href="#cb93-348" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-349"><a href="#cb93-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-350"><a href="#cb93-350" aria-hidden="true" tabindex="-1"></a>The error rates that result from taking this approach are shown in Table 4.5. Now LDA predicts that 430 individuals will default. Of the 333 individuals who default, LDA correctly predicts all but 138, or 41.4 %. This is a vast improvement over the error rate of 75.7% that resulted from using the threshold of 50%. However, this improvement comes at a cost: now 235 individuals who do not default are incorrectly classified. As a result, the overall error rate has increased slightly to 3.73 %.</span>
<span id="cb93-351"><a href="#cb93-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-352"><a href="#cb93-352" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-confusion-matrix2.png)</span>{width="50%"}</span>
<span id="cb93-353"><a href="#cb93-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-354"><a href="#cb93-354" aria-hidden="true" tabindex="-1"></a>Figure 4.7 illustrates the trade-off that results from modifying the threshold value for the posterior probability of default. Various error rates are shown as a function of the threshold value. How can we decide which threshold value is best? Such a decision must be based on *domain knowledge*, such as detailed information about the costs associated with default.</span>
<span id="cb93-355"><a href="#cb93-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-356"><a href="#cb93-356" aria-hidden="true" tabindex="-1"></a>Increasing threshold to say 80% causes:</span>
<span id="cb93-357"><a href="#cb93-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-358"><a href="#cb93-358" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sensitivity to DECREASE and specificity to INCREASE ([good source, for modelling too]([https://workshops.tidymodels.org/slides/intro-04-evaluating-models.html#/two-class-data)).**</span>
<span id="cb93-359"><a href="#cb93-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-360"><a href="#cb93-360" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This means higher proportion of true negatives, with the cost of lower proportion of true positives.</span>
<span id="cb93-361"><a href="#cb93-361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-362"><a href="#cb93-362" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Predict more as no (harder to predict as yes), so capture more true no's, but miss some of the yes's.</span>
<span id="cb93-363"><a href="#cb93-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-364"><a href="#cb93-364" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Just flip below: False positive rate decreases, but the false negative rate increases.</span>
<span id="cb93-365"><a href="#cb93-365" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-366"><a href="#cb93-366" aria-hidden="true" tabindex="-1"></a>Thus, decreasing the threshold:</span>
<span id="cb93-367"><a href="#cb93-367" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-368"><a href="#cb93-368" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Just flip above: Sensitivity increases and specificity decreases.</span>
<span id="cb93-369"><a href="#cb93-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-370"><a href="#cb93-370" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Error rates:</span>
<span id="cb93-371"><a href="#cb93-371" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-372"><a href="#cb93-372" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Decreases the false negative rate (error rate among defaulters), because more confident in the negatives.</span>
<span id="cb93-373"><a href="#cb93-373" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb93-374"><a href="#cb93-374" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>But increases the false positive rate (error rate among non-defaulters), because easier to be classified as yes.</span>
<span id="cb93-375"><a href="#cb93-375" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb93-376"><a href="#cb93-376" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Thus increasing the threshold **</span>
<span id="cb93-377"><a href="#cb93-377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-378"><a href="#cb93-378" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-comparing-thresholds.png)</span>{width="50%"}</span>
<span id="cb93-379"><a href="#cb93-379" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-380"><a href="#cb93-380" aria-hidden="true" tabindex="-1"></a>The *ROC curve* is a popular graphic for simultaneously displaying the two types of errors for all possible thresholds. The overall performance of a classifier, summarized over all possible thresholds, is given by the *area under the (ROC) curve* (AUC). An ideal ROC curve will hug the top left corner, so the larger the AUC the better the classifier. We expect a classifier that performs no better than chance to have an AUC of 0.5 (when evaluated on an independent test set not used in model training). ROC curves are useful for comparing different classifiers, since they take into account all possible thresholds. As we have seen above, varying the classifier threshold changes its true positive and false positive rate. These are also called the *sensitivity* and one minus the *specificity* of our classifier.</span>
<span id="cb93-381"><a href="#cb93-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-382"><a href="#cb93-382" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-roc-curve.png)</span>{width="50%"}</span>
<span id="cb93-383"><a href="#cb93-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-384"><a href="#cb93-384" aria-hidden="true" tabindex="-1"></a>Here is a summary of the terms in this section. To make the connection with the epidemiology literature, we think of "+" as the "disease" that we are trying to detect, and "−" as the "non-disease" state. To make the connection to the classical hypothesis testing literature, we think of "−" as the null hypothesis and "'+" as the alternative (non-null) hypothesis. In the context of the Default data, "+" indicates an individual who defaults, and "−" indicates one who does not.</span>
<span id="cb93-385"><a href="#cb93-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-386"><a href="#cb93-386" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-definitions.png)</span>{width="50%"}</span>
<span id="cb93-387"><a href="#cb93-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-388"><a href="#cb93-388" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Sensitivity = true positive rate</span>
<span id="cb93-389"><a href="#cb93-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-390"><a href="#cb93-390" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Specificity = true negative rate</span>
<span id="cb93-391"><a href="#cb93-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-392"><a href="#cb93-392" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Quadratic discriminant analysis</span></span>
<span id="cb93-393"><a href="#cb93-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-394"><a href="#cb93-394" aria-hidden="true" tabindex="-1"></a>As we have discussed, LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a covariance matrix that is common to all $K$ classes. *Quadratic discriminant analysis* (QDA) provides an alternative approach. Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction. However, unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the kth class is of the form $X \sim N(\mu_k,\Sigma_k)$, where $\Sigma_k$ is a covariance matrix for the $k$th class. Under this assumption, the Bayes classifier assigns an observation $X = x$ to the class for which</span>
<span id="cb93-395"><a href="#cb93-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-396"><a href="#cb93-396" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-qda-formula.png)</span>{width="50%"}</span>
<span id="cb93-397"><a href="#cb93-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-398"><a href="#cb93-398" aria-hidden="true" tabindex="-1"></a>is the largest. Unlike before, the quantity $x$ appears as a quadratic function now.</span>
<span id="cb93-399"><a href="#cb93-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-400"><a href="#cb93-400" aria-hidden="true" tabindex="-1"></a>Why does it matter whether or not we assume that the K classes share a common covariance matrix? In other words, why would one prefer LDA to QDA, or vice-versa? The answer lies in the bias-variance trade-off. When there are $p$ predictors, then estimating a covariance matrix requires estimating $p(p+1)/2$ parameters (lower triangle + diagonal). QDA estimates a separate covariance matrix for each class, for a total of $Kp(p+1)/2$ parameters. With 50 predictors this is some multiple of 1,275, which is a lot of parameters. By instead assuming that the $K$ classes share a common covariance matrix, the LDA model becomes linear in $x$, which means there are $Kp$ linear coefficients to estimate. Consequently, LDA is a much less flexible classifier than QDA, and so has substantially lower variance. This can potentially lead to improved prediction performance. But there is a trade-off: if LDA’s assumption that the $K$ classes share a common covariance matrix is badly off, then LDA can suffer from high bias. **Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the $K$ classes is clearly untenable.**</span>
<span id="cb93-401"><a href="#cb93-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-402"><a href="#cb93-402" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ??? how to estimate sigma for LDA? why p parameters --&gt;</span></span>
<span id="cb93-403"><a href="#cb93-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-404"><a href="#cb93-404" aria-hidden="true" tabindex="-1"></a>Figure 4.9 illustrates the performances of LDA and QDA in two scenarios. In the left-hand panel, the two Gaussian classes have a common correlation of 0.7 between $X_1$ and $X_2$. As a result, the Bayes decision boundary is linear and is accurately approximated by the LDA decision boundary. The QDA decision boundary is inferior, because it suffers from higher variance without a corresponding decrease in bias. In contrast, the right-hand panel displays a situation in which the orange class has a correlation of 0.7 between the variables and the blue class has a correlation of −0.7. Now the Bayes decision boundary is quadratic, and so QDA more accurately approximates this boundary than does LDA.</span>
<span id="cb93-405"><a href="#cb93-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-406"><a href="#cb93-406" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-qda-example.png)</span>{width="50%"}</span>
<span id="cb93-407"><a href="#cb93-407" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-408"><a href="#cb93-408" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Naive Bayes</span></span>
<span id="cb93-409"><a href="#cb93-409" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-410"><a href="#cb93-410" aria-hidden="true" tabindex="-1"></a>Here, we use Bayes’ theorem to motivate the popular naive Bayes classifier. Recall that Bayes’ theorem @eq-bayes-theorem provides an expression for the posterior probability $p_k(x) = P(Y = k \mid X = x)$ in terms of $\pi_1, \ldots, \pi_K$ and $f_k(x), \ldots, f_K(x)$. To use this in practice, we need estimates for for these terms. Estimating the prior probabilities $\pi_k$ is typically straightforward.</span>
<span id="cb93-411"><a href="#cb93-411" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-412"><a href="#cb93-412" aria-hidden="true" tabindex="-1"></a>However estimating $f_k(x)$ is more subtle. Recall that $f_k(x)$ is the $p$-dimensional density function for an observation in the $k$th class, for $k = 1, \ldots, K$. In general, estimating a $p$-dimensional density function is challenging. In LDA, we make a very strong assumption that greatly simplifies the task: we assume that $f_k$ is the density function for a multivariate normal random variable with class-specific mean $\mu_k$, and shared covariance matrix $\Sigma$. By contrast, in QDA we change it to have class-specific covariance matrix $\Sigma_k$. By making these very strong assumptions, we are able to replace the very challenging problem of estimating $K$ $p$-dimensional density functions with the much simpler problem of estimating $K$ $p$-dimensional mean vectors and one (in the case of LDA) or $K$ (in the case of QDA) $(p \times p)$-dimensional covariance matrices.</span>
<span id="cb93-413"><a href="#cb93-413" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-414"><a href="#cb93-414" aria-hidden="true" tabindex="-1"></a>The naive Bayes classifier takes a different tack for estimating $f_1(x), \ldots, f_K(x)$. Instead of assuming that these functions belong to a particular family of distributions (e.g. multivariate normal), we instead make a single assumption: </span>
<span id="cb93-415"><a href="#cb93-415" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-416"><a href="#cb93-416" aria-hidden="true" tabindex="-1"></a>**Within the kth class, the $p$ predictors are independent.**</span>
<span id="cb93-417"><a href="#cb93-417" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-418"><a href="#cb93-418" aria-hidden="true" tabindex="-1"></a>Stated mathematically, this assumption means that for $k = 1, \ldots, K$, </span>
<span id="cb93-419"><a href="#cb93-419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-420"><a href="#cb93-420" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-421"><a href="#cb93-421" aria-hidden="true" tabindex="-1"></a>f_k(x) = f_{k1}(x_1) \times f_{k2}(x_2) \times \cdots \times f_{kp}(x_p)</span>
<span id="cb93-422"><a href="#cb93-422" aria-hidden="true" tabindex="-1"></a>$${#eq-naive-bayes-assumption}</span>
<span id="cb93-423"><a href="#cb93-423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-424"><a href="#cb93-424" aria-hidden="true" tabindex="-1"></a>where $f_{kj}$ is the density function of the $j$th predictor among observations in the $k$th class.</span>
<span id="cb93-425"><a href="#cb93-425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-426"><a href="#cb93-426" aria-hidden="true" tabindex="-1"></a>Why is this assumption so powerful? Essentially, estimating a $p$-dimensional density function is challenging because we must consider not only the marginal distribution of each predictor -- that is, the distribution of each predictor on its own -- but also the *joint distribution* of the predictors -- that is, the association between the different predictors. In the case of a multivariate normal distribution, the association between the different predictors is summarized by the off-diagonal elements of the covariance matrix. However, in general, this association can be very hard to characterize, and exceedingly challenging to estimate. But by assuming that the $p$ covariates are independent within each class, we completely eliminate the need to worry about the association between the $p$ predictors, because we have simply assumed that there is no association between the predictors!</span>
<span id="cb93-427"><a href="#cb93-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-428"><a href="#cb93-428" aria-hidden="true" tabindex="-1"></a>Do we really believe the naive Bayes assumption that the $p$ covariates are independent within each class? In most settings, we do not. **But even though this modeling assumption is made for convenience, it often leads to pretty decent results, especially in settings where $n$ is not large enough relative to $p$ for us to effectively estimate the joint distribution of the predictors within each class. In fact, since estimating a joint distribution requires such a huge amount of data, naive Bayes is a good choice in a wide range of settings.** Essentially, the naive Bayes assumption introduces some bias, but reduces variance, leading to a classifier that works quite well in practice as a result of the bias-variance trade-off.</span>
<span id="cb93-429"><a href="#cb93-429" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-430"><a href="#cb93-430" aria-hidden="true" tabindex="-1"></a>Once we have made the naive Bayes assumption, we can plug @eq-naive-bayes-assumption into @eq-bayes-theorem to obtain an expression for the posterior probability,</span>
<span id="cb93-431"><a href="#cb93-431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-432"><a href="#cb93-432" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-433"><a href="#cb93-433" aria-hidden="true" tabindex="-1"></a>P(Y = k \mid X = x) = \frac{\pi_k \times f_{k1}(x_1) \times \cdots \times f_{kp}(x_p)}{\sum_{l = 1}^K \pi_l \times f_{l1}(x_1) \times \cdots \times f_{lp}(x_p)}</span>
<span id="cb93-434"><a href="#cb93-434" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-435"><a href="#cb93-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-436"><a href="#cb93-436" aria-hidden="true" tabindex="-1"></a>for $k = 1, \ldots, K$.</span>
<span id="cb93-437"><a href="#cb93-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-438"><a href="#cb93-438" aria-hidden="true" tabindex="-1"></a>To estimate the one-dimensional density function $f_{kj}$ using training data</span>
<span id="cb93-439"><a href="#cb93-439" aria-hidden="true" tabindex="-1"></a>$x_{1j}, \cdots, x_{nj}$, we have a few options.</span>
<span id="cb93-440"><a href="#cb93-440" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-441"><a href="#cb93-441" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $X_j$ is quantitative, then we can assume that $X_j \mid Y = k \sim N(\mu_{jk}, \sigma_{jk}^2)$. In other words, we assume that within each class, the $j$th predictor is drawn from a (univariate) normal distribution. While this may sound a bit like QDA, there is one key difference, in that here we are assuming that the predictors are independent; this amounts to QDA with an additional assumption that the class-specific covariance matrix is diagonal.</span>
<span id="cb93-442"><a href="#cb93-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-443"><a href="#cb93-443" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $X_j$ is quantitative, then another option is to use a non-parametric estimate for $f_{kj}$. A very simple way to do this is by making a histogram for the observations of the $j$th predictor within each class. Then we can estimate $f_{kj}(x_j)$ as the fraction of the training observations in the $k$th class that belong to the same histogram bin as $x_j$. Alternatively, we can use a kernel density estimator, which is essentially a smoothed version of a histogram.</span>
<span id="cb93-444"><a href="#cb93-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-445"><a href="#cb93-445" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If $X_j$ is qualitative, then we can simply count the proportion of training observations for the $j$th predictor corresponding to each class. For instance, suppose that $X_j \in \{1, 2, 3\}$, and we have 100 observations in the $k$th class. Suppose that the $j$th predictor takes on values of 1, 2, and 3 in 32, 55, and 13 of those observations, respectively. Then we can estimate $f_{kj}$ as</span>
<span id="cb93-446"><a href="#cb93-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-447"><a href="#cb93-447" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-448"><a href="#cb93-448" aria-hidden="true" tabindex="-1"></a>\hat{f}_{kj}(x_j) = </span>
<span id="cb93-449"><a href="#cb93-449" aria-hidden="true" tabindex="-1"></a>\begin{cases}</span>
<span id="cb93-450"><a href="#cb93-450" aria-hidden="true" tabindex="-1"></a>0.32 &amp; \text{if } x_j = 1<span class="sc">\\</span></span>
<span id="cb93-451"><a href="#cb93-451" aria-hidden="true" tabindex="-1"></a>0.55 &amp; \text{if } x_j = 2<span class="sc">\\</span></span>
<span id="cb93-452"><a href="#cb93-452" aria-hidden="true" tabindex="-1"></a>0.13 &amp; \text{if } x_j = 3<span class="sc">\\</span></span>
<span id="cb93-453"><a href="#cb93-453" aria-hidden="true" tabindex="-1"></a>\end{cases}</span>
<span id="cb93-454"><a href="#cb93-454" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-455"><a href="#cb93-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-456"><a href="#cb93-456" aria-hidden="true" tabindex="-1"></a>Just as with LDA, we can easily adjust the probability threshold for predicting a default. In this example, it should not be too surprising that naive Bayes does not convincingly outperform LDA: this data set has $n = 10,000$ and $p = 4$, and so the reduction in variance resulting from the naive Bayes assumption is not necessarily worthwhile. We expect to see a greater pay-off to using naive Bayes relative to LDA or QDA in instances where $p$ is larger or $n$ is smaller, so that reducing the variance is very important.</span>
<span id="cb93-457"><a href="#cb93-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-458"><a href="#cb93-458" aria-hidden="true" tabindex="-1"></a><span class="fu">### A comparison of classification methods</span></span>
<span id="cb93-459"><a href="#cb93-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-460"><a href="#cb93-460" aria-hidden="true" tabindex="-1"></a><span class="fu">#### An analytical comparison</span></span>
<span id="cb93-461"><a href="#cb93-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-462"><a href="#cb93-462" aria-hidden="true" tabindex="-1"></a>We now perform an *analytical* (or mathematical) comparison of LDA, QDA, naive Bayes, and logistic regression. We consider these approaches in a setting with K classes, so that we assign an observation to the class that maximizes $P(Y = k \mid X = x)$. Equivalently, we can set $K$ as the baseline class and assign an observation to the class that maximizes</span>
<span id="cb93-463"><a href="#cb93-463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-464"><a href="#cb93-464" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-465"><a href="#cb93-465" aria-hidden="true" tabindex="-1"></a>\log\Big(\frac{P(Y = k \mid X = x)}{P(Y = K \mid X = x)}\Big)</span>
<span id="cb93-466"><a href="#cb93-466" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-467"><a href="#cb93-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-468"><a href="#cb93-468" aria-hidden="true" tabindex="-1"></a>for $k = 1, \ldots, K$. Examining the specific form of this equation for each method provides a clear understanding of their similarities and differences.</span>
<span id="cb93-469"><a href="#cb93-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-470"><a href="#cb93-470" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-lda-derivation.png)</span>{width="50%"}</span>
<span id="cb93-471"><a href="#cb93-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-472"><a href="#cb93-472" aria-hidden="true" tabindex="-1"></a>where $a_k = \log\Big(\frac{\pi_k}{\pi_K}\Big) - \frac{1}{2}(\mu_k + \mu_K)^T \Sigma^{-1}(\mu_k - \mu_K)$ and $b_{kj}$ is the $j$th component of $\Sigma^{-1}(\mu_k - \mu_K)$. **Hence LDA, like logistic regression, assumes that the log odds of the posterior probabilities is linear in $x$.**</span>
<span id="cb93-473"><a href="#cb93-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-474"><a href="#cb93-474" aria-hidden="true" tabindex="-1"></a>Using similar calculations, in the QDA setting this becomes</span>
<span id="cb93-475"><a href="#cb93-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-476"><a href="#cb93-476" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-qda-derivation.png)</span>{width="50%"}</span>
<span id="cb93-477"><a href="#cb93-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-478"><a href="#cb93-478" aria-hidden="true" tabindex="-1"></a>Again, as the name suggests, QDA assumes that the log odds of the posterior probabilities is quadratic in $x$.</span>
<span id="cb93-479"><a href="#cb93-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-480"><a href="#cb93-480" aria-hidden="true" tabindex="-1"></a>Finally with naive Bayes setting, we get</span>
<span id="cb93-481"><a href="#cb93-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-482"><a href="#cb93-482" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-naive-bayes-derivation.png)</span>{width="50%"}</span>
<span id="cb93-483"><a href="#cb93-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-484"><a href="#cb93-484" aria-hidden="true" tabindex="-1"></a>where $a_k = \log\big(\frac{\pi_k}{\pi_K}\big)$ and $g_{kj}(x_j) = \log\big(\frac{f_{kj}(x_j)}{f_{Kj}(x_j)}\big)$. Hence, the right-hand side of above takes the form of a *generalized additive model*, which will be discussed later.</span>
<span id="cb93-485"><a href="#cb93-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-486"><a href="#cb93-486" aria-hidden="true" tabindex="-1"></a>Inspection of the above results yields the following observations about LDA, QDA, and naive Bayes:</span>
<span id="cb93-487"><a href="#cb93-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-488"><a href="#cb93-488" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>LDA is a special case of QDA with $c_{kjl} = 0$ for all $j = 1, \ldots, p, l = 1, \ldots, p$, and $k = 1, \ldots, K$. (Of course, this is not surprising, since LDA is simply a restricted version of QDA with $\Sigma_1 = \cdots = \Sigma_K = \Sigma$.)</span>
<span id="cb93-489"><a href="#cb93-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-490"><a href="#cb93-490" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Any classifier with a linear decision boundary is a special case of naive Bayes with $g_{kj}(x_j) = b_{kj}x_j$. In particular, this means that LDA is a special case of naive Bayes! This is not at all obvious from the descriptions of LDA and naive Bayes earlier in the chapter, since each method makes very different assumptions: LDA assumes that the features are normally distributed with a common within-class covariance matrix, and naive Bayes instead assumes independence of the features.</span>
<span id="cb93-491"><a href="#cb93-491" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-492"><a href="#cb93-492" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>If we model $f_{kj}(x_j)$ in the naive Bayes classifier using a one-dimensional Gaussian distribution $N(\mu_{kj}, \sigma_j^2)$, then we end up with $g_{kj}(x_j) = b_{kj}x_j$ where $b_{kj} = (\mu_{kj} - \mu_{Kj}) / \sigma_j^2$. In this case, naive Bayes is actually a special case of LDA with $\Sigma$ restricted to be a diagonal matrix with $j$th diagonal element equal to $\sigma_j^2$.</span>
<span id="cb93-493"><a href="#cb93-493" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-494"><a href="#cb93-494" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Neither QDA nor naive Bayes is a special case of the other. Naive Bayes can produce a more flexible fit, since any choice can be made for $g_{kj}(x_j)$. However, it is restricted to a purely additive fit, in the sense that in the final derivation, a function of $x_j$ is added to a function of $x$, for $j \ne l$; however, these terms are never multiplied. By contrast, QDA includes multiplicative terms of the form $c_{kjl} x_j x_l$. **Therefore, QDA has the potential to be more accurate in settings where interactions among the predictors are important in discriminating between classes.**</span>
<span id="cb93-495"><a href="#cb93-495" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-496"><a href="#cb93-496" aria-hidden="true" tabindex="-1"></a>None of these methods uniformly dominates the others: in any setting, the choice of method will depend on the true distribution of the predictors in each of the $K$ classes, as well as other considerations, such as the values of $n$ and $p$. The latter ties into the bias-variance trade-off.</span>
<span id="cb93-497"><a href="#cb93-497" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-498"><a href="#cb93-498" aria-hidden="true" tabindex="-1"></a>How does logistic regression tie into this story? Recall from @eq-multinomial-logreg that multinomial logistic regression takes the form</span>
<span id="cb93-499"><a href="#cb93-499" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-500"><a href="#cb93-500" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-501"><a href="#cb93-501" aria-hidden="true" tabindex="-1"></a>\log\Big(\frac{P(Y = k \mid X = x)}{P(Y = K \mid X = x)}\Big) = \beta_{k0} + \sum_{j=1}^p \beta_{kj} x_j</span>
<span id="cb93-502"><a href="#cb93-502" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-503"><a href="#cb93-503" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-504"><a href="#cb93-504" aria-hidden="true" tabindex="-1"></a>This is identical to the *linear form* of LDA the derivation: in both cases $\log\Big(\frac{P(Y = k \mid X = x)}{P(Y = K \mid X = x)}\Big) $ is a linear function of the predictors. In LDA, the coefficients in this linear function are functions of estimates for $\pi_k$, $\pi_K$, $\mu_k$, $\mu_K$, and $\Sigma$ obtained by assuming that $X_1,\ldots,X_p$ follow a normal distribution within each class. By contrast, in logistic regression, the coefficients are chosen to maximize the likelihood function @eq-logistic-likelihood. **Thus, we expect LDA to outperform logistic regression when the normality assumption (approximately) holds, and we expect logistic regression to perform better when it does not.**</span>
<span id="cb93-505"><a href="#cb93-505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-506"><a href="#cb93-506" aria-hidden="true" tabindex="-1"></a>We close with a brief discussion of $K$-nearest neighbors (KNN), introduced in earlier. Recall that KNN takes a completely different approach from the classifiers seen in this chapter. In order to make a prediction for an observation $X = x$, the training observations that are closest to $x$ are identified. Then $X$ is assigned to the class to which the plurality of these observations belong. Hence KNN is a completely non-parametric approach: no assumptions are made about the shape of the decision boundary. We make the following observations about KNN:</span>
<span id="cb93-507"><a href="#cb93-507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-508"><a href="#cb93-508" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Because KNN is completely non-parametric, we can expect this ap- proach to dominate LDA and logistic regression when the decision boundary is highly non-linear, provided that n is very large and p is small.**</span>
<span id="cb93-509"><a href="#cb93-509" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-510"><a href="#cb93-510" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**In order to provide accurate classification, KNN requires a lot of observations relative to the number of predictors** -- that is, $n$ much larger than $p$. This has to do with the fact that KNN is non-parametric, and thus tends to reduce the bias while incurring a lot of variance.</span>
<span id="cb93-511"><a href="#cb93-511" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-512"><a href="#cb93-512" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**In settings where the decision boundary is non-linear but $n$ is only modest, or $p$ is not very small, then QDA may be preferred to KNN**. This is because QDA can provide a non-linear decision boundary while taking advantage of a parametric form, which means that it requires a smaller sample size for accurate classification, relative to KNN.</span>
<span id="cb93-513"><a href="#cb93-513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-514"><a href="#cb93-514" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Note that the of $K$ in KNN is really important and is often chosen via *cross-validation*.</span>
<span id="cb93-515"><a href="#cb93-515" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-516"><a href="#cb93-516" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Unlike logistic regression, KNN does not tell us which predictors are important: we don’t get a table of coefficients as in Table 4.3.</span>
<span id="cb93-517"><a href="#cb93-517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-518"><a href="#cb93-518" aria-hidden="true" tabindex="-1"></a><span class="fu">#### An empirical comparison</span></span>
<span id="cb93-519"><a href="#cb93-519" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-520"><a href="#cb93-520" aria-hidden="true" tabindex="-1"></a>Checkout section 4.5.2 of textbook for some simulation results. This could help when determining which model to use for a particular situation.</span>
<span id="cb93-521"><a href="#cb93-521" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-522"><a href="#cb93-522" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**When the true decision boundaries are linear, then the LDA and logistic regression approaches will tend to perform well.**</span>
<span id="cb93-523"><a href="#cb93-523" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-524"><a href="#cb93-524" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**When the boundaries are moderately non-linear, QDA or naive Bayes may give better results.**</span>
<span id="cb93-525"><a href="#cb93-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-526"><a href="#cb93-526" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Finally, for much more complicated decision boundaries, a non-parametric approach such as KNN can be superior. But the level of smoothness for a non-parametric approach must be chosen carefully.**</span>
<span id="cb93-527"><a href="#cb93-527" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-528"><a href="#cb93-528" aria-hidden="true" tabindex="-1"></a>In the next chapter we examine a number of approaches for choosing the correct level of smoothness and, in general, for selecting the best overall method.</span>
<span id="cb93-529"><a href="#cb93-529" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-530"><a href="#cb93-530" aria-hidden="true" tabindex="-1"></a>Finally, recall from the previous chapter that in the regression setting we can accommodate a non-linear relationship between the predictors and the response by performing regression using transformations of the predictors. A similar approach could be taken in the classification setting. For instance, we could create a more flexible version of logistic regression by including $X^2$, $X^3$, and even $X^4$ as predictors. This may or may not improve logistic regression's performance, depending on whether the increase in variance due to the added flexibility is offset by a sufficiently large reduction in bias. We could do the same for LDA. If we added all possible quadratic terms and cross-products to LDA, the form of the model would be the same as the QDA model, although the parameter estimates would be different. This device allows us to move somewhere between an LDA and a QDA model.</span>
<span id="cb93-531"><a href="#cb93-531" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-532"><a href="#cb93-532" aria-hidden="true" tabindex="-1"></a><span class="fu">### Generalized linear models</span></span>
<span id="cb93-533"><a href="#cb93-533" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-534"><a href="#cb93-534" aria-hidden="true" tabindex="-1"></a>In the previous chapter, we assumed that the response $Y$ is quantitative, and explored the use of least squares linear regression to predict $Y$. Thus far in this chapter, we have instead assumed that $Y$ is qualitative. However, we may sometimes be faced with situations in which $Y$ is neither qualitative nor quantitative, and so neither linear regression nor the classification approaches  is applicable.</span>
<span id="cb93-535"><a href="#cb93-535" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-536"><a href="#cb93-536" aria-hidden="true" tabindex="-1"></a>This occurs when we are working with *count* data (non-negative integers). If we fit linear regression to count data, here are some issues that can occur:</span>
<span id="cb93-537"><a href="#cb93-537" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-538"><a href="#cb93-538" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Can predict negative counts. This calls into question our ability to perform meaningful predictions on the data, and it also raises concerns about the accuracy of the coefficient estimates, confidence intervals, and other outputs of the regression model.</span>
<span id="cb93-539"><a href="#cb93-539" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-540"><a href="#cb93-540" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Furthermore, it is reasonable to suspect that with the expected value of the response is small, the variance of the response should be small as well; however, when the expected value of counts is large, the variance should increase as well. This is a major violation of the assumptions of a linear model, which state that $Y = \sum_{j=1}^p X_j \beta_j + \epsilon$, where $\epsilon$ is a mean-zero error term with variance $\sigma^2$ that is constant, and not a function of the covariates. Therefore, the heteroscedasticity of the data calls into question the suitability of a linear regression model. This is bad:</span>
<span id="cb93-541"><a href="#cb93-541" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-542"><a href="#cb93-542" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-error-variance-violation.png)</span>{width="50%"}</span>
<span id="cb93-543"><a href="#cb93-543" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-544"><a href="#cb93-544" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Finally, with a continuous-valued error term, we can get a continuous-valued response, which doesn't match the scenario.</span>
<span id="cb93-545"><a href="#cb93-545" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-546"><a href="#cb93-546" aria-hidden="true" tabindex="-1"></a>Some of the problems that arise when fitting a linear regression model to count data can be overcome by transforming the response; for instance, we can fit the model</span>
<span id="cb93-547"><a href="#cb93-547" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-548"><a href="#cb93-548" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-549"><a href="#cb93-549" aria-hidden="true" tabindex="-1"></a>\log(Y) = \sum_{j=1}^p X_j \beta_j = \epsilon</span>
<span id="cb93-550"><a href="#cb93-550" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-551"><a href="#cb93-551" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-552"><a href="#cb93-552" aria-hidden="true" tabindex="-1"></a>Transforming the response avoids the possibility of negative predictions, and it overcomes much of the heteroscedasticity in the untransformed data, as is shown in the right-hand panel of Figure 4.14. However, it is not quite a satisfactory solution, since predictions and inference are made in terms of the log of the response, rather than the response. This leads to challenges in interpretation, e.g. "a one-unit increase in $X_j$ is associated with an increase in the mean of the log of $Y$ by an amount $\beta_j$". Furthermore, a log transformation of the response cannot be applied in settings where the response can take on a value of 0. Thus, while fitting a linear model to a transformation of the response may be an adequate approach for some count-valued data sets, it often leaves something to be desired. We will see in the next section that a Poisson regression model provides a much more natural and elegant approach for this task.</span>
<span id="cb93-553"><a href="#cb93-553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-554"><a href="#cb93-554" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Poisson regression</span></span>
<span id="cb93-555"><a href="#cb93-555" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-556"><a href="#cb93-556" aria-hidden="true" tabindex="-1"></a>To overcome the inadequacies of linear regression for analyzing count data, we will make use of an alternative approach, called *Poisson regression*. Recall if $Y \follow{Poisson}(\lambda)$, then </span>
<span id="cb93-557"><a href="#cb93-557" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-558"><a href="#cb93-558" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-559"><a href="#cb93-559" aria-hidden="true" tabindex="-1"></a>P(Y = k) = \frac{\e^{-\lambda}\lambda^k}{k!}, \quad \text{for } k = 1, 2, \ldots</span>
<span id="cb93-560"><a href="#cb93-560" aria-hidden="true" tabindex="-1"></a>$${#eq-poisson}</span>
<span id="cb93-561"><a href="#cb93-561" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-562"><a href="#cb93-562" aria-hidden="true" tabindex="-1"></a>Here, $\lambda &gt; 0$ is the expected value of $Y$, i.e. $E(Y)$. It turns out that $\lambda$ also equals the variance of $Y$ , i.e. $\lambda = E(Y) = V(Y)$. This means that if $Y$ follows the Poisson distribution, then the larger the mean of $Y$, the larger its variance. </span>
<span id="cb93-563"><a href="#cb93-563" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-564"><a href="#cb93-564" aria-hidden="true" tabindex="-1"></a>The Poisson distribution is typically used to model counts. To see how we</span>
<span id="cb93-565"><a href="#cb93-565" aria-hidden="true" tabindex="-1"></a>might use the Poisson distribution in practice, let $Y$ denote the number of</span>
<span id="cb93-566"><a href="#cb93-566" aria-hidden="true" tabindex="-1"></a>users of the bike sharing program during a particular hour of the day, under</span>
<span id="cb93-567"><a href="#cb93-567" aria-hidden="true" tabindex="-1"></a>a particular set of weather conditions, and during a particular month of the</span>
<span id="cb93-568"><a href="#cb93-568" aria-hidden="true" tabindex="-1"></a>year. We might model $Y$ as a Poisson distribution with mean $E(Y) = \lambda = 5$.</span>
<span id="cb93-569"><a href="#cb93-569" aria-hidden="true" tabindex="-1"></a>This means that the probability of no users during this particular hour is</span>
<span id="cb93-570"><a href="#cb93-570" aria-hidden="true" tabindex="-1"></a>$P(Y = 0) = \frac{\e^{-5} 5^0}{0!} = e^{-5} = 0.0067$. Of course, in reality, we expect the mean number of users of the bike sharing program, $\lambda = E(Y)$, to vary as a function of the hour of the day, the month of the year, the weather conditions, and so forth. So rather than modeling the number of bikers, $Y$, as a Poisson distribution with a *fixed* mean value like $\lambda = 5$, we would like to *allow the mean to vary as a function of the covariates*. In particular, we consider the following model for the mean $\lambda = E(Y)$, which we now write as $\lambda(X_1, \ldots, X_p)$ to emphasize that it is a function of the covariates $X_1, \ldots, X_p$:</span>
<span id="cb93-571"><a href="#cb93-571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-572"><a href="#cb93-572" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-573"><a href="#cb93-573" aria-hidden="true" tabindex="-1"></a>\log(\lambda(X_1, \ldots, X_p)) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p</span>
<span id="cb93-574"><a href="#cb93-574" aria-hidden="true" tabindex="-1"></a>$${#eq-log-lambda}</span>
<span id="cb93-575"><a href="#cb93-575" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-576"><a href="#cb93-576" aria-hidden="true" tabindex="-1"></a>or equivalently</span>
<span id="cb93-577"><a href="#cb93-577" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-578"><a href="#cb93-578" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-579"><a href="#cb93-579" aria-hidden="true" tabindex="-1"></a>\lambda(X_1, \ldots, X_p) = \e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}</span>
<span id="cb93-580"><a href="#cb93-580" aria-hidden="true" tabindex="-1"></a>$${#eq-lambda}</span>
<span id="cb93-581"><a href="#cb93-581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-582"><a href="#cb93-582" aria-hidden="true" tabindex="-1"></a>Here, $\beta_0, \beta_1, \ldots, \beta_p$ are parameters to be estimated. Together, @eq-poisson and @eq-log-lambda define the Poisson regression model. Notice that in @eq-log-lambda, we take the *log* of $\lambda(X_1, \ldots, X_p)$ to be linear in $X_1, \ldots, X_p$, in order to ensure that $\lambda(X_1, \ldots, X_p)$ takes on nonnegative values for all values of the covariates.</span>
<span id="cb93-583"><a href="#cb93-583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-584"><a href="#cb93-584" aria-hidden="true" tabindex="-1"></a>To estimate the coefficients $\beta_0, \beta_1, \ldots, \beta_p$, we use the use the same maximum likelihood approach that we adopted for logistic regression. Specifically, given $n$ independent observations from the Poisson regression model, the likelihood takes the form</span>
<span id="cb93-585"><a href="#cb93-585" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-586"><a href="#cb93-586" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-587"><a href="#cb93-587" aria-hidden="true" tabindex="-1"></a>\ell(\beta_0, \beta_1, \ldots, \beta_p) = \prod_{i = 1}^n \frac{e^{-\lambda(x_i)} \lambda(x_i)^{y_i}}{y_i!}</span>
<span id="cb93-588"><a href="#cb93-588" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-589"><a href="#cb93-589" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-590"><a href="#cb93-590" aria-hidden="true" tabindex="-1"></a>where $\lambda(x_i) = \e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}$ due to @eq-lambda. We estimate the coefficients that maximize the likelihood $\ell(\beta_0, \beta_1, \ldots, \beta_p)$, i.e. that make the observed data as likely as possible.</span>
<span id="cb93-591"><a href="#cb93-591" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-592"><a href="#cb93-592" aria-hidden="true" tabindex="-1"></a>Some important distinctions between the Poisson regression model and the linear regression model are as follows:</span>
<span id="cb93-593"><a href="#cb93-593" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-594"><a href="#cb93-594" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*Interpretation*: To interpret the coefficients in the Poisson regression model, we must pay close attention to @eq-lambda, which states that an increase in $X_j$ by one unit is associated with a change in $E(Y) = \lambda$ by a factor of $\e^{\beta_j}$. For example, a change in weather from clear to cloudy skies is associated with a change in mean bike usage by a factor of $\exp(−0.08) = 0.923$, i.e. on average, only 92.3% as many people will use bikes when it is cloudy relative to when it is clear.</span>
<span id="cb93-595"><a href="#cb93-595" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-596"><a href="#cb93-596" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*Mean-variance relationship*: As mentioned earlier, under the Poisson model, $\lambda = E(Y) = V(Y)$. Thus, by modeling bike usage with a Poisson regression, we implicitly assume that mean bike usage in a given hour equals the variance of bike usage during that hour. By contrast, under a linear regression model, the variance of bike usage always takes on a constant value.</span>
<span id="cb93-597"><a href="#cb93-597" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-598"><a href="#cb93-598" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>*nonnegative fitted values*: There are no negative predictions using the Poisson regression model.</span>
<span id="cb93-599"><a href="#cb93-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-600"><a href="#cb93-600" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Generalized linear models in greater generality</span></span>
<span id="cb93-601"><a href="#cb93-601" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-602"><a href="#cb93-602" aria-hidden="true" tabindex="-1"></a>We have now discussed three types of regression models: linear, logistic and Poisson. These approaches share some common characteristics:</span>
<span id="cb93-603"><a href="#cb93-603" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-604"><a href="#cb93-604" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each approach uses predictors $X_1, \ldots, X_p$ to predict a response $Y$. We assume that, conditional on $X_1, \ldots, X_p$, $Y$ belongs to a certain family of distributions. For linear regression, we typically assume that $Y$ follows a Gaussian or normal distribution. For logistic regression, we assume that $Y$ follows a Bernoulli distribution. Finally, for Poisson regression, we assume that $Y$ follows a Poisson distribution.</span>
<span id="cb93-605"><a href="#cb93-605" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-606"><a href="#cb93-606" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Each approach models the mean of $Y$ as a function of the predictors. In linear regression, the mean of $Y$ takes the form</span>
<span id="cb93-607"><a href="#cb93-607" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-608"><a href="#cb93-608" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-609"><a href="#cb93-609" aria-hidden="true" tabindex="-1"></a>E(Y \mid X_1, \ldots, X_p) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p</span>
<span id="cb93-610"><a href="#cb93-610" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-611"><a href="#cb93-611" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-612"><a href="#cb93-612" aria-hidden="true" tabindex="-1"></a>i.e., it is a linear function of the predictors. For logistic regression, the mean instead takes the form </span>
<span id="cb93-613"><a href="#cb93-613" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-614"><a href="#cb93-614" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-615"><a href="#cb93-615" aria-hidden="true" tabindex="-1"></a>\begin{align*}</span>
<span id="cb93-616"><a href="#cb93-616" aria-hidden="true" tabindex="-1"></a>E(Y \mid X_1, \ldots, X_p) &amp;= P(Y = 1 \mid X_1, \ldots, X_p)<span class="sc">\\</span></span>
<span id="cb93-617"><a href="#cb93-617" aria-hidden="true" tabindex="-1"></a> &amp;= \frac{\e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}{1 + \e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}}</span>
<span id="cb93-618"><a href="#cb93-618" aria-hidden="true" tabindex="-1"></a>\end{align*}</span>
<span id="cb93-619"><a href="#cb93-619" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-620"><a href="#cb93-620" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-621"><a href="#cb93-621" aria-hidden="true" tabindex="-1"></a>(this is because mean(Bernoulli) = $p$ = ... &lt; logistic function &gt;) while for Poisson regression it takes the form</span>
<span id="cb93-622"><a href="#cb93-622" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-623"><a href="#cb93-623" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-624"><a href="#cb93-624" aria-hidden="true" tabindex="-1"></a>E(Y \mid X_1, \ldots, X_p) = \lambda(X_1, \ldots, X_p) = \e^{\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p}</span>
<span id="cb93-625"><a href="#cb93-625" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-626"><a href="#cb93-626" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-627"><a href="#cb93-627" aria-hidden="true" tabindex="-1"></a>All of these above equations can be expressed using a *link function*, $\eta$, which applies a transformation to $E(Y \mid X_1, \ldots, X_p) $ so that the transformed mean is a linear function of the predictors. That is,</span>
<span id="cb93-628"><a href="#cb93-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-629"><a href="#cb93-629" aria-hidden="true" tabindex="-1"></a>$$</span>
<span id="cb93-630"><a href="#cb93-630" aria-hidden="true" tabindex="-1"></a>\eta(E(Y \mid X_1, \ldots, X_p)) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p</span>
<span id="cb93-631"><a href="#cb93-631" aria-hidden="true" tabindex="-1"></a>$${#eq-link-function}</span>
<span id="cb93-632"><a href="#cb93-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-633"><a href="#cb93-633" aria-hidden="true" tabindex="-1"></a>The link functions for linear, logistic and Poisson regression are $\eta(\mu) = \mu$, $\eta(\mu) = \log(\mu/(1 − \mu))$, and $\eta(\mu) = \log(\mu)$, respectively.</span>
<span id="cb93-634"><a href="#cb93-634" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-635"><a href="#cb93-635" aria-hidden="true" tabindex="-1"></a>The Gaussian, Bernoulli and Poisson distributions are all members of a wider class of distributions, known as the *exponential family*. Other well-known members of this family are the exponential distribution, the Gamma distribution, and the negative binomial distribution. In general, we can perform a regression by modeling the response Y as coming from a particular member of the exponential family, and then transforming the mean of the response so that the transformed mean is a linear function of the predictors via @eq-link-function. Any regression approach that follows this very general recipe is known as a generalized linear model (GLM). Thus, linear regression, logistic regression, and Poisson regression are three examples of GLMs. Other examples not covered here include Gamma regression and negative binomial regression.</span>
<span id="cb93-636"><a href="#cb93-636" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-637"><a href="#cb93-637" aria-hidden="true" tabindex="-1"></a><span class="fu">## Lab</span></span>
<span id="cb93-638"><a href="#cb93-638" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-639"><a href="#cb93-639" aria-hidden="true" tabindex="-1"></a><span class="fu">### Load data</span></span>
<span id="cb93-640"><a href="#cb93-640" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-643"><a href="#cb93-643" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-644"><a href="#cb93-644" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb93-645"><a href="#cb93-645" aria-hidden="true" tabindex="-1"></a>data_stock <span class="ot">&lt;-</span> ISLR2<span class="sc">::</span>Smarket</span>
<span id="cb93-646"><a href="#cb93-646" aria-hidden="true" tabindex="-1"></a><span class="fu">colnames</span>(data_stock)</span>
<span id="cb93-647"><a href="#cb93-647" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-648"><a href="#cb93-648" aria-hidden="true" tabindex="-1"></a><span class="co"># view correlations</span></span>
<span id="cb93-649"><a href="#cb93-649" aria-hidden="true" tabindex="-1"></a>data_stock <span class="sc">%&gt;%</span> </span>
<span id="cb93-650"><a href="#cb93-650" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span> </span>
<span id="cb93-651"><a href="#cb93-651" aria-hidden="true" tabindex="-1"></a>  as.matrix <span class="sc">%&gt;%</span> </span>
<span id="cb93-652"><a href="#cb93-652" aria-hidden="true" tabindex="-1"></a>  cor <span class="sc">%&gt;%</span> </span>
<span id="cb93-653"><a href="#cb93-653" aria-hidden="true" tabindex="-1"></a>  corrplot<span class="sc">::</span><span class="fu">corrplot</span>()</span>
<span id="cb93-654"><a href="#cb93-654" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-655"><a href="#cb93-655" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-656"><a href="#cb93-656" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-657"><a href="#cb93-657" aria-hidden="true" tabindex="-1"></a><span class="fu">### Logisitic regression</span></span>
<span id="cb93-658"><a href="#cb93-658" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-659"><a href="#cb93-659" aria-hidden="true" tabindex="-1"></a>Lets fit the full model and view some model summaries.</span>
<span id="cb93-660"><a href="#cb93-660" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-663"><a href="#cb93-663" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-664"><a href="#cb93-664" aria-hidden="true" tabindex="-1"></a><span class="co"># load packages</span></span>
<span id="cb93-665"><a href="#cb93-665" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(magrittr)</span>
<span id="cb93-666"><a href="#cb93-666" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-667"><a href="#cb93-667" aria-hidden="true" tabindex="-1"></a><span class="co"># fit logistic regression model</span></span>
<span id="cb93-668"><a href="#cb93-668" aria-hidden="true" tabindex="-1"></a>mod_logreg <span class="ot">&lt;-</span> <span class="fu">glm</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2 <span class="sc">+</span> Lag3 <span class="sc">+</span> Lag4 <span class="sc">+</span> Lag5 <span class="sc">+</span> Volume,</span>
<span id="cb93-669"><a href="#cb93-669" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> data_stock,</span>
<span id="cb93-670"><a href="#cb93-670" aria-hidden="true" tabindex="-1"></a>                  <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb93-671"><a href="#cb93-671" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-672"><a href="#cb93-672" aria-hidden="true" tabindex="-1"></a><span class="co"># view model summary</span></span>
<span id="cb93-673"><a href="#cb93-673" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_logreg)</span>
<span id="cb93-674"><a href="#cb93-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-675"><a href="#cb93-675" aria-hidden="true" tabindex="-1"></a><span class="co"># view coding of response</span></span>
<span id="cb93-676"><a href="#cb93-676" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; thus model is predicting P(Direction = Up | X = x)</span></span>
<span id="cb93-677"><a href="#cb93-677" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(data_stock<span class="sc">$</span>Direction)</span>
<span id="cb93-678"><a href="#cb93-678" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-679"><a href="#cb93-679" aria-hidden="true" tabindex="-1"></a><span class="co"># get model fits for probabilities</span></span>
<span id="cb93-680"><a href="#cb93-680" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; base R</span></span>
<span id="cb93-681"><a href="#cb93-681" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; to get the fitted log odds, use type = "link"</span></span>
<span id="cb93-682"><a href="#cb93-682" aria-hidden="true" tabindex="-1"></a>preds_logreg <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod_logreg, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb93-683"><a href="#cb93-683" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; broom</span></span>
<span id="cb93-684"><a href="#cb93-684" aria-hidden="true" tabindex="-1"></a><span class="co"># --&gt; uses the same arguments as type for .fitted</span></span>
<span id="cb93-685"><a href="#cb93-685" aria-hidden="true" tabindex="-1"></a><span class="co"># --&gt; then classify</span></span>
<span id="cb93-686"><a href="#cb93-686" aria-hidden="true" tabindex="-1"></a>preds_logreg <span class="ot">&lt;-</span> broom<span class="sc">::</span><span class="fu">augment</span>(mod_logreg, <span class="at">type.predict =</span> <span class="st">"response"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb93-687"><a href="#cb93-687" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Direction, .fitted) <span class="sc">%&gt;%</span> </span>
<span id="cb93-688"><a href="#cb93-688" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">predicted =</span> <span class="fu">if_else</span>(.fitted <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">"Up"</span>, <span class="st">"Down"</span>) <span class="sc">%&gt;%</span> <span class="fu">factor</span>(<span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"Down"</span>, <span class="st">"Up"</span>)))</span>
<span id="cb93-689"><a href="#cb93-689" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-690"><a href="#cb93-690" aria-hidden="true" tabindex="-1"></a><span class="co"># create confusion matrix</span></span>
<span id="cb93-691"><a href="#cb93-691" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; base R</span></span>
<span id="cb93-692"><a href="#cb93-692" aria-hidden="true" tabindex="-1"></a>preds_logreg <span class="sc">%$%</span> <span class="fu">table</span>(predicted, </span>
<span id="cb93-693"><a href="#cb93-693" aria-hidden="true" tabindex="-1"></a>                     Direction,</span>
<span id="cb93-694"><a href="#cb93-694" aria-hidden="true" tabindex="-1"></a>                     <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">"predicted"</span>, <span class="st">"actual"</span>))</span>
<span id="cb93-695"><a href="#cb93-695" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; tidymodels</span></span>
<span id="cb93-696"><a href="#cb93-696" aria-hidden="true" tabindex="-1"></a>(c_mat <span class="ot">&lt;-</span> yardstick<span class="sc">::</span><span class="fu">conf_mat</span>(preds_logreg <span class="sc">%&gt;%</span> </span>
<span id="cb93-697"><a href="#cb93-697" aria-hidden="true" tabindex="-1"></a>                      <span class="fu">mutate</span>(<span class="at">predicted =</span> <span class="fu">as.factor</span>(predicted)),</span>
<span id="cb93-698"><a href="#cb93-698" aria-hidden="true" tabindex="-1"></a>                    <span class="at">truth =</span> <span class="st">"Direction"</span>,</span>
<span id="cb93-699"><a href="#cb93-699" aria-hidden="true" tabindex="-1"></a>                    <span class="at">estimate =</span> <span class="st">"predicted"</span>))</span>
<span id="cb93-700"><a href="#cb93-700" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(c_mat)</span>
<span id="cb93-701"><a href="#cb93-701" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-702"><a href="#cb93-702" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate accuracy and error rate = 1 - accuracy</span></span>
<span id="cb93-703"><a href="#cb93-703" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; base R</span></span>
<span id="cb93-704"><a href="#cb93-704" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(preds_logreg<span class="sc">$</span>predicted <span class="sc">==</span> data_stock<span class="sc">$</span>Direction)</span>
<span id="cb93-705"><a href="#cb93-705" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">mean</span>(preds_logreg<span class="sc">$</span>predicted <span class="sc">==</span> data_stock<span class="sc">$</span>Direction)</span>
<span id="cb93-706"><a href="#cb93-706" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-707"><a href="#cb93-707" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-708"><a href="#cb93-708" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-709"><a href="#cb93-709" aria-hidden="true" tabindex="-1"></a>Now repeat analysis, but use a holdout sample.</span>
<span id="cb93-710"><a href="#cb93-710" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-713"><a href="#cb93-713" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-714"><a href="#cb93-714" aria-hidden="true" tabindex="-1"></a><span class="co"># sample data </span></span>
<span id="cb93-715"><a href="#cb93-715" aria-hidden="true" tabindex="-1"></a>data_train <span class="ot">&lt;-</span> data_stock <span class="sc">%&gt;%</span> </span>
<span id="cb93-716"><a href="#cb93-716" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(Year <span class="sc">&lt;</span> <span class="dv">2005</span>)</span>
<span id="cb93-717"><a href="#cb93-717" aria-hidden="true" tabindex="-1"></a>data_test <span class="ot">&lt;-</span> data_stock <span class="sc">%&gt;%</span> </span>
<span id="cb93-718"><a href="#cb93-718" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(Year <span class="sc">&gt;=</span> <span class="dv">2005</span>)</span>
<span id="cb93-719"><a href="#cb93-719" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-720"><a href="#cb93-720" aria-hidden="true" tabindex="-1"></a><span class="co"># fit logistic regression model on training data</span></span>
<span id="cb93-721"><a href="#cb93-721" aria-hidden="true" tabindex="-1"></a>mod_logreg2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2 <span class="sc">+</span> Lag3 <span class="sc">+</span> Lag4 <span class="sc">+</span> Lag5 <span class="sc">+</span> Volume,</span>
<span id="cb93-722"><a href="#cb93-722" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> data_train,</span>
<span id="cb93-723"><a href="#cb93-723" aria-hidden="true" tabindex="-1"></a>                   <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb93-724"><a href="#cb93-724" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-725"><a href="#cb93-725" aria-hidden="true" tabindex="-1"></a><span class="co"># predict on holdout data</span></span>
<span id="cb93-726"><a href="#cb93-726" aria-hidden="true" tabindex="-1"></a>preds_logreg2 <span class="ot">&lt;-</span> broom<span class="sc">::</span><span class="fu">augment</span>(mod_logreg2,</span>
<span id="cb93-727"><a href="#cb93-727" aria-hidden="true" tabindex="-1"></a>                              <span class="at">newdata =</span> data_test,</span>
<span id="cb93-728"><a href="#cb93-728" aria-hidden="true" tabindex="-1"></a>                              <span class="at">type.predict =</span> <span class="st">"response"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb93-729"><a href="#cb93-729" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Direction, .fitted) <span class="sc">%&gt;%</span> </span>
<span id="cb93-730"><a href="#cb93-730" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">predicted =</span> <span class="fu">if_else</span>(.fitted <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">"Up"</span>, <span class="st">"Down"</span>) <span class="sc">%&gt;%</span> <span class="fu">factor</span>(<span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"Down"</span>, <span class="st">"Up"</span>)))</span>
<span id="cb93-731"><a href="#cb93-731" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-732"><a href="#cb93-732" aria-hidden="true" tabindex="-1"></a><span class="co"># view results</span></span>
<span id="cb93-733"><a href="#cb93-733" aria-hidden="true" tabindex="-1"></a>preds_logreg2 <span class="sc">%&gt;%</span> </span>
<span id="cb93-734"><a href="#cb93-734" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">conf_mat</span>(<span class="at">truth =</span> <span class="st">"Direction"</span>,</span>
<span id="cb93-735"><a href="#cb93-735" aria-hidden="true" tabindex="-1"></a>                      <span class="at">estimate =</span> <span class="st">"predicted"</span>) <span class="co">#%&gt;% </span></span>
<span id="cb93-736"><a href="#cb93-736" aria-hidden="true" tabindex="-1"></a>  <span class="co">#summary(event_level = "second")</span></span>
<span id="cb93-737"><a href="#cb93-737" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-738"><a href="#cb93-738" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-739"><a href="#cb93-739" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-740"><a href="#cb93-740" aria-hidden="true" tabindex="-1"></a>Now we can do some model selection to get a better model. **After all, using predictors that have no relationship with the response tends to cause a deterioration in the test error rate (since such predictors cause an increase in variance without a corresponding decrease in bias), and so removing such predictors may in turn yield an improvement.**</span>
<span id="cb93-741"><a href="#cb93-741" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-744"><a href="#cb93-744" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-745"><a href="#cb93-745" aria-hidden="true" tabindex="-1"></a><span class="co"># fit smaller model</span></span>
<span id="cb93-746"><a href="#cb93-746" aria-hidden="true" tabindex="-1"></a>mod_logreg3 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2,</span>
<span id="cb93-747"><a href="#cb93-747" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> data_train,</span>
<span id="cb93-748"><a href="#cb93-748" aria-hidden="true" tabindex="-1"></a>                   <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb93-749"><a href="#cb93-749" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-750"><a href="#cb93-750" aria-hidden="true" tabindex="-1"></a><span class="co"># view model summary</span></span>
<span id="cb93-751"><a href="#cb93-751" aria-hidden="true" tabindex="-1"></a>mod_logreg3 <span class="sc">%&gt;%</span> broom<span class="sc">::</span><span class="fu">glance</span>()</span>
<span id="cb93-752"><a href="#cb93-752" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-753"><a href="#cb93-753" aria-hidden="true" tabindex="-1"></a><span class="co"># predict on holdout data</span></span>
<span id="cb93-754"><a href="#cb93-754" aria-hidden="true" tabindex="-1"></a>preds_logreg3 <span class="ot">&lt;-</span> broom<span class="sc">::</span><span class="fu">augment</span>(mod_logreg3,</span>
<span id="cb93-755"><a href="#cb93-755" aria-hidden="true" tabindex="-1"></a>                              <span class="at">newdata =</span> data_test,</span>
<span id="cb93-756"><a href="#cb93-756" aria-hidden="true" tabindex="-1"></a>                              <span class="at">type.predict =</span> <span class="st">"response"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb93-757"><a href="#cb93-757" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(Direction, .fitted) <span class="sc">%&gt;%</span> </span>
<span id="cb93-758"><a href="#cb93-758" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">predicted =</span> <span class="fu">if_else</span>(.fitted <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">"Up"</span>, <span class="st">"Down"</span>) <span class="sc">%&gt;%</span> <span class="fu">factor</span>(<span class="at">levels =</span> <span class="fu">c</span>(<span class="st">"Down"</span>, <span class="st">"Up"</span>)))</span>
<span id="cb93-759"><a href="#cb93-759" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-760"><a href="#cb93-760" aria-hidden="true" tabindex="-1"></a><span class="co"># view results</span></span>
<span id="cb93-761"><a href="#cb93-761" aria-hidden="true" tabindex="-1"></a>preds_logreg3 <span class="sc">%&gt;%</span> </span>
<span id="cb93-762"><a href="#cb93-762" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">conf_mat</span>(<span class="at">truth =</span> <span class="st">"Direction"</span>,</span>
<span id="cb93-763"><a href="#cb93-763" aria-hidden="true" tabindex="-1"></a>                      <span class="at">estimate =</span> <span class="st">"predicted"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb93-764"><a href="#cb93-764" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summary</span>(<span class="at">event_level =</span> <span class="st">"second"</span>)</span>
<span id="cb93-765"><a href="#cb93-765" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-766"><a href="#cb93-766" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate accuracy of naive approach, which is just predicting yes everyday</span></span>
<span id="cb93-767"><a href="#cb93-767" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; get also numbers from truth confusion matrix</span></span>
<span id="cb93-768"><a href="#cb93-768" aria-hidden="true" tabindex="-1"></a>data_test <span class="sc">%&gt;%</span> </span>
<span id="cb93-769"><a href="#cb93-769" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="fu">mean</span>(Direction <span class="sc">==</span> <span class="st">"Up"</span>))</span>
<span id="cb93-770"><a href="#cb93-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-771"><a href="#cb93-771" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-772"><a href="#cb93-772" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-773"><a href="#cb93-773" aria-hidden="true" tabindex="-1"></a><span class="fu">### Linear discriminant analysis</span></span>
<span id="cb93-774"><a href="#cb93-774" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-777"><a href="#cb93-777" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-778"><a href="#cb93-778" aria-hidden="true" tabindex="-1"></a><span class="co"># fit LDA model</span></span>
<span id="cb93-779"><a href="#cb93-779" aria-hidden="true" tabindex="-1"></a>mod_lda <span class="ot">&lt;-</span> MASS<span class="sc">::</span><span class="fu">lda</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2, <span class="at">data =</span> data_train)</span>
<span id="cb93-780"><a href="#cb93-780" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-781"><a href="#cb93-781" aria-hidden="true" tabindex="-1"></a><span class="co"># view model</span></span>
<span id="cb93-782"><a href="#cb93-782" aria-hidden="true" tabindex="-1"></a>mod_lda</span>
<span id="cb93-783"><a href="#cb93-783" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-784"><a href="#cb93-784" aria-hidden="true" tabindex="-1"></a><span class="co"># plot model</span></span>
<span id="cb93-785"><a href="#cb93-785" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; produces plots of the linear discriminants, obtained by computing −0.642 × Lag1 − 0.514 × Lag2 for each of the training observations.</span></span>
<span id="cb93-786"><a href="#cb93-786" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mod_lda)</span>
<span id="cb93-787"><a href="#cb93-787" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-788"><a href="#cb93-788" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-789"><a href="#cb93-789" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-790"><a href="#cb93-790" aria-hidden="true" tabindex="-1"></a>The *coefficients of linear discriminants* output provides the linear combination of Lag1 and Lag2 that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of X = x in @eq-bayes-decision-boundary3.</span>
<span id="cb93-791"><a href="#cb93-791" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-792"><a href="#cb93-792" aria-hidden="true" tabindex="-1"></a><span class="co">&lt;!-- ??? could do more research into exactly what these are --&gt;</span></span>
<span id="cb93-793"><a href="#cb93-793" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-794"><a href="#cb93-794" aria-hidden="true" tabindex="-1"></a>If −0.642 × Lag1 − 0.514 × Lag2 is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline.</span>
<span id="cb93-795"><a href="#cb93-795" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-798"><a href="#cb93-798" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-799"><a href="#cb93-799" aria-hidden="true" tabindex="-1"></a><span class="co"># predict</span></span>
<span id="cb93-800"><a href="#cb93-800" aria-hidden="true" tabindex="-1"></a>preds_lda <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod_lda, <span class="at">newdata =</span> data_test)</span>
<span id="cb93-801"><a href="#cb93-801" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-802"><a href="#cb93-802" aria-hidden="true" tabindex="-1"></a><span class="co"># view prediction output</span></span>
<span id="cb93-803"><a href="#cb93-803" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; class prediction</span></span>
<span id="cb93-804"><a href="#cb93-804" aria-hidden="true" tabindex="-1"></a>preds_lda<span class="sc">$</span>class <span class="sc">%&gt;%</span> head</span>
<span id="cb93-805"><a href="#cb93-805" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; posterior probabilities</span></span>
<span id="cb93-806"><a href="#cb93-806" aria-hidden="true" tabindex="-1"></a>preds_lda<span class="sc">$</span>posterior <span class="sc">%&gt;%</span> head</span>
<span id="cb93-807"><a href="#cb93-807" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; linear discriminants</span></span>
<span id="cb93-808"><a href="#cb93-808" aria-hidden="true" tabindex="-1"></a>preds_lda<span class="sc">$</span>x <span class="sc">%&gt;%</span> head</span>
<span id="cb93-809"><a href="#cb93-809" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-810"><a href="#cb93-810" aria-hidden="true" tabindex="-1"></a><span class="co"># analyze predictions</span></span>
<span id="cb93-811"><a href="#cb93-811" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(preds_lda<span class="sc">$</span>class,</span>
<span id="cb93-812"><a href="#cb93-812" aria-hidden="true" tabindex="-1"></a>      data_test<span class="sc">$</span>Direction,</span>
<span id="cb93-813"><a href="#cb93-813" aria-hidden="true" tabindex="-1"></a>      <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">"predicted"</span>, <span class="st">"Direction"</span>))</span>
<span id="cb93-814"><a href="#cb93-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-815"><a href="#cb93-815" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate accuracy</span></span>
<span id="cb93-816"><a href="#cb93-816" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(preds_lda<span class="sc">$</span>class <span class="sc">==</span> data_test<span class="sc">$</span>Direction)</span>
<span id="cb93-817"><a href="#cb93-817" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-818"><a href="#cb93-818" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-819"><a href="#cb93-819" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-820"><a href="#cb93-820" aria-hidden="true" tabindex="-1"></a><span class="fu">### Quadratic discriminant analysis</span></span>
<span id="cb93-821"><a href="#cb93-821" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-824"><a href="#cb93-824" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-825"><a href="#cb93-825" aria-hidden="true" tabindex="-1"></a><span class="co"># fit QDA model</span></span>
<span id="cb93-826"><a href="#cb93-826" aria-hidden="true" tabindex="-1"></a>mod_qda <span class="ot">&lt;-</span> MASS<span class="sc">::</span><span class="fu">qda</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2, <span class="at">data =</span> data_train)</span>
<span id="cb93-827"><a href="#cb93-827" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-828"><a href="#cb93-828" aria-hidden="true" tabindex="-1"></a><span class="co"># view model</span></span>
<span id="cb93-829"><a href="#cb93-829" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; now output does not contain the coefficients of the linear discriminants, because no longer linear</span></span>
<span id="cb93-830"><a href="#cb93-830" aria-hidden="true" tabindex="-1"></a>mod_qda</span>
<span id="cb93-831"><a href="#cb93-831" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-832"><a href="#cb93-832" aria-hidden="true" tabindex="-1"></a><span class="co"># predict</span></span>
<span id="cb93-833"><a href="#cb93-833" aria-hidden="true" tabindex="-1"></a>preds_qda <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod_qda, <span class="at">newdata =</span> data_test)</span>
<span id="cb93-834"><a href="#cb93-834" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-835"><a href="#cb93-835" aria-hidden="true" tabindex="-1"></a><span class="co"># analyze predictions</span></span>
<span id="cb93-836"><a href="#cb93-836" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(preds_qda<span class="sc">$</span>class,</span>
<span id="cb93-837"><a href="#cb93-837" aria-hidden="true" tabindex="-1"></a>      data_test<span class="sc">$</span>Direction,</span>
<span id="cb93-838"><a href="#cb93-838" aria-hidden="true" tabindex="-1"></a>      <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">"predicted"</span>, <span class="st">"Direction"</span>))</span>
<span id="cb93-839"><a href="#cb93-839" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-840"><a href="#cb93-840" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate accuracy</span></span>
<span id="cb93-841"><a href="#cb93-841" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(preds_qda<span class="sc">$</span>class <span class="sc">==</span> data_test<span class="sc">$</span>Direction)</span>
<span id="cb93-842"><a href="#cb93-842" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-843"><a href="#cb93-843" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-844"><a href="#cb93-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-845"><a href="#cb93-845" aria-hidden="true" tabindex="-1"></a>A higher accuracy for the test data suggests that the quadratic form assumed by QDA may capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression.</span>
<span id="cb93-846"><a href="#cb93-846" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-847"><a href="#cb93-847" aria-hidden="true" tabindex="-1"></a><span class="fu">### Naive Bayes </span></span>
<span id="cb93-848"><a href="#cb93-848" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-851"><a href="#cb93-851" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-852"><a href="#cb93-852" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb93-853"><a href="#cb93-853" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; by default, this models each quantitative feature with a Gaussian distribution</span></span>
<span id="cb93-854"><a href="#cb93-854" aria-hidden="true" tabindex="-1"></a><span class="co"># --&gt; but a kernal density method can also be used to estimate the distributions</span></span>
<span id="cb93-855"><a href="#cb93-855" aria-hidden="true" tabindex="-1"></a>mod_nb <span class="ot">&lt;-</span> e1071<span class="sc">::</span><span class="fu">naiveBayes</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2, <span class="at">data =</span> data_train)</span>
<span id="cb93-856"><a href="#cb93-856" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-857"><a href="#cb93-857" aria-hidden="true" tabindex="-1"></a><span class="co"># view model output</span></span>
<span id="cb93-858"><a href="#cb93-858" aria-hidden="true" tabindex="-1"></a>mod_nb</span>
<span id="cb93-859"><a href="#cb93-859" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-860"><a href="#cb93-860" aria-hidden="true" tabindex="-1"></a><span class="co"># predict</span></span>
<span id="cb93-861"><a href="#cb93-861" aria-hidden="true" tabindex="-1"></a>preds_nb <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod_nb, <span class="at">newdata =</span> data_test)</span>
<span id="cb93-862"><a href="#cb93-862" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-863"><a href="#cb93-863" aria-hidden="true" tabindex="-1"></a><span class="co"># analyze predictions</span></span>
<span id="cb93-864"><a href="#cb93-864" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(preds_nb,</span>
<span id="cb93-865"><a href="#cb93-865" aria-hidden="true" tabindex="-1"></a>      data_test<span class="sc">$</span>Direction,</span>
<span id="cb93-866"><a href="#cb93-866" aria-hidden="true" tabindex="-1"></a>      <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">"predicted"</span>, <span class="st">"Direction"</span>))</span>
<span id="cb93-867"><a href="#cb93-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-868"><a href="#cb93-868" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate accuracy</span></span>
<span id="cb93-869"><a href="#cb93-869" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(preds_nb <span class="sc">==</span> data_test<span class="sc">$</span>Direction)</span>
<span id="cb93-870"><a href="#cb93-870" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-871"><a href="#cb93-871" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-872"><a href="#cb93-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-873"><a href="#cb93-873" aria-hidden="true" tabindex="-1"></a>Naive Bayes performs very well on this data, with accurate predictions over 59% of the time. This is slightly worse than QDA, but much better than LDA.</span>
<span id="cb93-874"><a href="#cb93-874" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-875"><a href="#cb93-875" aria-hidden="true" tabindex="-1"></a><span class="fu">### KNN</span></span>
<span id="cb93-876"><a href="#cb93-876" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-879"><a href="#cb93-879" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-880"><a href="#cb93-880" aria-hidden="true" tabindex="-1"></a><span class="co"># fit model</span></span>
<span id="cb93-881"><a href="#cb93-881" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; this function forms prediction in a single command (not in two steps like the other models: fit then predict)</span></span>
<span id="cb93-882"><a href="#cb93-882" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; </span><span class="al">NOTE</span><span class="co">: if there is a tie (say k = 2 and one observation from each class), then R randomly breaks the tie</span></span>
<span id="cb93-883"><a href="#cb93-883" aria-hidden="true" tabindex="-1"></a><span class="co"># --&gt; so need to set seed if want reproducibility</span></span>
<span id="cb93-884"><a href="#cb93-884" aria-hidden="true" tabindex="-1"></a>preds_knn1 <span class="ot">&lt;-</span> class<span class="sc">::</span><span class="fu">knn</span>(<span class="at">train =</span> data_train <span class="sc">%&gt;%</span> <span class="fu">select</span>(Lag1, Lag2) <span class="sc">%&gt;%</span> as.matrix, <span class="co"># predictors of train set as matrix</span></span>
<span id="cb93-885"><a href="#cb93-885" aria-hidden="true" tabindex="-1"></a>                      <span class="at">test =</span> data_test <span class="sc">%&gt;%</span> <span class="fu">select</span>(Lag1, Lag2) <span class="sc">%&gt;%</span> as.matrix, <span class="co"># predictors of test set as matrix</span></span>
<span id="cb93-886"><a href="#cb93-886" aria-hidden="true" tabindex="-1"></a>                      <span class="at">cl =</span> data_train<span class="sc">$</span>Direction, <span class="co"># truth for train set</span></span>
<span id="cb93-887"><a href="#cb93-887" aria-hidden="true" tabindex="-1"></a>                      <span class="at">k =</span> <span class="dv">1</span>)</span>
<span id="cb93-888"><a href="#cb93-888" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-889"><a href="#cb93-889" aria-hidden="true" tabindex="-1"></a><span class="co"># analyze predictions</span></span>
<span id="cb93-890"><a href="#cb93-890" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(preds_knn1,</span>
<span id="cb93-891"><a href="#cb93-891" aria-hidden="true" tabindex="-1"></a>      data_test<span class="sc">$</span>Direction,</span>
<span id="cb93-892"><a href="#cb93-892" aria-hidden="true" tabindex="-1"></a>      <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">"predicted"</span>, <span class="st">"Direction"</span>))</span>
<span id="cb93-893"><a href="#cb93-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-894"><a href="#cb93-894" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate accuracy</span></span>
<span id="cb93-895"><a href="#cb93-895" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(preds_knn1 <span class="sc">==</span> data_test<span class="sc">$</span>Direction)</span>
<span id="cb93-896"><a href="#cb93-896" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-897"><a href="#cb93-897" aria-hidden="true" tabindex="-1"></a><span class="co"># overfitting...</span></span>
<span id="cb93-898"><a href="#cb93-898" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-899"><a href="#cb93-899" aria-hidden="true" tabindex="-1"></a><span class="co"># refit with better k</span></span>
<span id="cb93-900"><a href="#cb93-900" aria-hidden="true" tabindex="-1"></a>preds_knn3 <span class="ot">&lt;-</span> class<span class="sc">::</span><span class="fu">knn</span>(<span class="at">train =</span> data_train <span class="sc">%&gt;%</span> <span class="fu">select</span>(Lag1, Lag2) <span class="sc">%&gt;%</span> as.matrix, <span class="co"># predictors of train set as matrix</span></span>
<span id="cb93-901"><a href="#cb93-901" aria-hidden="true" tabindex="-1"></a>                      <span class="at">test =</span> data_test <span class="sc">%&gt;%</span> <span class="fu">select</span>(Lag1, Lag2) <span class="sc">%&gt;%</span> as.matrix, <span class="co"># predictors of test set as matrix</span></span>
<span id="cb93-902"><a href="#cb93-902" aria-hidden="true" tabindex="-1"></a>                      <span class="at">cl =</span> data_train<span class="sc">$</span>Direction, <span class="co"># truth for train set</span></span>
<span id="cb93-903"><a href="#cb93-903" aria-hidden="true" tabindex="-1"></a>                      <span class="at">k =</span> <span class="dv">3</span>)</span>
<span id="cb93-904"><a href="#cb93-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-905"><a href="#cb93-905" aria-hidden="true" tabindex="-1"></a><span class="co"># analyze predictions</span></span>
<span id="cb93-906"><a href="#cb93-906" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(preds_knn3,</span>
<span id="cb93-907"><a href="#cb93-907" aria-hidden="true" tabindex="-1"></a>      data_test<span class="sc">$</span>Direction,</span>
<span id="cb93-908"><a href="#cb93-908" aria-hidden="true" tabindex="-1"></a>      <span class="at">dnn =</span> <span class="fu">c</span>(<span class="st">"predicted"</span>, <span class="st">"Direction"</span>))</span>
<span id="cb93-909"><a href="#cb93-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-910"><a href="#cb93-910" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate accuracy</span></span>
<span id="cb93-911"><a href="#cb93-911" aria-hidden="true" tabindex="-1"></a><span class="fu">mean</span>(preds_knn3 <span class="sc">==</span> data_test<span class="sc">$</span>Direction)</span>
<span id="cb93-912"><a href="#cb93-912" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-913"><a href="#cb93-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-914"><a href="#cb93-914" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-915"><a href="#cb93-915" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-916"><a href="#cb93-916" aria-hidden="true" tabindex="-1"></a>The results have improved slightly. But increasing $K$ further turns out to provide no further improvements.</span>
<span id="cb93-917"><a href="#cb93-917" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-918"><a href="#cb93-918" aria-hidden="true" tabindex="-1"></a>It appears that for this data, QDA provides the best results of the methods that we have examined so far.</span>
<span id="cb93-919"><a href="#cb93-919" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-920"><a href="#cb93-920" aria-hidden="true" tabindex="-1"></a>Notes about KNN in general:</span>
<span id="cb93-921"><a href="#cb93-921" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-922"><a href="#cb93-922" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Because the KNN classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters. Variables that are on a large scale will have a much larger effect on the *distance* between the observations, and hence on the KNN classifier, than variables that are on a small scale.</span>
<span id="cb93-923"><a href="#cb93-923" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-924"><a href="#cb93-924" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For instance, imagine a data set that contains two variables, salary and age (measured in dollars and years, respectively). As far as KNN is concerned, a difference of $1,000 in salary is enormous compared to a difference of 50 years in age. Consequently, salary will drive the KNN classification results, and age will have almost no effect. Same for scale of the response, changing units from dollars to cents will lead to different classification results.</span>
<span id="cb93-925"><a href="#cb93-925" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-926"><a href="#cb93-926" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**A good way to handle this problem is to *standardize* the data so that all variables are given a mean of zero and a standard deviation of one. Can use `scale()` to accomplish this.**</span>
<span id="cb93-927"><a href="#cb93-927" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-928"><a href="#cb93-928" aria-hidden="true" tabindex="-1"></a>When using classifying methods in general, can look at the naive approach of guessing all positives as a baseline.</span>
<span id="cb93-929"><a href="#cb93-929" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-930"><a href="#cb93-930" aria-hidden="true" tabindex="-1"></a><span class="fu">### Poisson regression</span></span>
<span id="cb93-931"><a href="#cb93-931" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-932"><a href="#cb93-932" aria-hidden="true" tabindex="-1"></a>First, try to fit a linear regression model.</span>
<span id="cb93-933"><a href="#cb93-933" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-936"><a href="#cb93-936" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-937"><a href="#cb93-937" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb93-938"><a href="#cb93-938" aria-hidden="true" tabindex="-1"></a>data_bike <span class="ot">&lt;-</span> ISLR2<span class="sc">::</span>Bikeshare</span>
<span id="cb93-939"><a href="#cb93-939" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-940"><a href="#cb93-940" aria-hidden="true" tabindex="-1"></a><span class="co"># fit linear regression model</span></span>
<span id="cb93-941"><a href="#cb93-941" aria-hidden="true" tabindex="-1"></a>mod_lr <span class="ot">&lt;-</span> <span class="fu">lm</span>(bikers <span class="sc">~</span> mnth <span class="sc">+</span> hr <span class="sc">+</span> workingday <span class="sc">+</span> temp <span class="sc">+</span> weathersit, <span class="at">data =</span> data_bike)</span>
<span id="cb93-942"><a href="#cb93-942" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-943"><a href="#cb93-943" aria-hidden="true" tabindex="-1"></a><span class="co"># view model summary</span></span>
<span id="cb93-944"><a href="#cb93-944" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_lr)</span>
<span id="cb93-945"><a href="#cb93-945" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">coef</span>(mod_lr))</span>
<span id="cb93-946"><a href="#cb93-946" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-947"><a href="#cb93-947" aria-hidden="true" tabindex="-1"></a><span class="co"># view predictions #check negative ones</span></span>
<span id="cb93-948"><a href="#cb93-948" aria-hidden="true" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">predict</span>(mod))</span>
<span id="cb93-949"><a href="#cb93-949" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-950"><a href="#cb93-950" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-951"><a href="#cb93-951" aria-hidden="true" tabindex="-1"></a>Now we can change contrasts so can get a coefficient estimate for every level of the predictors.</span>
<span id="cb93-952"><a href="#cb93-952" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-953"><a href="#cb93-953" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Contrasts</span><span class="co">](https://learnb4ss.github.io/learnB4SS/articles/contrasts.html)</span> are an attribute of the column.</span>
<span id="cb93-954"><a href="#cb93-954" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-955"><a href="#cb93-955" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Default coding is dummy variable (aka treatment contrasts), where coefficients are set relative to the reference level.</span>
<span id="cb93-956"><a href="#cb93-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-957"><a href="#cb93-957" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>For sum contrasts, all contrasts sum to 0 for each dummy variable and the reference level is in fact the grand mean. Also, now coefficients are the difference relative to the grand mean (which is now the intercept).</span>
<span id="cb93-958"><a href="#cb93-958" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-959"><a href="#cb93-959" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Lastly for sum contrasts, the last coefficient isn't given in the output, but it can be easily calculated: *it is the negative sum of the coefficient estimates for all of the other levels*.</span>
<span id="cb93-960"><a href="#cb93-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-963"><a href="#cb93-963" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-964"><a href="#cb93-964" aria-hidden="true" tabindex="-1"></a><span class="co"># set new contrasts</span></span>
<span id="cb93-965"><a href="#cb93-965" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(data_bike<span class="sc">$</span>hr) <span class="ot">&lt;-</span> <span class="st">"contr.sum"</span></span>
<span id="cb93-966"><a href="#cb93-966" aria-hidden="true" tabindex="-1"></a><span class="fu">contrasts</span>(data_bike<span class="sc">$</span>mnth) <span class="ot">&lt;-</span> <span class="st">"contr.sum"</span></span>
<span id="cb93-967"><a href="#cb93-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-968"><a href="#cb93-968" aria-hidden="true" tabindex="-1"></a><span class="co"># refit model</span></span>
<span id="cb93-969"><a href="#cb93-969" aria-hidden="true" tabindex="-1"></a>mod_lr2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(bikers <span class="sc">~</span> mnth <span class="sc">+</span> hr <span class="sc">+</span> workingday <span class="sc">+</span> temp <span class="sc">+</span> weathersit, <span class="at">data =</span> data_bike)</span>
<span id="cb93-970"><a href="#cb93-970" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-971"><a href="#cb93-971" aria-hidden="true" tabindex="-1"></a><span class="co"># view model summary</span></span>
<span id="cb93-972"><a href="#cb93-972" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_lr2)</span>
<span id="cb93-973"><a href="#cb93-973" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod_lr2) <span class="sc">%&gt;%</span> {<span class="fu">c</span>(<span class="fu">head</span>(.), <span class="fu">tail</span>(.))}</span>
<span id="cb93-974"><a href="#cb93-974" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-975"><a href="#cb93-975" aria-hidden="true" tabindex="-1"></a><span class="co"># create plot of the coefficients for one of the factor variables</span></span>
<span id="cb93-976"><a href="#cb93-976" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod_lr2)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>] <span class="sc">%&gt;%</span> </span>
<span id="cb93-977"><a href="#cb93-977" aria-hidden="true" tabindex="-1"></a>  {<span class="fu">c</span>(., <span class="sc">-</span><span class="fu">sum</span>(.))} <span class="sc">%&gt;%</span> </span>
<span id="cb93-978"><a href="#cb93-978" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="at">type =</span> <span class="st">"b"</span>, <span class="at">xlab =</span> <span class="st">"Month"</span>, <span class="at">ylab =</span> <span class="st">"Coefficient"</span>)</span>
<span id="cb93-979"><a href="#cb93-979" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-980"><a href="#cb93-980" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-981"><a href="#cb93-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-982"><a href="#cb93-982" aria-hidden="true" tabindex="-1"></a>Now we can fit a Poisson regression model.</span>
<span id="cb93-983"><a href="#cb93-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-986"><a href="#cb93-986" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-987"><a href="#cb93-987" aria-hidden="true" tabindex="-1"></a><span class="co"># fit poisson regression model</span></span>
<span id="cb93-988"><a href="#cb93-988" aria-hidden="true" tabindex="-1"></a>mod_pois <span class="ot">&lt;-</span> <span class="fu">glm</span>(bikers <span class="sc">~</span> mnth <span class="sc">+</span> hr <span class="sc">+</span> workingday <span class="sc">+</span> temp <span class="sc">+</span> weathersit,</span>
<span id="cb93-989"><a href="#cb93-989" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> data_bike,</span>
<span id="cb93-990"><a href="#cb93-990" aria-hidden="true" tabindex="-1"></a>                <span class="at">family =</span> <span class="st">"poisson"</span>)</span>
<span id="cb93-991"><a href="#cb93-991" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-992"><a href="#cb93-992" aria-hidden="true" tabindex="-1"></a><span class="co"># view model summary</span></span>
<span id="cb93-993"><a href="#cb93-993" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mod_pois)</span>
<span id="cb93-994"><a href="#cb93-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-995"><a href="#cb93-995" aria-hidden="true" tabindex="-1"></a><span class="co"># plot estimated coefficients (still using sum contrasts)</span></span>
<span id="cb93-996"><a href="#cb93-996" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(mod_pois)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>] <span class="sc">%&gt;%</span> </span>
<span id="cb93-997"><a href="#cb93-997" aria-hidden="true" tabindex="-1"></a>  {<span class="fu">c</span>(., <span class="sc">-</span><span class="fu">sum</span>(.))} <span class="sc">%&gt;%</span> </span>
<span id="cb93-998"><a href="#cb93-998" aria-hidden="true" tabindex="-1"></a>  <span class="fu">plot</span>(<span class="at">type =</span> <span class="st">"b"</span>, <span class="at">xlab =</span> <span class="st">"Month"</span>, <span class="at">ylab =</span> <span class="st">"Coefficient"</span>)</span>
<span id="cb93-999"><a href="#cb93-999" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1000"><a href="#cb93-1000" aria-hidden="true" tabindex="-1"></a><span class="co"># make predictions</span></span>
<span id="cb93-1001"><a href="#cb93-1001" aria-hidden="true" tabindex="-1"></a>preds_pois <span class="ot">&lt;-</span> <span class="fu">predict</span>(mod_pois, <span class="at">type =</span> <span class="st">"response"</span>)</span>
<span id="cb93-1002"><a href="#cb93-1002" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1003"><a href="#cb93-1003" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-1004"><a href="#cb93-1004" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1005"><a href="#cb93-1005" aria-hidden="true" tabindex="-1"></a><span class="fu">## Exercises</span></span>
<span id="cb93-1006"><a href="#cb93-1006" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1007"><a href="#cb93-1007" aria-hidden="true" tabindex="-1"></a><span class="fu">### Conceptual</span></span>
<span id="cb93-1008"><a href="#cb93-1008" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1009"><a href="#cb93-1009" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 1</span></span>
<span id="cb93-1010"><a href="#cb93-1010" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1011"><a href="#cb93-1011" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Using a little bit of algebra, prove that (4.2) is equivalent to (4.3). In other words, the logistic function representation and logit representation for the logistic regression model are equivalent.</span></span>
<span id="cb93-1012"><a href="#cb93-1012" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1013"><a href="#cb93-1013" aria-hidden="true" tabindex="-1"></a>&lt; already showed in notes &gt;</span>
<span id="cb93-1014"><a href="#cb93-1014" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1015"><a href="#cb93-1015" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 2</span></span>
<span id="cb93-1016"><a href="#cb93-1016" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1017"><a href="#cb93-1017" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; It was stated in the text that classifying an observation to the class for which (4.12) is largest is equivalent to classifying an observation to the class for which (4.13) is largest. Prove that this is the case. In other words, under the assumption that the observations in the $k$th class are drawn from a $N(\mu_k,\sigma^2)$ distribution, the Bayes' classifier assigns an observation to the class for which the discriminant function is maximized.</span></span>
<span id="cb93-1018"><a href="#cb93-1018" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1019"><a href="#cb93-1019" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-q2.png)</span></span>
<span id="cb93-1020"><a href="#cb93-1020" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1021"><a href="#cb93-1021" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 3</span></span>
<span id="cb93-1022"><a href="#cb93-1022" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1023"><a href="#cb93-1023" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This problem relates to the QDA model, in which the observations within each class are drawn from a normal distribution with a class-specific mean vector and a class specific covariance matrix. We consider the simple case where $p = 1$; i.e. there is only one feature.</span></span>
<span id="cb93-1024"><a href="#cb93-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1025"><a href="#cb93-1025" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Suppose that we have $K$ classes, and that if an observation belongs to the kth class then $X$ comes from a one-dimensional normal distribution, $X \sim N(\mu_k,\sigma^2_k)$. Recall that the density function for the one-dimensional normal distribution is given in (4.16). Prove that in this case, the Bayes classifier is *not* linear. Argue that it is in fact quadratic.</span></span>
<span id="cb93-1026"><a href="#cb93-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1027"><a href="#cb93-1027" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; *Hint: For this problem, you should follow the arguments laid out in Section 4.4.1, but without making the assumption that $\sigma_1^2 = ... = \sigma_K^2$.*</span></span>
<span id="cb93-1028"><a href="#cb93-1028" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1029"><a href="#cb93-1029" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-q3.png)</span></span>
<span id="cb93-1030"><a href="#cb93-1030" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1031"><a href="#cb93-1031" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 4</span></span>
<span id="cb93-1032"><a href="#cb93-1032" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1033"><a href="#cb93-1033" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; When the number of features $p$ is large, there tends to be a deterioration in the performance of KNN and other *local* approaches that perform prediction using only observations that are *near* the test observation for which a prediction must be made. This phenomenon is known as the *curse of dimensionality*, and it ties into the fact that non-parametric approaches often perform poorly when $p$ is large. We will now investigate this curse.</span></span>
<span id="cb93-1034"><a href="#cb93-1034" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1035"><a href="#cb93-1035" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a. Suppose that we have a set of observations, each with measurements on $p = 1$ feature, $X$. We assume that $X$ is uniformly (evenly) distributed on $</span><span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span><span class="at">$. Associated with each observation is a response value. Suppose that we wish to predict a test observation's response using only observations that are within 10% of the range of $X$ closest to that test observation. For instance, in order to predict the response for a test observation with $X = 0.6$, we will use observations in the range $</span><span class="co">[</span><span class="ot">0.55, 0.65</span><span class="co">]</span><span class="at">$. On average, what fraction of the available observations will we use to make the prediction?</span></span>
<span id="cb93-1036"><a href="#cb93-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1037"><a href="#cb93-1037" aria-hidden="true" tabindex="-1"></a>For values in $<span class="co">[</span><span class="ot">0,0.05</span><span class="co">]</span>$, we use less than 10% of observations (between 5% and 10%, 7.5% on average), similarly with values in $<span class="co">[</span><span class="ot">0.95,1</span><span class="co">]</span>$. For values in $<span class="co">[</span><span class="ot">0.05,0.95</span><span class="co">]</span>$ we use 10% of available observations. The (weighted) average is then $7.5 \times 0.1 + 10 \times 0.9 = 9.75\%$.</span>
<span id="cb93-1038"><a href="#cb93-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1039"><a href="#cb93-1039" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; b. Now suppose that we have a set of observations, each with measurements on</span></span>
<span id="cb93-1040"><a href="#cb93-1040" aria-hidden="true" tabindex="-1"></a><span class="at">$p = 2$ features, $X_1$ and $X_2$. We assume that $(X_1, X_2)$ are</span></span>
<span id="cb93-1041"><a href="#cb93-1041" aria-hidden="true" tabindex="-1"></a><span class="at">uniformly distributed on $</span><span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span><span class="at"> \times </span><span class="co">[</span><span class="ot">0, 1</span><span class="co">]</span><span class="at">$. We wish to predict a test</span></span>
<span id="cb93-1042"><a href="#cb93-1042" aria-hidden="true" tabindex="-1"></a><span class="at">observation's response using only observations that are within 10% of the</span></span>
<span id="cb93-1043"><a href="#cb93-1043" aria-hidden="true" tabindex="-1"></a><span class="at">range of $X_1$ _and_ within 10% of the range of $X_2$ closest to that test</span></span>
<span id="cb93-1044"><a href="#cb93-1044" aria-hidden="true" tabindex="-1"></a><span class="at">observation. For instance, in order to predict the response for a test</span></span>
<span id="cb93-1045"><a href="#cb93-1045" aria-hidden="true" tabindex="-1"></a><span class="at">observation with $X_1 = 0.6$ and $X_2 = 0.35$, we will use observations in</span></span>
<span id="cb93-1046"><a href="#cb93-1046" aria-hidden="true" tabindex="-1"></a><span class="at">the range $</span><span class="co">[</span><span class="ot">0.55, 0.65</span><span class="co">]</span><span class="at">$ for $X_1$ and in the range $</span><span class="co">[</span><span class="ot">0.3, 0.4</span><span class="co">]</span><span class="at">$ for $X_2$.</span></span>
<span id="cb93-1047"><a href="#cb93-1047" aria-hidden="true" tabindex="-1"></a><span class="at">On average, what fraction of the available observations will we use to</span></span>
<span id="cb93-1048"><a href="#cb93-1048" aria-hidden="true" tabindex="-1"></a><span class="at">make the prediction?</span></span>
<span id="cb93-1049"><a href="#cb93-1049" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1050"><a href="#cb93-1050" aria-hidden="true" tabindex="-1"></a>Since we need the observation to be within range for $X_1$ and $X_2$ we square</span>
<span id="cb93-1051"><a href="#cb93-1051" aria-hidden="true" tabindex="-1"></a>9.75% = $0.0975^2 \times 100 = 0.95\%$</span>
<span id="cb93-1052"><a href="#cb93-1052" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1053"><a href="#cb93-1053" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; c. Now suppose that we have a set of observations on $p = 100$ features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation's response using observations within the 10% of each feature's range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?</span></span>
<span id="cb93-1054"><a href="#cb93-1054" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1055"><a href="#cb93-1055" aria-hidden="true" tabindex="-1"></a>Similar to above, we use: $0.0975^{100} \times 100 = 8 \times 10^{-100}\%$, essentially zero.</span>
<span id="cb93-1056"><a href="#cb93-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1057"><a href="#cb93-1057" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; d. Using your answers to parts (a)--(c), argue that a drawback of KNN when $p$ is large is that there are very few training observations "near" any given test observation.</span></span>
<span id="cb93-1058"><a href="#cb93-1058" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1059"><a href="#cb93-1059" aria-hidden="true" tabindex="-1"></a>As $p$ increases, the fraction of observations near any given point rapidly approaches zero. For instance, even if you use 50% of the nearest observations for each $p$, with $p = 10$, only $0.5^{10} \times 100 \approx 0.1\%$ points are "near".</span>
<span id="cb93-1060"><a href="#cb93-1060" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1061"><a href="#cb93-1061" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; e. Now suppose that we wish to make a prediction for a test observation by creating a $p$-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For $p = $1,2, and 100, what is the length of each side of the hypercube? Comment on your answer. </span></span>
<span id="cb93-1062"><a href="#cb93-1062" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1063"><a href="#cb93-1063" aria-hidden="true" tabindex="-1"></a>When $p = 1$, clearly the length is 0.1. When $p = 2$, we need the value $l$ such that $l^2 = 0.1$, so $l = \sqrt{0.1} \approx 0.32$. With $p$ variables, $l = 0.1^{1/p}$, so in the case of $p = 100$, $l = 0.977$. Therefore, the length of each side of the hypercube rapidly approaches 1 (or 100%) of the range of each $p$.</span>
<span id="cb93-1064"><a href="#cb93-1064" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1065"><a href="#cb93-1065" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 5 </span></span>
<span id="cb93-1066"><a href="#cb93-1066" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1067"><a href="#cb93-1067" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; We now examine the differences between LDA and QDA.</span></span>
<span id="cb93-1068"><a href="#cb93-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1069"><a href="#cb93-1069" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a. If the Bayes decision boundary is linear, do we expect LDA or QDA to perform better on the training set? On the test set?</span></span>
<span id="cb93-1070"><a href="#cb93-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1071"><a href="#cb93-1071" aria-hidden="true" tabindex="-1"></a>QDA because because of overfitting, will always perform better on the training set. But because the decision boundary is linear, on the testing set LDA will perform better.</span>
<span id="cb93-1072"><a href="#cb93-1072" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1073"><a href="#cb93-1073" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; b. If the Bayes decision boundary is non-linear, do we expect LDA or QDA to perform better on the training set? On the test set?</span></span>
<span id="cb93-1074"><a href="#cb93-1074" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1075"><a href="#cb93-1075" aria-hidden="true" tabindex="-1"></a>QDA because because it is a more flexible model will perform better. And because the decision boundary is non-linear, on the testing set QDA will still perform better.</span>
<span id="cb93-1076"><a href="#cb93-1076" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1077"><a href="#cb93-1077" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; c. In general, as the sample size $n$ increases, do we expect the test prediction accuracy of QDA relative to LDA to improve, decline, or be unchanged? Why?</span></span>
<span id="cb93-1078"><a href="#cb93-1078" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1079"><a href="#cb93-1079" aria-hidden="true" tabindex="-1"></a>Improve, will have more data points to take into account the added parameters from LDA (less overfitting), more data to pick up on smaller effects.</span>
<span id="cb93-1080"><a href="#cb93-1080" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1081"><a href="#cb93-1081" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; d. True or False: Even if the Bayes decision boundary for a given problem is linear, we will probably achieve a superior test error rate using QDA rather than LDA because QDA is flexible enough to model a linear decision boundary. Justify your answer.</span></span>
<span id="cb93-1082"><a href="#cb93-1082" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1083"><a href="#cb93-1083" aria-hidden="true" tabindex="-1"></a>Not necessarily. The decrease in error rate from flexibility may not offset the increase in bias due to overfitting.</span>
<span id="cb93-1084"><a href="#cb93-1084" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1085"><a href="#cb93-1085" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 6</span></span>
<span id="cb93-1086"><a href="#cb93-1086" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1087"><a href="#cb93-1087" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Suppose we collect data for a group of students in a statistics class with variables $X_1 =$ hours studied, $X_2 =$ undergrad GPA, and $Y =$ receive an A. We fit a logistic regression and produce estimated coefficient, $\hat\beta_0 = -6$, $\hat\beta_1= 0.05$, $\hat\beta_2 = 1$.</span></span>
<span id="cb93-1088"><a href="#cb93-1088" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1089"><a href="#cb93-1089" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a. Estimate the probability that a student who studies for 40h and has an undergrad GPA of 3.5 gets an A in the class.</span></span>
<span id="cb93-1090"><a href="#cb93-1090" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1091"><a href="#cb93-1091" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; b. How many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?</span></span>
<span id="cb93-1092"><a href="#cb93-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1093"><a href="#cb93-1093" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-q6.png)</span></span>
<span id="cb93-1094"><a href="#cb93-1094" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1095"><a href="#cb93-1095" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 7</span></span>
<span id="cb93-1096"><a href="#cb93-1096" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1097"><a href="#cb93-1097" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Suppose that we wish to predict whether a given stock will issue a dividend this year ("Yes" or "No") based on $X$, last year's percent profit. We examine a large number of companies and discover that the mean value of $X$ for companies that issued a dividend was $\bar{X} = 10$, while the mean for those that didn't was $\bar{X} = 0$. In addition, the variance of $X$ for these two sets of companies was $\hat{\sigma}^2 = 36$. Finally, 80% of companies issued dividends. Assuming that $X$ follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was $X = 4$ last year.</span></span>
<span id="cb93-1098"><a href="#cb93-1098" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1099"><a href="#cb93-1099" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; *Hint: Recall that the density function for a normal random variable is $f(x) =\frac{1}{\sqrt{2\pi\sigma^2}}e^{-(x-\mu)^2/2\sigma^2}$. You will need to use Bayes' theorem.*</span></span>
<span id="cb93-1100"><a href="#cb93-1100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1101"><a href="#cb93-1101" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-q7.png)</span></span>
<span id="cb93-1102"><a href="#cb93-1102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1103"><a href="#cb93-1103" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 8</span></span>
<span id="cb93-1104"><a href="#cb93-1104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1105"><a href="#cb93-1105" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Suppose that we take a data set, divide it into equally-sized training and test sets, and then try out two different classification procedures. First we use logistic regression and get an error rate of 20% on the training data and 30% on the test data. Next we use 1-nearest neighbors (i.e. $K = 1$) and get an average error rate (averaged over both test and training data sets) of 18%. Based on these results, which method should we prefer to use for classification of new observations? Why?</span></span>
<span id="cb93-1106"><a href="#cb93-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1107"><a href="#cb93-1107" aria-hidden="true" tabindex="-1"></a>For $K = 1$, performance on the training set is perfect and the error rate</span>
<span id="cb93-1108"><a href="#cb93-1108" aria-hidden="true" tabindex="-1"></a>is zero, implying a test error rate of 36%. Logistic regression outperforms</span>
<span id="cb93-1109"><a href="#cb93-1109" aria-hidden="true" tabindex="-1"></a>1-nearest neighbor on the test set and therefore should be preferred.</span>
<span id="cb93-1110"><a href="#cb93-1110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1111"><a href="#cb93-1111" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 9</span></span>
<span id="cb93-1112"><a href="#cb93-1112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1113"><a href="#cb93-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1114"><a href="#cb93-1114" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This problem has to do with *odds*.</span></span>
<span id="cb93-1115"><a href="#cb93-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1116"><a href="#cb93-1116" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a. On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?</span></span>
<span id="cb93-1117"><a href="#cb93-1117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1118"><a href="#cb93-1118" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; b. Suppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?</span></span>
<span id="cb93-1119"><a href="#cb93-1119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1120"><a href="#cb93-1120" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-q9.png)</span></span>
<span id="cb93-1121"><a href="#cb93-1121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1122"><a href="#cb93-1122" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 10</span></span>
<span id="cb93-1123"><a href="#cb93-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1124"><a href="#cb93-1124" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Equation 4.32 derived an expression for $\log(\frac{Pr(Y=k|X=x)}{Pr(Y=K|X=x)})$ in the setting where $p &gt; 1$, so that the mean for the $k$th class, $\mu_k$, is a $p$-dimensional vector, and the shared covariance $\Sigma$ is a  $p \times p$ matrix. However, in the setting with $p = 1$, (4.32) takes a  simpler form, since the means $\mu_1, ..., \mu_k$ and the variance $\sigma^2$ are scalars. In this simpler setting, repeat the calculation in (4.32), and provide expressions for $a_k$ and $b_{kj}$ in terms of $\pi_k, \pi_K, \mu_k, \mu_K,$ and $\sigma^2$.</span></span>
<span id="cb93-1125"><a href="#cb93-1125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1126"><a href="#cb93-1126" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-q10.png)</span></span>
<span id="cb93-1127"><a href="#cb93-1127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1128"><a href="#cb93-1128" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 11</span></span>
<span id="cb93-1129"><a href="#cb93-1129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1130"><a href="#cb93-1130" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Work out the detailed forms of $a_k$, $b_{kj}$, and $b_{kjl}$ in (4.33). Your answer should involve $\pi_k$, $\pi_K$, $\mu_k$, $\mu_K$, $\Sigma_k$, and $\Sigma_K$.</span></span>
<span id="cb93-1131"><a href="#cb93-1131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1132"><a href="#cb93-1132" aria-hidden="true" tabindex="-1"></a>&lt; skipping &gt;</span>
<span id="cb93-1133"><a href="#cb93-1133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1134"><a href="#cb93-1134" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 12</span></span>
<span id="cb93-1135"><a href="#cb93-1135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1136"><a href="#cb93-1136" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Suppose that you wish to classify an observation $X \in \mathbb{R}$ into </span><span class="in">`apples`</span><span class="at"> and </span><span class="in">`oranges`</span><span class="at">. You fit a logistic regression model and find that</span></span>
<span id="cb93-1137"><a href="#cb93-1137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1138"><a href="#cb93-1138" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $$</span></span>
<span id="cb93-1139"><a href="#cb93-1139" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; \hat{Pr}(Y=orange|X=x) = </span></span>
<span id="cb93-1140"><a href="#cb93-1140" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; \frac{\exp(\hat\beta_0 + \hat\beta_1x)}{1 + \exp(\hat\beta_0 + \hat\beta_1x)}</span></span>
<span id="cb93-1141"><a href="#cb93-1141" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $$</span></span>
<span id="cb93-1142"><a href="#cb93-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1143"><a href="#cb93-1143" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Your friend fits a logistic regression model to the same data using the *softmax* formulation in (4.13), and finds that</span></span>
<span id="cb93-1144"><a href="#cb93-1144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1145"><a href="#cb93-1145" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $$</span></span>
<span id="cb93-1146"><a href="#cb93-1146" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; \hat{Pr}(Y=orange|X=x) = </span></span>
<span id="cb93-1147"><a href="#cb93-1147" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; \frac{\exp(\hat\alpha_{orange0} + \hat\alpha_{orange1}x)}</span></span>
<span id="cb93-1148"><a href="#cb93-1148" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; {\exp(\hat\alpha_{orange0} + \hat\alpha_{orange1}x) + \exp(\hat\alpha_{apple0} + \hat\alpha_{apple1}x)}</span></span>
<span id="cb93-1149"><a href="#cb93-1149" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; $$</span></span>
<span id="cb93-1150"><a href="#cb93-1150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1151"><a href="#cb93-1151" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; a. What is the log odds of </span><span class="in">`orange`</span><span class="at"> versus </span><span class="in">`apple`</span><span class="at"> in your model?</span></span>
<span id="cb93-1152"><a href="#cb93-1152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1153"><a href="#cb93-1153" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; b. What is the log odds of </span><span class="in">`orange`</span><span class="at"> versus </span><span class="in">`apple`</span><span class="at"> in your friend's model?</span></span>
<span id="cb93-1154"><a href="#cb93-1154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1155"><a href="#cb93-1155" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; c. Suppose that in your model, $\hat\beta_0 = 2$ and $\hat\beta_1 = −1$. What are the coefficient estimates in your friend's model? Be as specific as possible.</span></span>
<span id="cb93-1156"><a href="#cb93-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1157"><a href="#cb93-1157" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; d. Now suppose that you and your friend fit the same two models on a different data set. This time, your friend gets the coefficient estimates $\hat\alpha_{orange0} = 1.2$, $\hat\alpha_{orange1} = −2$, $\hat\alpha_{apple0} = 3$, $\hat\alpha_{apple1} = 0.6$. What are the coefficient estimates in your model?</span></span>
<span id="cb93-1158"><a href="#cb93-1158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1159"><a href="#cb93-1159" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; e. Finally, suppose you apply both models from (d) to a data set with 2,000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from your friend's model? Explain your answer.</span></span>
<span id="cb93-1160"><a href="#cb93-1160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1161"><a href="#cb93-1161" aria-hidden="true" tabindex="-1"></a><span class="al">![](files/images/4-q12.png)</span></span>
<span id="cb93-1162"><a href="#cb93-1162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1163"><a href="#cb93-1163" aria-hidden="true" tabindex="-1"></a><span class="fu">### Applied</span></span>
<span id="cb93-1164"><a href="#cb93-1164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1165"><a href="#cb93-1165" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1166"><a href="#cb93-1166" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 13</span></span>
<span id="cb93-1167"><a href="#cb93-1167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1168"><a href="#cb93-1168" aria-hidden="true" tabindex="-1"></a>EDA</span>
<span id="cb93-1169"><a href="#cb93-1169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1172"><a href="#cb93-1172" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-1173"><a href="#cb93-1173" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb93-1174"><a href="#cb93-1174" aria-hidden="true" tabindex="-1"></a>data_weekly <span class="ot">&lt;-</span> ISLR2<span class="sc">::</span>Weekly</span>
<span id="cb93-1175"><a href="#cb93-1175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1176"><a href="#cb93-1176" aria-hidden="true" tabindex="-1"></a><span class="co"># table for the response</span></span>
<span id="cb93-1177"><a href="#cb93-1177" aria-hidden="true" tabindex="-1"></a>data_weekly <span class="sc">%&gt;%</span> </span>
<span id="cb93-1178"><a href="#cb93-1178" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">.by =</span> Direction,</span>
<span id="cb93-1179"><a href="#cb93-1179" aria-hidden="true" tabindex="-1"></a>            <span class="at">n =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1180"><a href="#cb93-1180" aria-hidden="true" tabindex="-1"></a>  gt<span class="sc">::</span><span class="fu">gt</span>() <span class="co"># try gtSummary to add total row</span></span>
<span id="cb93-1181"><a href="#cb93-1181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1182"><a href="#cb93-1182" aria-hidden="true" tabindex="-1"></a><span class="co"># correlation plot</span></span>
<span id="cb93-1183"><a href="#cb93-1183" aria-hidden="true" tabindex="-1"></a>data_weekly <span class="sc">%&gt;%</span> </span>
<span id="cb93-1184"><a href="#cb93-1184" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1185"><a href="#cb93-1185" aria-hidden="true" tabindex="-1"></a>  as.matrix <span class="sc">%&gt;%</span> </span>
<span id="cb93-1186"><a href="#cb93-1186" aria-hidden="true" tabindex="-1"></a>  cor <span class="sc">%&gt;%</span> </span>
<span id="cb93-1187"><a href="#cb93-1187" aria-hidden="true" tabindex="-1"></a>  corrplot<span class="sc">::</span><span class="fu">corrplot</span>()</span>
<span id="cb93-1188"><a href="#cb93-1188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1189"><a href="#cb93-1189" aria-hidden="true" tabindex="-1"></a><span class="co"># density plots of numeric predictors</span></span>
<span id="cb93-1190"><a href="#cb93-1190" aria-hidden="true" tabindex="-1"></a>data_weekly <span class="sc">%&gt;%</span> </span>
<span id="cb93-1191"><a href="#cb93-1191" aria-hidden="true" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">starts_with</span>(<span class="st">"Lag"</span>),</span>
<span id="cb93-1192"><a href="#cb93-1192" aria-hidden="true" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">"lag"</span>,</span>
<span id="cb93-1193"><a href="#cb93-1193" aria-hidden="true" tabindex="-1"></a>               <span class="at">values_to =</span> <span class="st">"value"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1194"><a href="#cb93-1194" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb93-1195"><a href="#cb93-1195" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">x =</span> value,</span>
<span id="cb93-1196"><a href="#cb93-1196" aria-hidden="true" tabindex="-1"></a>                   <span class="at">color =</span> lag)) <span class="sc">+</span> </span>
<span id="cb93-1197"><a href="#cb93-1197" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_grid</span>(Direction <span class="sc">~</span> .)</span>
<span id="cb93-1198"><a href="#cb93-1198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1199"><a href="#cb93-1199" aria-hidden="true" tabindex="-1"></a><span class="co"># line plot of number of increases and decreases per year</span></span>
<span id="cb93-1200"><a href="#cb93-1200" aria-hidden="true" tabindex="-1"></a>data_weekly <span class="sc">%&gt;%</span> </span>
<span id="cb93-1201"><a href="#cb93-1201" aria-hidden="true" tabindex="-1"></a>  <span class="fu">group_by</span>(Year, Direction) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1202"><a href="#cb93-1202" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">n =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1203"><a href="#cb93-1203" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Year,</span>
<span id="cb93-1204"><a href="#cb93-1204" aria-hidden="true" tabindex="-1"></a>                <span class="at">y =</span> n,</span>
<span id="cb93-1205"><a href="#cb93-1205" aria-hidden="true" tabindex="-1"></a>                <span class="at">color =</span> Direction)) <span class="sc">+</span> </span>
<span id="cb93-1206"><a href="#cb93-1206" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span> </span>
<span id="cb93-1207"><a href="#cb93-1207" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">se =</span> <span class="cn">FALSE</span>)</span>
<span id="cb93-1208"><a href="#cb93-1208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1209"><a href="#cb93-1209" aria-hidden="true" tabindex="-1"></a><span class="co"># histograms of other predictors</span></span>
<span id="cb93-1210"><a href="#cb93-1210" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; some by class of response</span></span>
<span id="cb93-1211"><a href="#cb93-1211" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(data_weekly<span class="sc">$</span>Today)</span>
<span id="cb93-1212"><a href="#cb93-1212" aria-hidden="true" tabindex="-1"></a>data_weekly <span class="sc">%&gt;%</span> </span>
<span id="cb93-1213"><a href="#cb93-1213" aria-hidden="true" tabindex="-1"></a>  <span class="fu">split</span>(.<span class="sc">$</span>Direction) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1214"><a href="#cb93-1214" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(\(df) <span class="fu">hist</span>(df<span class="sc">$</span>Volume))</span>
<span id="cb93-1215"><a href="#cb93-1215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1216"><a href="#cb93-1216" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-1217"><a href="#cb93-1217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1218"><a href="#cb93-1218" aria-hidden="true" tabindex="-1"></a>Observations</span>
<span id="cb93-1219"><a href="#cb93-1219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1220"><a href="#cb93-1220" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Balanced-ish response</span>
<span id="cb93-1221"><a href="#cb93-1221" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Fluctuation in number of weeks with Up/Down by year, no real pattern though</span>
<span id="cb93-1222"><a href="#cb93-1222" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Lag variables are roughly normal within class; Volume is not (right skewed)</span>
<span id="cb93-1223"><a href="#cb93-1223" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Year and Volume have a strong, positive correlation</span>
<span id="cb93-1224"><a href="#cb93-1224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1225"><a href="#cb93-1225" aria-hidden="true" tabindex="-1"></a>Now we can fit a logistic regression model</span>
<span id="cb93-1226"><a href="#cb93-1226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1229"><a href="#cb93-1229" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-1230"><a href="#cb93-1230" aria-hidden="true" tabindex="-1"></a><span class="co"># fit logistic regression model</span></span>
<span id="cb93-1231"><a href="#cb93-1231" aria-hidden="true" tabindex="-1"></a>mod_logreg <span class="ot">&lt;-</span> <span class="fu">glm</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2 <span class="sc">+</span> Lag3 <span class="sc">+</span> Lag4 <span class="sc">+</span> Lag5 <span class="sc">+</span> Volume,</span>
<span id="cb93-1232"><a href="#cb93-1232" aria-hidden="true" tabindex="-1"></a>                  <span class="at">data =</span> data_weekly,</span>
<span id="cb93-1233"><a href="#cb93-1233" aria-hidden="true" tabindex="-1"></a>                  <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb93-1234"><a href="#cb93-1234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1235"><a href="#cb93-1235" aria-hidden="true" tabindex="-1"></a><span class="co"># view model summary</span></span>
<span id="cb93-1236"><a href="#cb93-1236" aria-hidden="true" tabindex="-1"></a>broom<span class="sc">::</span><span class="fu">tidy</span>(mod_logreg)</span>
<span id="cb93-1237"><a href="#cb93-1237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1238"><a href="#cb93-1238" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-1239"><a href="#cb93-1239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1240"><a href="#cb93-1240" aria-hidden="true" tabindex="-1"></a>Only Lag2 is significant.</span>
<span id="cb93-1241"><a href="#cb93-1241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1242"><a href="#cb93-1242" aria-hidden="true" tabindex="-1"></a>Analyze predictions on training data.</span>
<span id="cb93-1243"><a href="#cb93-1243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1246"><a href="#cb93-1246" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-1247"><a href="#cb93-1247" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate confusion matrix</span></span>
<span id="cb93-1248"><a href="#cb93-1248" aria-hidden="true" tabindex="-1"></a>mod_logreg <span class="sc">%&gt;%</span> </span>
<span id="cb93-1249"><a href="#cb93-1249" aria-hidden="true" tabindex="-1"></a>  broom<span class="sc">::</span><span class="fu">augment</span>(<span class="at">type.predict =</span> <span class="st">"response"</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1250"><a href="#cb93-1250" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">predicted =</span> <span class="fu">if_else</span>(.fitted <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">"Up"</span>, <span class="st">"Down"</span>) <span class="sc">%&gt;%</span> as.factor) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1251"><a href="#cb93-1251" aria-hidden="true" tabindex="-1"></a>  yardstick<span class="sc">::</span><span class="fu">conf_mat</span>(<span class="at">truth =</span> <span class="st">"Direction"</span>,</span>
<span id="cb93-1252"><a href="#cb93-1252" aria-hidden="true" tabindex="-1"></a>                      <span class="at">estimate =</span> <span class="st">"predicted"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb93-1253"><a href="#cb93-1253" aria-hidden="true" tabindex="-1"></a>  <span class="fu">summary</span>()</span>
<span id="cb93-1254"><a href="#cb93-1254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1255"><a href="#cb93-1255" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-1256"><a href="#cb93-1256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1257"><a href="#cb93-1257" aria-hidden="true" tabindex="-1"></a>Model is misclassifying the true "Down"s at a high rate; model is not specific.</span>
<span id="cb93-1258"><a href="#cb93-1258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1261"><a href="#cb93-1261" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-1262"><a href="#cb93-1262" aria-hidden="true" tabindex="-1"></a><span class="co"># split data</span></span>
<span id="cb93-1263"><a href="#cb93-1263" aria-hidden="true" tabindex="-1"></a>data_train <span class="ot">&lt;-</span> data_weekly <span class="sc">%&gt;%</span> </span>
<span id="cb93-1264"><a href="#cb93-1264" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(Year <span class="sc">&lt;=</span> <span class="dv">2008</span>)</span>
<span id="cb93-1265"><a href="#cb93-1265" aria-hidden="true" tabindex="-1"></a>data_test <span class="ot">&lt;-</span> data_weekly <span class="sc">%&gt;%</span> </span>
<span id="cb93-1266"><a href="#cb93-1266" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(Year <span class="sc">&gt;</span> <span class="dv">2008</span>)</span>
<span id="cb93-1267"><a href="#cb93-1267" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1268"><a href="#cb93-1268" aria-hidden="true" tabindex="-1"></a><span class="co"># define function to fit different types of models</span></span>
<span id="cb93-1269"><a href="#cb93-1269" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; </span><span class="al">NOTE</span><span class="co">: only works with first order models</span></span>
<span id="cb93-1270"><a href="#cb93-1270" aria-hidden="true" tabindex="-1"></a>fit_model <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">model =</span> <span class="fu">c</span>(<span class="st">"logreg"</span>, <span class="st">"lda"</span>, <span class="st">"qda"</span>, <span class="st">"knn"</span>, <span class="st">"nb"</span>), formula, response_levels, df_train, df_test, <span class="at">threshold =</span> <span class="fl">0.5</span>, <span class="at">k =</span> <span class="dv">1</span>){</span>
<span id="cb93-1271"><a href="#cb93-1271" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb93-1272"><a href="#cb93-1272" aria-hidden="true" tabindex="-1"></a>  <span class="co"># set items</span></span>
<span id="cb93-1273"><a href="#cb93-1273" aria-hidden="true" tabindex="-1"></a>  model <span class="ot">=</span> <span class="fu">match.arg</span>(model)</span>
<span id="cb93-1274"><a href="#cb93-1274" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb93-1275"><a href="#cb93-1275" aria-hidden="true" tabindex="-1"></a>  <span class="co"># knn does model and prediction in one step</span></span>
<span id="cb93-1276"><a href="#cb93-1276" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">identical</span>(model, <span class="st">"knn"</span>)) {</span>
<span id="cb93-1277"><a href="#cb93-1277" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb93-1278"><a href="#cb93-1278" aria-hidden="true" tabindex="-1"></a>    <span class="co"># extract string of predictors (and format as vector) and response from formula</span></span>
<span id="cb93-1279"><a href="#cb93-1279" aria-hidden="true" tabindex="-1"></a>    x <span class="ot">=</span> formula <span class="sc">%&gt;%</span> <span class="fu">str_sub</span>(.,</span>
<span id="cb93-1280"><a href="#cb93-1280" aria-hidden="true" tabindex="-1"></a>                            <span class="at">start =</span>  <span class="fu">str_locate</span>(., <span class="st">"~"</span>)[<span class="dv">1</span>]<span class="sc">+</span><span class="dv">2</span>,</span>
<span id="cb93-1281"><a href="#cb93-1281" aria-hidden="true" tabindex="-1"></a>                            <span class="at">end =</span> <span class="sc">-</span><span class="dv">1</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1282"><a href="#cb93-1282" aria-hidden="true" tabindex="-1"></a>      <span class="fu">data.frame</span>(<span class="at">x =</span> .) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1283"><a href="#cb93-1283" aria-hidden="true" tabindex="-1"></a>      <span class="fu">separate_wider_delim</span>(<span class="at">cols =</span> x,</span>
<span id="cb93-1284"><a href="#cb93-1284" aria-hidden="true" tabindex="-1"></a>                           <span class="at">delim =</span> <span class="st">" + "</span>,</span>
<span id="cb93-1285"><a href="#cb93-1285" aria-hidden="true" tabindex="-1"></a>                           <span class="at">names_sep =</span> <span class="st">""</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1286"><a href="#cb93-1286" aria-hidden="true" tabindex="-1"></a>      <span class="fu">reduce</span>(<span class="at">.f =</span> c)</span>
<span id="cb93-1287"><a href="#cb93-1287" aria-hidden="true" tabindex="-1"></a>    y <span class="ot">=</span> formula <span class="sc">%&gt;%</span> <span class="fu">str_sub</span>(.,</span>
<span id="cb93-1288"><a href="#cb93-1288" aria-hidden="true" tabindex="-1"></a>                            <span class="at">start =</span> <span class="dv">1</span>,</span>
<span id="cb93-1289"><a href="#cb93-1289" aria-hidden="true" tabindex="-1"></a>                            <span class="at">end =</span> <span class="fu">str_locate</span>(., <span class="st">"~"</span>)[<span class="dv">1</span>]<span class="sc">-</span><span class="dv">2</span>) </span>
<span id="cb93-1290"><a href="#cb93-1290" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb93-1291"><a href="#cb93-1291" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fit model and calculate predictions</span></span>
<span id="cb93-1292"><a href="#cb93-1292" aria-hidden="true" tabindex="-1"></a>    pred_class <span class="ot">=</span> class<span class="sc">::</span><span class="fu">knn</span>(<span class="at">train =</span> df_train <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">any_of</span>(x)) <span class="sc">%&gt;%</span> as.matrix,</span>
<span id="cb93-1293"><a href="#cb93-1293" aria-hidden="true" tabindex="-1"></a>                            <span class="at">test =</span> df_test <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">any_of</span>(x)) <span class="sc">%&gt;%</span> as.matrix,</span>
<span id="cb93-1294"><a href="#cb93-1294" aria-hidden="true" tabindex="-1"></a>                            <span class="at">cl =</span> df_train <span class="sc">%&gt;%</span> <span class="fu">pull</span>(<span class="fu">any_of</span>(y)),</span>
<span id="cb93-1295"><a href="#cb93-1295" aria-hidden="true" tabindex="-1"></a>                            <span class="at">k =</span> k) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1296"><a href="#cb93-1296" aria-hidden="true" tabindex="-1"></a>      <span class="fu">data.frame</span>(<span class="at">pred_class =</span> .)</span>
<span id="cb93-1297"><a href="#cb93-1297" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb93-1298"><a href="#cb93-1298" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initialize empty dataframe for returning</span></span>
<span id="cb93-1299"><a href="#cb93-1299" aria-hidden="true" tabindex="-1"></a>    pred_prob <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">pred_prob =</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="fu">length</span>(pred_class)))</span>
<span id="cb93-1300"><a href="#cb93-1300" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb93-1301"><a href="#cb93-1301" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb93-1302"><a href="#cb93-1302" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span>{</span>
<span id="cb93-1303"><a href="#cb93-1303" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb93-1304"><a href="#cb93-1304" aria-hidden="true" tabindex="-1"></a>    <span class="co"># set item</span></span>
<span id="cb93-1305"><a href="#cb93-1305" aria-hidden="true" tabindex="-1"></a>    formula <span class="ot">=</span> <span class="fu">as.formula</span>(formula)</span>
<span id="cb93-1306"><a href="#cb93-1306" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb93-1307"><a href="#cb93-1307" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fit model</span></span>
<span id="cb93-1308"><a href="#cb93-1308" aria-hidden="true" tabindex="-1"></a>    mod <span class="ot">=</span> <span class="cf">if</span>(<span class="fu">identical</span>(model, <span class="st">"logreg"</span>)){</span>
<span id="cb93-1309"><a href="#cb93-1309" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1310"><a href="#cb93-1310" aria-hidden="true" tabindex="-1"></a>      <span class="fu">glm</span>(formula, <span class="at">data =</span> df_train, <span class="at">family =</span> <span class="st">"binomial"</span>)</span>
<span id="cb93-1311"><a href="#cb93-1311" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1312"><a href="#cb93-1312" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb93-1313"><a href="#cb93-1313" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> <span class="cf">if</span>(<span class="fu">identical</span>(model, <span class="st">"lda"</span>)){</span>
<span id="cb93-1314"><a href="#cb93-1314" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1315"><a href="#cb93-1315" aria-hidden="true" tabindex="-1"></a>      MASS<span class="sc">::</span><span class="fu">lda</span>(formula, <span class="at">data =</span> df_train)</span>
<span id="cb93-1316"><a href="#cb93-1316" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1317"><a href="#cb93-1317" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb93-1318"><a href="#cb93-1318" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> <span class="cf">if</span>(<span class="fu">identical</span>(model, <span class="st">"qda"</span>)){</span>
<span id="cb93-1319"><a href="#cb93-1319" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1320"><a href="#cb93-1320" aria-hidden="true" tabindex="-1"></a>      MASS<span class="sc">::</span><span class="fu">qda</span>(formula, <span class="at">data =</span> df_train)</span>
<span id="cb93-1321"><a href="#cb93-1321" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1322"><a href="#cb93-1322" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb93-1323"><a href="#cb93-1323" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> <span class="cf">if</span>(<span class="fu">identical</span>(model, <span class="st">"nb"</span>)){</span>
<span id="cb93-1324"><a href="#cb93-1324" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1325"><a href="#cb93-1325" aria-hidden="true" tabindex="-1"></a>      e1071<span class="sc">::</span><span class="fu">naiveBayes</span>(formula, <span class="at">data =</span> df_train)</span>
<span id="cb93-1326"><a href="#cb93-1326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1327"><a href="#cb93-1327" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb93-1328"><a href="#cb93-1328" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>{</span>
<span id="cb93-1329"><a href="#cb93-1329" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1330"><a href="#cb93-1330" aria-hidden="true" tabindex="-1"></a>      e1071<span class="sc">::</span><span class="fu">naiveBayes</span>(formula, <span class="at">data =</span> df_train)</span>
<span id="cb93-1331"><a href="#cb93-1331" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb93-1332"><a href="#cb93-1332" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb93-1333"><a href="#cb93-1333" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make predictions</span></span>
<span id="cb93-1334"><a href="#cb93-1334" aria-hidden="true" tabindex="-1"></a>    pred_prob <span class="ot">=</span> <span class="cf">if</span>(<span class="fu">identical</span>(model, <span class="st">"logreg"</span>)){</span>
<span id="cb93-1335"><a href="#cb93-1335" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1336"><a href="#cb93-1336" aria-hidden="true" tabindex="-1"></a>      <span class="fu">predict</span>(mod, <span class="at">type =</span> <span class="st">"response"</span>, <span class="at">newdata =</span> df_test)</span>
<span id="cb93-1337"><a href="#cb93-1337" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1338"><a href="#cb93-1338" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb93-1339"><a href="#cb93-1339" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span> <span class="cf">if</span>(model <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">"lda"</span>, <span class="st">"qda"</span>)){</span>
<span id="cb93-1340"><a href="#cb93-1340" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1341"><a href="#cb93-1341" aria-hidden="true" tabindex="-1"></a>      <span class="fu">predict</span>(mod, <span class="at">newdata =</span> df_test)<span class="sc">$</span>posterior[,response_levels[<span class="dv">2</span>]]</span>
<span id="cb93-1342"><a href="#cb93-1342" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1343"><a href="#cb93-1343" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb93-1344"><a href="#cb93-1344" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>{</span>
<span id="cb93-1345"><a href="#cb93-1345" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb93-1346"><a href="#cb93-1346" aria-hidden="true" tabindex="-1"></a>      <span class="fu">predict</span>(mod, <span class="at">type =</span> <span class="st">"raw"</span>, <span class="at">newdata =</span> df_test)[,response_levels[<span class="dv">2</span>]]</span>
<span id="cb93-1347"><a href="#cb93-1347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1348"><a href="#cb93-1348" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb93-1349"><a href="#cb93-1349" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb93-1350"><a href="#cb93-1350" aria-hidden="true" tabindex="-1"></a>    <span class="co"># classify based on predicted probability and threshold</span></span>
<span id="cb93-1351"><a href="#cb93-1351" aria-hidden="true" tabindex="-1"></a>    pred_class <span class="ot">=</span> <span class="fu">ifelse</span>(pred_prob <span class="sc">&gt;</span> threshold, <span class="dv">1</span>, <span class="dv">0</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1352"><a href="#cb93-1352" aria-hidden="true" tabindex="-1"></a>      <span class="fu">data.frame</span>(<span class="at">pred_class =</span> .) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1353"><a href="#cb93-1353" aria-hidden="true" tabindex="-1"></a>      <span class="fu">mutate</span>(<span class="at">pred_class =</span> <span class="fu">case_when</span>(pred_class <span class="sc">==</span> <span class="dv">1</span> <span class="sc">~</span> response_levels[<span class="dv">2</span>],</span>
<span id="cb93-1354"><a href="#cb93-1354" aria-hidden="true" tabindex="-1"></a>                                    <span class="at">.default =</span> response_levels[<span class="dv">1</span>]) <span class="sc">%&gt;%</span> <span class="fu">factor</span>(<span class="at">levels =</span> response_levels))</span>
<span id="cb93-1355"><a href="#cb93-1355" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb93-1356"><a href="#cb93-1356" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb93-1357"><a href="#cb93-1357" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb93-1358"><a href="#cb93-1358" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="at">pred_prob =</span> pred_prob, pred_class) <span class="sc">%&gt;%</span> return</span>
<span id="cb93-1359"><a href="#cb93-1359" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb93-1360"><a href="#cb93-1360" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb93-1361"><a href="#cb93-1361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1362"><a href="#cb93-1362" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize items</span></span>
<span id="cb93-1363"><a href="#cb93-1363" aria-hidden="true" tabindex="-1"></a>models <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"logreg"</span>, <span class="st">"lda"</span>, <span class="st">"qda"</span>, <span class="st">"knn"</span>, <span class="st">"nb"</span>)  </span>
<span id="cb93-1364"><a href="#cb93-1364" aria-hidden="true" tabindex="-1"></a>formula <span class="ot">&lt;-</span> <span class="st">"Direction ~ Lag2"</span></span>
<span id="cb93-1365"><a href="#cb93-1365" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb93-1366"><a href="#cb93-1366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1367"><a href="#cb93-1367" aria-hidden="true" tabindex="-1"></a><span class="co"># define function to calculate confusion matrix / summary statistics</span></span>
<span id="cb93-1368"><a href="#cb93-1368" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; </span><span class="al">NOTE</span><span class="co">: preds is output of fit_model()</span></span>
<span id="cb93-1369"><a href="#cb93-1369" aria-hidden="true" tabindex="-1"></a>calc_conf_mat <span class="ot">&lt;-</span> <span class="cf">function</span>(preds, df_test, truth, estimate) {</span>
<span id="cb93-1370"><a href="#cb93-1370" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb93-1371"><a href="#cb93-1371" aria-hidden="true" tabindex="-1"></a>  df_test <span class="sc">%&gt;%</span> </span>
<span id="cb93-1372"><a href="#cb93-1372" aria-hidden="true" tabindex="-1"></a>      <span class="fu">bind_cols</span>(preds) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1373"><a href="#cb93-1373" aria-hidden="true" tabindex="-1"></a>      yardstick<span class="sc">::</span><span class="fu">conf_mat</span>(<span class="at">truth =</span> truth,</span>
<span id="cb93-1374"><a href="#cb93-1374" aria-hidden="true" tabindex="-1"></a>                          <span class="at">estimate =</span> estimate)</span>
<span id="cb93-1375"><a href="#cb93-1375" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb93-1376"><a href="#cb93-1376" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb93-1377"><a href="#cb93-1377" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1378"><a href="#cb93-1378" aria-hidden="true" tabindex="-1"></a><span class="co"># fit models and assess quality</span></span>
<span id="cb93-1379"><a href="#cb93-1379" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; good enough, could loop over a set of formulas</span></span>
<span id="cb93-1380"><a href="#cb93-1380" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; just manually change formula / k above and look at results</span></span>
<span id="cb93-1381"><a href="#cb93-1381" aria-hidden="true" tabindex="-1"></a><span class="co"># -&gt; change event_level if needed</span></span>
<span id="cb93-1382"><a href="#cb93-1382" aria-hidden="true" tabindex="-1"></a>models <span class="sc">%&gt;%</span> </span>
<span id="cb93-1383"><a href="#cb93-1383" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_names</span>(<span class="at">x =</span> ., <span class="at">nm =</span> .) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1384"><a href="#cb93-1384" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(\(model) <span class="fu">fit_model</span>(<span class="at">model =</span> model, <span class="at">formula =</span> formula, <span class="at">response_levels =</span> <span class="fu">c</span>(<span class="st">"Down"</span>, <span class="st">"Up"</span>), <span class="at">df_train =</span> data_train, <span class="at">df_test =</span> data_test, <span class="at">threshold =</span> <span class="fl">0.5</span>, <span class="at">k =</span> k)) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1385"><a href="#cb93-1385" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(\(preds) <span class="fu">calc_conf_mat</span>(preds, <span class="at">df_test =</span> data_test, <span class="at">truth =</span> <span class="st">"Direction"</span>, <span class="at">estimate =</span> <span class="st">"pred_class"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1386"><a href="#cb93-1386" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(\(conf_mat) <span class="fu">summary</span>(conf_mat, <span class="at">event_level =</span> <span class="st">"second"</span>)[<span class="dv">1</span>,]) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1387"><a href="#cb93-1387" aria-hidden="true" tabindex="-1"></a>  <span class="fu">reduce</span>(bind_rows) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1388"><a href="#cb93-1388" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">data.frame</span>(<span class="at">model =</span> models,</span>
<span id="cb93-1389"><a href="#cb93-1389" aria-hidden="true" tabindex="-1"></a>                       <span class="at">formula =</span> formula)) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1390"><a href="#cb93-1390" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(model, formula, .estimate) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1391"><a href="#cb93-1391" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">k =</span> <span class="fu">ifelse</span>(model <span class="sc">==</span> <span class="st">"knn"</span>, k, <span class="cn">NA</span>),</span>
<span id="cb93-1392"><a href="#cb93-1392" aria-hidden="true" tabindex="-1"></a>         <span class="at">.after =</span> formula)</span>
<span id="cb93-1393"><a href="#cb93-1393" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb93-1394"><a href="#cb93-1394" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-1395"><a href="#cb93-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1396"><a href="#cb93-1396" aria-hidden="true" tabindex="-1"></a>Logistic regression and LDA are the best performing.</span>
<span id="cb93-1397"><a href="#cb93-1397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1398"><a href="#cb93-1398" aria-hidden="true" tabindex="-1"></a>&lt; Didn't experiment too much with different predictor sets / transformations &gt;</span>
<span id="cb93-1399"><a href="#cb93-1399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1400"><a href="#cb93-1400" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 14</span></span>
<span id="cb93-1401"><a href="#cb93-1401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1404"><a href="#cb93-1404" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-1405"><a href="#cb93-1405" aria-hidden="true" tabindex="-1"></a><span class="co"># load data and modify</span></span>
<span id="cb93-1406"><a href="#cb93-1406" aria-hidden="true" tabindex="-1"></a>data_mpg <span class="ot">&lt;-</span> ISLR2<span class="sc">::</span>Auto <span class="sc">%&gt;%</span> </span>
<span id="cb93-1407"><a href="#cb93-1407" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">mpg01 =</span> <span class="fu">ifelse</span>(mpg <span class="sc">&lt;=</span> <span class="fu">median</span>(mpg), <span class="dv">0</span>, <span class="dv">1</span>) <span class="sc">%&gt;%</span> as.factor)</span>
<span id="cb93-1408"><a href="#cb93-1408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1409"><a href="#cb93-1409" aria-hidden="true" tabindex="-1"></a><span class="co"># comparative boxplots of the response against each numeric X</span></span>
<span id="cb93-1410"><a href="#cb93-1410" aria-hidden="true" tabindex="-1"></a>nms_x <span class="ot">&lt;-</span> data_mpg <span class="sc">%&gt;%</span> </span>
<span id="cb93-1411"><a href="#cb93-1411" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(<span class="fu">where</span>(is.numeric), <span class="sc">-</span>mpg) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1412"><a href="#cb93-1412" aria-hidden="true" tabindex="-1"></a>  colnames</span>
<span id="cb93-1413"><a href="#cb93-1413" aria-hidden="true" tabindex="-1"></a><span class="fu">map2</span>(data_mpg <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">where</span>(is.numeric), <span class="sc">-</span>mpg), nms_x, <span class="cf">function</span>(x, nm) {</span>
<span id="cb93-1414"><a href="#cb93-1414" aria-hidden="true" tabindex="-1"></a>  <span class="fu">boxplot</span>(x <span class="sc">~</span> mpg01, <span class="at">main =</span> nm, <span class="at">data =</span> data_mpg)</span>
<span id="cb93-1415"><a href="#cb93-1415" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb93-1416"><a href="#cb93-1416" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1417"><a href="#cb93-1417" aria-hidden="true" tabindex="-1"></a><span class="co"># scatterplot matrix</span></span>
<span id="cb93-1418"><a href="#cb93-1418" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(data_mpg <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="fu">where</span>(is.numeric)))</span>
<span id="cb93-1419"><a href="#cb93-1419" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1420"><a href="#cb93-1420" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-1421"><a href="#cb93-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1422"><a href="#cb93-1422" aria-hidden="true" tabindex="-1"></a>Displacement, horsepower, weight, and year appear to have a relationship with mpg.</span>
<span id="cb93-1423"><a href="#cb93-1423" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1424"><a href="#cb93-1424" aria-hidden="true" tabindex="-1"></a>Now we can split into train and test sets.</span>
<span id="cb93-1425"><a href="#cb93-1425" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1428"><a href="#cb93-1428" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-1429"><a href="#cb93-1429" aria-hidden="true" tabindex="-1"></a><span class="co"># make test / train split</span></span>
<span id="cb93-1430"><a href="#cb93-1430" aria-hidden="true" tabindex="-1"></a>split_mpg <span class="ot">&lt;-</span> rsample<span class="sc">::</span><span class="fu">initial_split</span>(<span class="at">data =</span> data_mpg, <span class="at">prop =</span> .<span class="dv">8</span>)</span>
<span id="cb93-1431"><a href="#cb93-1431" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1432"><a href="#cb93-1432" aria-hidden="true" tabindex="-1"></a><span class="co"># save train and data</span></span>
<span id="cb93-1433"><a href="#cb93-1433" aria-hidden="true" tabindex="-1"></a>data_train <span class="ot">&lt;-</span> split_mpg <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">training</span>()</span>
<span id="cb93-1434"><a href="#cb93-1434" aria-hidden="true" tabindex="-1"></a>data_test <span class="ot">&lt;-</span> split_mpg <span class="sc">%&gt;%</span> rsample<span class="sc">::</span><span class="fu">testing</span>()</span>
<span id="cb93-1435"><a href="#cb93-1435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1436"><a href="#cb93-1436" aria-hidden="true" tabindex="-1"></a><span class="co"># fit all models and get error rates</span></span>
<span id="cb93-1437"><a href="#cb93-1437" aria-hidden="true" tabindex="-1"></a>models <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"logreg"</span>, <span class="st">"lda"</span>, <span class="st">"qda"</span>, <span class="st">"knn"</span>, <span class="st">"nb"</span>)</span>
<span id="cb93-1438"><a href="#cb93-1438" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">7</span></span>
<span id="cb93-1439"><a href="#cb93-1439" aria-hidden="true" tabindex="-1"></a>models <span class="sc">%&gt;%</span> </span>
<span id="cb93-1440"><a href="#cb93-1440" aria-hidden="true" tabindex="-1"></a>  <span class="fu">set_names</span>(<span class="at">x =</span> ., <span class="at">nm =</span> .) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1441"><a href="#cb93-1441" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(\(model) <span class="fu">fit_model</span>(<span class="at">model =</span> model, <span class="at">formula =</span> <span class="st">"mpg01 ~ displacement + horsepower + weight + year"</span>, <span class="at">response_levels =</span> <span class="fu">c</span>(<span class="st">"0"</span>, <span class="st">"1"</span>), <span class="at">df_train =</span> data_train, <span class="at">df_test =</span> data_test, <span class="at">threshold =</span> <span class="fl">0.5</span>, <span class="at">k =</span> k)) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1442"><a href="#cb93-1442" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(\(preds) <span class="fu">calc_conf_mat</span>(preds, <span class="at">df_test =</span> data_test, <span class="at">truth =</span> <span class="st">"mpg01"</span>, <span class="at">estimate =</span> <span class="st">"pred_class"</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1443"><a href="#cb93-1443" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(\(conf_mat) <span class="fu">summary</span>(conf_mat, <span class="at">event_level =</span> <span class="st">"second"</span>)[<span class="dv">1</span>,]) <span class="sc">%&gt;%</span> <span class="co"># pull accuracy (first row)</span></span>
<span id="cb93-1444"><a href="#cb93-1444" aria-hidden="true" tabindex="-1"></a>  <span class="fu">reduce</span>(bind_rows) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1445"><a href="#cb93-1445" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">error_rate =</span> <span class="dv">1</span> <span class="sc">-</span> .estimate) <span class="sc">%&gt;%</span>  <span class="co"># add error rates</span></span>
<span id="cb93-1446"><a href="#cb93-1446" aria-hidden="true" tabindex="-1"></a>  <span class="fu">bind_cols</span>(<span class="fu">data.frame</span>(<span class="at">model =</span> models)) <span class="sc">%&gt;%</span> </span>
<span id="cb93-1447"><a href="#cb93-1447" aria-hidden="true" tabindex="-1"></a>  <span class="fu">select</span>(model, <span class="at">accuracy =</span> .estimate, error_rate) <span class="sc">%&gt;%</span> <span class="co"># rename summary of interest</span></span>
<span id="cb93-1448"><a href="#cb93-1448" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">k =</span> <span class="fu">ifelse</span>(model <span class="sc">==</span> <span class="st">"knn"</span>, k, <span class="cn">NA</span>),</span>
<span id="cb93-1449"><a href="#cb93-1449" aria-hidden="true" tabindex="-1"></a>         <span class="at">.after =</span> model)</span>
<span id="cb93-1450"><a href="#cb93-1450" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1451"><a href="#cb93-1451" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb93-1452"><a href="#cb93-1452" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1453"><a href="#cb93-1453" aria-hidden="true" tabindex="-1"></a>For the models tested here, $k = 7$ appears to perform best. QDA has a lower</span>
<span id="cb93-1454"><a href="#cb93-1454" aria-hidden="true" tabindex="-1"></a>error rate overall than LDA and logistic regression, but only slightly.</span>
<span id="cb93-1455"><a href="#cb93-1455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1456"><a href="#cb93-1456" aria-hidden="true" tabindex="-1"></a>These results suggests the decision boundary is slightly more complicated than linear.</span>
<span id="cb93-1457"><a href="#cb93-1457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1458"><a href="#cb93-1458" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 15</span></span>
<span id="cb93-1459"><a href="#cb93-1459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1460"><a href="#cb93-1460" aria-hidden="true" tabindex="-1"></a>&lt; function writing &gt;</span>
<span id="cb93-1461"><a href="#cb93-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1462"><a href="#cb93-1462" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Question 16</span></span>
<span id="cb93-1463"><a href="#cb93-1463" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1464"><a href="#cb93-1464" aria-hidden="true" tabindex="-1"></a>&lt; another model fitting problem with a different dataset, save for learning tidy models &gt;</span>
<span id="cb93-1465"><a href="#cb93-1465" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb93-1468"><a href="#cb93-1468" aria-hidden="true" tabindex="-1"></a><span class="in">```{r}</span></span>
<span id="cb93-1469"><a href="#cb93-1469" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>